{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhruvv-raghu/DL_Presentation/blob/main/nb/Meta_Synthetic_Data_Llama3_2_(3B).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HopWm5zTARrK"
      },
      "source": [
        "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n",
        "\n",
        "\n",
        "<a href=\"https://github.com/meta-llama/synthetic-data-kit\"><img src=\"https://raw.githubusercontent.com/unslothai/notebooks/refs/heads/main/assets/meta%20round%20logo.png\" width=\"137\"></a>\n",
        "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
        "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ⭐ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ⭐\n",
        "</div>\n",
        "\n",
        "To install Unsloth on your own computer, follow the installation instructions on our Github page [here](https://docs.unsloth.ai/get-started/installing-+-updating).\n",
        "\n",
        "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Om2qjxs5PSr0"
      },
      "source": [
        "### News"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCa86oMuPSr0"
      },
      "source": [
        "\n",
        "Unsloth now supports [gpt-oss RL](https://docs.unsloth.ai/new/gpt-oss-reinforcement-learning) with the fastest inference & lowest VRAM. Try our [new notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-GRPO.ipynb) which automatically creates kernels!\n",
        "\n",
        "[Vision RL](https://docs.unsloth.ai/new/vision-reinforcement-learning-vlm-rl) is now supported! Train Qwen2.5-VL, Gemma 3 etc. with GSPO or GRPO.\n",
        "\n",
        "Introducing Unsloth [Standby for RL](https://docs.unsloth.ai/basics/memory-efficient-rl): GRPO is now faster, uses 30% less memory with 2x longer context.\n",
        "\n",
        "Unsloth now supports Text-to-Speech (TTS) models. Read our [guide here](https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning).\n",
        "\n",
        "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imwJhjjYPSr0"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "as6X9O13ARrN"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "!pip install --upgrade -qqq uv\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    # If you're not in Colab, just use pip install!\n",
        "    !pip install unsloth vllm synthetic-data-kit==0.0.3\n",
        "else:\n",
        "    try: import numpy; get_numpy = f\"numpy=={numpy.__version__}\"\n",
        "    except: get_numpy = \"numpy\"\n",
        "    try: import subprocess; is_t4 = \"Tesla T4\" in str(subprocess.check_output([\"nvidia-smi\"]))\n",
        "    except: is_t4 = False\n",
        "    get_vllm, get_triton = (\"vllm==0.9.2\", \"triton==3.2.0\") if is_t4 else (\"vllm==0.10.2\", \"triton\")\n",
        "    !uv pip install -qqq --upgrade         unsloth {get_vllm} {get_numpy} torchvision bitsandbytes xformers\n",
        "    !uv pip install -qqq {get_triton}\n",
        "    !uv pip install synthetic-data-kit==0.0.3\n",
        "!uv pip install transformers==4.55.4\n",
        "!uv pip install --no-deps trl==0.22.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "M97bgGcdARrO"
      },
      "outputs": [],
      "source": [
        "#@title Colab Extra Install { display-mode: \"form\" }\n",
        "%%capture\n",
        "import os\n",
        "!pip install --upgrade -qqq uv\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    # If you're not in Colab, just use pip install!\n",
        "    !pip install unsloth vllm\n",
        "else:\n",
        "    try: import numpy; get_numpy = f\"numpy=={numpy.__version__}\"\n",
        "    except: get_numpy = \"numpy\"\n",
        "    try: import subprocess; is_t4 = \"Tesla T4\" in str(subprocess.check_output([\"nvidia-smi\"]))\n",
        "    except: is_t4 = False\n",
        "    get_vllm, get_triton = (\"vllm==0.9.2\", \"triton==3.2.0\") if is_t4 else (\"vllm==0.10.2\", \"triton\")\n",
        "    !uv pip install -qqq --upgrade \\\n",
        "        unsloth {get_vllm} {get_numpy} torchvision bitsandbytes xformers\n",
        "    !uv pip install -qqq {get_triton}\n",
        "!uv pip install transformers==4.55.4\n",
        "!uv pip install --no-deps trl==0.22.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCWl9UV_ARrO"
      },
      "source": [
        "### Unsloth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZ93zXtoARrO"
      },
      "source": [
        "## Primary Goal\n",
        "Our goal is to make Llama 3.2 3B understand the \"Byte Latent Transformer: Patches Scale Better Than Tokens\" [research paper](https://ai.meta.com/research/publications/byte-latent-transformer-patches-scale-better-than-tokens/) that was published in December 2024.\n",
        "\n",
        "We'll use https://github.com/meta-llama/synthetic-data-kit to generate question and answer pairs **fully locally** which will be used for finetuning Llama 3.2 3B!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "bc5255f132404e10aa6d8e189bab9b36",
            "c902678c9a1b46e3a4666b85a97a3acd",
            "5556359e0de44c9b9c66c818838aedf5",
            "1d8ffbac8a6041ba875512ea492b076c",
            "35d09c7ae0a049fcaaf8e86a8e417621",
            "4e18653fd6aa46acb621937f3aac9a8a",
            "9cdcae8f9cd5448e8c79154cadcc352b",
            "85f2e9b6f0064d8f8a9e9bc8bfd6df38",
            "2950e1ee64da49b4acbf9944dda3314d",
            "843acde25a6e45aa8b5c7a1c75cd3544",
            "e0af34e805c94388b8a8115a376897c7",
            "8ba5a4dbdb674789a0a617719437186a",
            "0b8f6cae89f444c5a3c5b541b9621d03",
            "a0ac13bcb305452ea5aec6ade87c68eb",
            "43a5b03a04e84627a154946dc2d41e93",
            "b472f5d4587a4ad39ea781e964fe0f5b",
            "5dfe7b3128a349369e930ebdf4842d44",
            "655ff447e5c640ba9025df3e9fc10512",
            "52c6e4e34953424f910df0f62f615160",
            "d7350a837b07472386b9ebb2a46c80be",
            "dcf94d67981e47fe86641b6bf2bec101",
            "c99d35bf75984cee9a2089166974b190",
            "0b44a74efd614512919ad1ad45c5b904",
            "1c676305631b43a68998ce34eeec566d",
            "c6fa1942a4e2484b82b8740a4b581436",
            "a2638345cfbf4949be4a7d107aad3ece",
            "574f8061456c46759600bf8e22f26409",
            "888d76a2597b4db3b9d1cbc196f67664",
            "89545bd2548f456f889e237b415c42dd",
            "76bfee01fe7c4f0aa75ead68c0bcc49a",
            "b04f99bb1c64449fbf132ed8e6129c6d",
            "731e4030f7b24b89870ff82eb7fa6eed",
            "6f650273d91544e08f99ca860392df55",
            "4e02c9fb37f149ac80c295eaf0eaeb80",
            "47e583bdc96e4682a0f514cb47658394",
            "41f4db933794493e9cecc3f895a39f8b",
            "18ed295f23c64bb8827bdf3b46e14e83",
            "907fb80c198c4bcdae5488c41d463924",
            "4103ea9f667b454683a8b1fbd4829297",
            "dcfc7aad9f12453d8f5cad38c175823c",
            "1158e1862798461a9b1985b3f352d2e6",
            "3bfc0ee630c7477d9f736915c22891c0",
            "1b74a0cae76e4c239cb8c589f787cf2d",
            "73609c795f6441018999cc9ebf799bfd",
            "25d95c66ad4e4f2ca54f8071beee56fc",
            "aea11869fbaf47419aa13f93bcc72743",
            "c9adc625c7d54d4cae3a3d9f290c2f74",
            "757c9a5d8a7848a5a431b0dbb00b2a50",
            "bee3a00153e443e6aec65b4a10a3ecf3",
            "09f7aab30e8a4d43b82d5aaa7382b064",
            "2d244b7c165d43f2a5f6bc894559d300",
            "fc42d1b265c0444a886415304e34ba1f",
            "a1d8c68967cf454aa1707721b9988f9f",
            "7c2a616e79f347f4a99d2a9016a0d729",
            "8509d49d58c94523801d45f23c4ce92d"
          ]
        },
        "id": "Ym8UA1PRiXsa",
        "outputId": "d43d4a59-37be-4f0e-ef0a-64d6f3ce7b3a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/894 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bc5255f132404e10aa6d8e189bab9b36"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8ba5a4dbdb674789a0a617719437186a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0b44a74efd614512919ad1ad45c5b904"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4e02c9fb37f149ac80c295eaf0eaeb80"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "chat_template.jinja: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "25d95c66ad4e4f2ca54f8071beee56fc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 10-03 08:30:15 [vllm_utils.py:689] Unsloth: Patching vLLM v1 graph capture\n",
            "INFO 10-03 08:30:15 [vllm_utils.py:717] Unsloth: Patching vLLM v0 graph capture\n",
            "Unsloth: Using dtype = torch.float16 for vLLM.\n",
            "Unsloth: vLLM loading unsloth/Llama-3.2-1B-Instruct with actual GPU utilization = 67.35%\n",
            "Unsloth: Your GPU has CUDA compute capability 7.5 with VRAM = 14.74 GB.\n",
            "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 2048. Num Sequences = 192.\n",
            "Unsloth: vLLM's KV Cache can use up to 7.63 GB. Also swap space = 0 GB.\n",
            "vLLM STDOUT: INFO 10-03 08:30:27 [__init__.py:244] Automatically detected platform cuda.\n",
            "vLLM STDOUT: INFO 10-03 08:30:31 [api_server.py:1395] vLLM API server version 0.9.2\n",
            "vLLM STDOUT: INFO 10-03 08:30:31 [cli_args.py:325] non-default args: {'model': 'unsloth/Llama-3.2-1B-Instruct', 'dtype': 'float16', 'seed': 0, 'max_model_len': 2048, 'max_logprobs': 0, 'gpu_memory_utilization': 0.6734991988274313, 'swap_space': 0.0, 'enable_prefix_caching': True, 'max_num_batched_tokens': 2048, 'max_num_seqs': 192, 'enable_chunked_prefill': True, 'disable_log_stats': True}\n",
            "vLLM STDOUT: INFO 10-03 08:30:45 [config.py:841] This model supports multiple tasks: {'embed', 'classify', 'generate', 'reward'}. Defaulting to 'generate'.\n",
            "vLLM STDOUT: WARNING 10-03 08:30:45 [config.py:3371] Casting torch.bfloat16 to torch.float16.\n",
            "vLLM STDOUT: INFO 10-03 08:30:45 [config.py:1472] Using max model len 2048\n",
            "vLLM STDOUT: WARNING 10-03 08:30:45 [arg_utils.py:1735] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \n",
            "vLLM STDOUT: INFO 10-03 08:30:46 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
            "vLLM STDOUT: INFO 10-03 08:30:46 [api_server.py:268] Started engine process with PID 11276\n",
            "vLLM STDOUT: INFO 10-03 08:30:57 [__init__.py:244] Automatically detected platform cuda.\n",
            "vLLM STDOUT: INFO 10-03 08:30:59 [llm_engine.py:230] Initializing a V0 LLM engine (v0.9.2) with config: model='unsloth/Llama-3.2-1B-Instruct', speculative_config=None, tokenizer='unsloth/Llama-3.2-1B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Llama-3.2-1B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":192,\"local_cache_dir\":null}, use_cached_outputs=True, \n",
            "vLLM STDOUT: INFO 10-03 08:31:01 [cuda.py:311] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
            "vLLM STDOUT: INFO 10-03 08:31:01 [cuda.py:360] Using XFormers backend.\n",
            "vLLM STDOUT: INFO 10-03 08:31:02 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
            "vLLM STDOUT: INFO 10-03 08:31:02 [model_runner.py:1171] Starting to load model unsloth/Llama-3.2-1B-Instruct...\n",
            "vLLM STDOUT: INFO 10-03 08:31:03 [weight_utils.py:292] Using model weights format ['*.safetensors']\n",
            "vLLM STDOUT: INFO 10-03 08:32:01 [weight_utils.py:308] Time spent downloading weights for unsloth/Llama-3.2-1B-Instruct: 57.977584 seconds\n",
            "vLLM STDOUT: INFO 10-03 08:32:01 [weight_utils.py:345] No model.safetensors.index.json found in remote.\n",
            "vLLM STDOUT: INFO 10-03 08:32:10 [default_loader.py:272] Loading weights took 9.13 seconds\n",
            "vLLM STDOUT: INFO 10-03 08:32:11 [model_runner.py:1203] Model loading took 2.3205 GiB and 67.779447 seconds\n",
            "vLLM STDOUT: INFO 10-03 08:32:12 [worker.py:294] Memory profiling takes 1.07 seconds\n",
            "vLLM STDOUT: INFO 10-03 08:32:12 [worker.py:294] the current vLLM instance can use total_gpu_memory (14.74GiB) x gpu_memory_utilization (0.67) = 9.93GiB\n",
            "vLLM STDOUT: INFO 10-03 08:32:12 [worker.py:294] model weights take 2.32GiB; non_torch_memory takes 0.05GiB; PyTorch activation peak memory takes 0.89GiB; the rest of the memory reserved for KV Cache is 6.67GiB.\n",
            "vLLM STDOUT: INFO 10-03 08:32:13 [executor_base.py:113] # cuda blocks: 13654, # CPU blocks: 0\n",
            "vLLM STDOUT: INFO 10-03 08:32:13 [executor_base.py:118] Maximum concurrency for 2048 tokens per request: 106.67x\n",
            "vLLM STDOUT: INFO 10-03 08:32:13 [model_runner.py:1513] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
            "vLLM STDOUT: INFO 10-03 08:32:38 [model_runner.py:1671] Graph capturing finished in 25 secs, took 0.11 GiB\n",
            "vLLM STDOUT: INFO 10-03 08:32:38 [llm_engine.py:428] init engine (profile, create kv cache, warmup model) took 26.90 seconds\n",
            "vLLM STDOUT: WARNING 10-03 08:32:38 [config.py:1392] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.\n",
            "vLLM STDOUT: INFO 10-03 08:32:38 [serving_chat.py:125] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}\n",
            "vLLM STDOUT: INFO 10-03 08:32:38 [serving_completion.py:72] Using default completion sampling params from model: {'temperature': 0.6, 'top_p': 0.9}\n",
            "vLLM STDOUT: INFO 10-03 08:32:38 [api_server.py:1457] Starting vLLM API server 0 on http://0.0.0.0:8000\n",
            "vLLM Server Ready Detected\n",
            "vLLM STDOUT: INFO 10-03 08:32:38 [launcher.py:29] Available routes are:\n",
            "vLLM STDOUT: INFO 10-03 08:32:38 [launcher.py:37] Route: /openapi.json, Methods: GET, HEAD\n",
            "vLLM STDOUT: INFO 10-03 08:32:38 [launcher.py:37] Route: /docs, Methods: GET, HEAD\n",
            "vLLM STDOUT: INFO 10-03 08:32:38 [launcher.py:37] Route: /docs/oauth2-redirect, Methods: GET, HEAD\n",
            "vLLM STDOUT: INFO 10-03 08:32:38 [launcher.py:37] Route: /redoc, Methods: GET, HEAD\n",
            "vLLM STDOUT: INFO 10-03 08:32:38 [launcher.py:37] Route: /health, Methods: GET\n",
            "vLLM STDOUT: INFO 10-03 08:32:38 [launcher.py:37] Route: /load, Methods: GET\n",
            "vLLM STDOUT: INFO 10-03 08:32:38 [launcher.py:37] Route: /ping, Methods: POST\n",
            "vLLM STDOUT: INFO 10-03 08:32:38 [launcher.py:37] Route: /ping, Methods: GET\n",
            "vLLM STDOUT: INFO 10-03 08:32:38 [launcher.py:37] Route: /tokenize, Methods: POST\n",
            "vLLM STDOUT: INFO 10-03 08:32:38 [launcher.py:37] Route: /detokenize, Methods: POST\n",
            "vLLM STDOUT: INFO 10-03 08:32:38 [launcher.py:37] Route: /v1/models, Methods: GET\n",
            "vLLM STDOUT: INFO 10-03 08:32:38 [launcher.py:37] Route: /version, Methods: GET\n",
            "vLLM STDOUT: INFO 10-03 08:32:38 [launcher.py:37] Route: /v1/chat/completions, Methods: POST\n",
            "vLLM STDOUT: INFO 10-03 08:32:38 [launcher.py:37] Route: /v1/completions, Methods: POST\n",
            "vLLM STDOUT: INFO 10-03 08:32:38 [launcher.py:37] Route: /v1/embeddings, Methods: POST\n",
            "vLLM STDOUT: INFO 10-03 08:32:38 [launcher.py:37] Route: /pooling, Methods: POST\n",
            "vLLM STDOUT: INFO 10-03 08:32:38 [launcher.py:37] Route: /classify, Methods: POST\n",
            "vLLM STDOUT: INFO 10-03 08:32:38 [launcher.py:37] Route: /score, Methods: POST\n",
            "vLLM STDOUT: INFO 10-03 08:32:38 [launcher.py:37] Route: /v1/score, Methods: POST\n",
            "vLLM STDOUT: INFO 10-03 08:32:38 [launcher.py:37] Route: /v1/audio/transcriptions, Methods: POST\n",
            "vLLM STDOUT: INFO 10-03 08:32:38 [launcher.py:37] Route: /v1/audio/translations, Methods: POST\n",
            "vLLM STDOUT: INFO 10-03 08:32:38 [launcher.py:37] Route: /rerank, Methods: POST\n",
            "vLLM STDOUT: INFO 10-03 08:32:38 [launcher.py:37] Route: /v1/rerank, Methods: POST\n",
            "vLLM STDOUT: INFO 10-03 08:32:38 [launcher.py:37] Route: /v2/rerank, Methods: POST\n",
            "vLLM STDOUT: INFO 10-03 08:32:38 [launcher.py:37] Route: /invocations, Methods: POST\n",
            "vLLM STDOUT: INFO 10-03 08:32:38 [launcher.py:37] Route: /metrics, Methods: GET\n",
            "vLLM STDOUT: INFO:     127.0.0.1:46738 - \"GET /metrics HTTP/1.1\" 200 OK\n",
            "Attempting to terminate the VLLM server gracefully...\n",
            "Server terminated gracefully.\n"
          ]
        }
      ],
      "source": [
        "from unsloth.dataprep import SyntheticDataKit\n",
        "\n",
        "generator = SyntheticDataKit.from_pretrained(\n",
        "    # Choose any model from https://huggingface.co/unsloth\n",
        "    model_name = \"unsloth/Llama-3.2-1B-Instruct\",\n",
        "    max_seq_length = 2048, # Longer sequence lengths will be slower!\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ef_MnK575tr2"
      },
      "source": [
        "## Generate QA Pairs + Auto clean data\n",
        "We now use synthetic data kit for question answer pair generation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "q487TNby-nwT"
      },
      "outputs": [],
      "source": [
        "generator.prepare_qa_generation(\n",
        "    output_folder = \"data\", # Output location of synthetic data\n",
        "    temperature = 0.7, # Higher temp makes more diverse datases\n",
        "    top_p = 0.95,\n",
        "    overlap = 64, # Overlap portion during chunking\n",
        "    max_generation_tokens = 512, # Can increase for longer QA pairs\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yV7DyufR51IN"
      },
      "source": [
        "Check if it succeeded:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2gQZcr_Wp94",
        "outputId": "45e2dfb6-ac6c-4e40-eab4-eb2527084821"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vLLM STDOUT: INFO:     127.0.0.1:48288 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "\u001b[?25l\u001b[32m VLLM server is running at \u001b[0m\u001b[4;94mhttp://localhost:8000/v1\u001b[0m\n",
            "\u001b[32m⠋\u001b[0m\u001b[32m Checking VLLM server at http://localhost:8000/v1...\u001b[0m\r\u001b[2KAvailable models: \u001b[1m{\u001b[0m\u001b[32m'object'\u001b[0m: \u001b[32m'list'\u001b[0m, \u001b[32m'data'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m{\u001b[0m\u001b[32m'id'\u001b[0m: \n",
            "\u001b[32m'unsloth/Llama-3.2-1B-Instruct'\u001b[0m, \u001b[32m'object'\u001b[0m: \u001b[32m'model'\u001b[0m, \u001b[32m'created'\u001b[0m: \u001b[1;36m1759480367\u001b[0m, \n",
            "\u001b[32m'owned_by'\u001b[0m: \u001b[32m'vllm'\u001b[0m, \u001b[32m'root'\u001b[0m: \u001b[32m'unsloth/Llama-3.2-1B-Instruct'\u001b[0m, \u001b[32m'parent'\u001b[0m: \u001b[3;35mNone\u001b[0m, \n",
            "\u001b[32m'max_model_len'\u001b[0m: \u001b[1;36m2048\u001b[0m, \u001b[32m'permission'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m{\u001b[0m\u001b[32m'id'\u001b[0m: \n",
            "\u001b[32m'modelperm-805fc1f9d1924ab7b605ddb0991cce40'\u001b[0m, \u001b[32m'object'\u001b[0m: \u001b[32m'model_permission'\u001b[0m, \n",
            "\u001b[32m'created'\u001b[0m: \u001b[1;36m1759480367\u001b[0m, \u001b[32m'allow_create_engine'\u001b[0m: \u001b[3;91mFalse\u001b[0m, \u001b[32m'allow_sampling'\u001b[0m: \u001b[3;92mTrue\u001b[0m, \n",
            "\u001b[32m'allow_logprobs'\u001b[0m: \u001b[3;92mTrue\u001b[0m, \u001b[32m'allow_search_indices'\u001b[0m: \u001b[3;91mFalse\u001b[0m, \u001b[32m'allow_view'\u001b[0m: \u001b[3;92mTrue\u001b[0m, \n",
            "\u001b[32m'allow_fine_tuning'\u001b[0m: \u001b[3;91mFalse\u001b[0m, \u001b[32m'organization'\u001b[0m: \u001b[32m'*'\u001b[0m, \u001b[32m'group'\u001b[0m: \u001b[3;35mNone\u001b[0m, \u001b[32m'is_blocking'\u001b[0m: \n",
            "\u001b[3;91mFalse\u001b[0m\u001b[1m}\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\n",
            "\u001b[32m⠋\u001b[0m Checking VLLM server at http://localhost:8000/v1...\r\u001b[2K\u001b[32m⠋\u001b[0m Checking VLLM server at http://localhost:8000/v1...\n",
            "\u001b[?25h\r\u001b[1A\u001b[2K"
          ]
        }
      ],
      "source": [
        "!synthetic-data-kit system-check"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdl7aPFK55M1"
      },
      "source": [
        "## Document Parsing (PDF, CSV, HTML etc.)\n",
        "Now, let's take the Byte Latent Transformer: Patches Scale Better Than Tokens research paper found at https://arxiv.org/abs/2412.09871 and covert it to Q&A pairs in order to finetune Llama 3.2!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8BN1yrPGmANA",
        "outputId": "1cbd5637-c3d7-4e10-c3ed-67da07f9d88a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Ingesting 16 documents ---\n",
            "Ingesting: https://arxiv.org/html/2411.02959v2\n",
            "\u001b[2K\u001b[32m⠴\u001b[0m Processing https://arxiv.org/html/2411.02959v2...\n",
            "\u001b[1A\u001b[2K\u001b[32m Text successfully extracted to \u001b[0m\u001b[1;32mdata/output/arxiv_org.txt\u001b[0m\n",
            "Ingesting: https://arxiv.org/html/2510.02312v1\n",
            "\u001b[2K\u001b[32m⠸\u001b[0m Processing https://arxiv.org/html/2510.02312v1...\n",
            "\u001b[1A\u001b[2K\u001b[32m Text successfully extracted to \u001b[0m\u001b[1;32mdata/output/arxiv_org.txt\u001b[0m\n",
            "Ingesting: https://arxiv.org/html/2510.02305v1\n",
            "\u001b[2K\u001b[32m⠼\u001b[0m Processing https://arxiv.org/html/2510.02305v1...\n",
            "\u001b[1A\u001b[2K\u001b[32m Text successfully extracted to \u001b[0m\u001b[1;32mdata/output/arxiv_org.txt\u001b[0m\n",
            "Ingesting: https://arxiv.org/html/2510.02180v1\n",
            "\u001b[2K\u001b[32m⠹\u001b[0m Processing https://arxiv.org/html/2510.02180v1...\n",
            "\u001b[1A\u001b[2K\u001b[32m Text successfully extracted to \u001b[0m\u001b[1;32mdata/output/arxiv_org.txt\u001b[0m\n",
            "Ingesting: https://arxiv.org/html/2509.21340v1\n",
            "\u001b[2K\u001b[32m⠋\u001b[0m Processing https://arxiv.org/html/2509.21340v1...\n",
            "\u001b[1A\u001b[2K\u001b[32m Text successfully extracted to \u001b[0m\u001b[1;32mdata/output/arxiv_org.txt\u001b[0m\n",
            "Ingesting: https://arxiv.org/html/2509.21346v1\n",
            "\u001b[2K\u001b[32m⠹\u001b[0m Processing https://arxiv.org/html/2509.21346v1...\n",
            "\u001b[1A\u001b[2K\u001b[32m Text successfully extracted to \u001b[0m\u001b[1;32mdata/output/arxiv_org.txt\u001b[0m\n",
            "Ingesting: https://arxiv.org/html/2510.02245v1\n",
            "\u001b[2K\u001b[32m⠧\u001b[0m Processing https://arxiv.org/html/2510.02245v1...\n",
            "\u001b[1A\u001b[2K\u001b[32m Text successfully extracted to \u001b[0m\u001b[1;32mdata/output/arxiv_org.txt\u001b[0m\n",
            "Ingesting: https://arxiv.org/html/2510.02227v1\n",
            "\u001b[2K\u001b[32m⠧\u001b[0m Processing https://arxiv.org/html/2510.02227v1...\n",
            "\u001b[1A\u001b[2K\u001b[32m Text successfully extracted to \u001b[0m\u001b[1;32mdata/output/arxiv_org.txt\u001b[0m\n",
            "Ingesting: https://arxiv.org/html/2510.02212v1\n",
            "\u001b[2K\u001b[32m⠴\u001b[0m Processing https://arxiv.org/html/2510.02212v1...\n",
            "\u001b[1A\u001b[2K\u001b[32m Text successfully extracted to \u001b[0m\u001b[1;32mdata/output/arxiv_org.txt\u001b[0m\n",
            "Ingesting: https://arxiv.org/html/2510.02140v1\n",
            "\u001b[2K\u001b[32m⠏\u001b[0m Processing https://arxiv.org/html/2510.02140v1...\n",
            "\u001b[1A\u001b[2K\u001b[32m Text successfully extracted to \u001b[0m\u001b[1;32mdata/output/arxiv_org.txt\u001b[0m\n",
            "Ingesting: https://arxiv.org/html/2510.01656v1\n",
            "\u001b[2K\u001b[32m⠼\u001b[0m Processing https://arxiv.org/html/2510.01656v1...\n",
            "\u001b[1A\u001b[2K\u001b[32m Text successfully extracted to \u001b[0m\u001b[1;32mdata/output/arxiv_org.txt\u001b[0m\n",
            "Ingesting: https://arxiv.org/html/2510.01544v1\n",
            "\u001b[2K\u001b[32m⠼\u001b[0m Processing https://arxiv.org/html/2510.01544v1...\n",
            "\u001b[1A\u001b[2K\u001b[32m Text successfully extracted to \u001b[0m\u001b[1;32mdata/output/arxiv_org.txt\u001b[0m\n",
            "Ingesting: https://arxiv.org/html/2510.01459v1\n",
            "\u001b[2K\u001b[32m⠹\u001b[0m Processing https://arxiv.org/html/2510.01459v1...\n",
            "\u001b[1A\u001b[2K\u001b[32m Text successfully extracted to \u001b[0m\u001b[1;32mdata/output/arxiv_org.txt\u001b[0m\n",
            "Ingesting: https://arxiv.org/html/2510.00911v1\n",
            "\u001b[2K\u001b[32m⠏\u001b[0m Processing https://arxiv.org/html/2510.00911v1...\n",
            "\u001b[1A\u001b[2K\u001b[32m Text successfully extracted to \u001b[0m\u001b[1;32mdata/output/arxiv_org.txt\u001b[0m\n",
            "Ingesting: https://arxiv.org/html/2510.00690v1\n",
            "\u001b[2K\u001b[32m⠹\u001b[0m Processing https://arxiv.org/html/2510.00690v1...\n",
            "\u001b[1A\u001b[2K\u001b[32m Text successfully extracted to \u001b[0m\u001b[1;32mdata/output/arxiv_org.txt\u001b[0m\n",
            "Ingesting: https://arxiv.org/html/2509.23967v1\n",
            "\u001b[2K\u001b[32m⠸\u001b[0m Processing https://arxiv.org/html/2509.23967v1...\n",
            "\u001b[1A\u001b[2K\u001b[32m Text successfully extracted to \u001b[0m\u001b[1;32mdata/output/arxiv_org.txt\u001b[0m\n",
            "\n",
            "--- Chunking all ingested documents ---\n",
            "Chunking data/output/arxiv_org_8.txt...\n",
            "Chunking data/output/arxiv_org_2_0.txt...\n",
            "Chunking data/output/arxiv_org_17.txt...\n",
            "Chunking data/output/arxiv_org_21.txt...\n",
            "Chunking data/output/arxiv_org_1.txt...\n",
            "Chunking data/output/arxiv_org_7.txt...\n",
            "Chunking data/output/arxiv_org_17_0.txt...\n",
            "Chunking data/output/arxiv_org_20_0.txt...\n",
            "Chunking data/output/arxiv_org_16_0.txt...\n",
            "Chunking data/output/arxiv_org_8_0.txt...\n",
            "Chunking data/output/arxiv_org_3_0.txt...\n",
            "Chunking data/output/arxiv_org_3.txt...\n",
            "Chunking data/output/arxiv_org_7_0.txt...\n",
            "Chunking data/output/arxiv_org_10.txt...\n",
            "Chunking data/output/arxiv_org_19_0.txt...\n",
            "Chunking data/output/arxiv_org_0_0.txt...\n",
            "Chunking data/output/arxiv_org_14_0.txt...\n",
            "Chunking data/output/arxiv_org_20.txt...\n",
            "Chunking data/output/arxiv_org_9_0.txt...\n",
            "Chunking data/output/arxiv_org_6.txt...\n",
            "Chunking data/output/arxiv_org_10_0.txt...\n",
            "Chunking data/output/arxiv_org_9.txt...\n",
            "Chunking data/output/arxiv_org_6_0.txt...\n",
            "Chunking data/output/arxiv_org_4_0.txt...\n",
            "Chunking data/output/arxiv_org_12.txt...\n",
            "Chunking data/output/arxiv_org_5.txt...\n",
            "Chunking data/output/arxiv_org_13.txt...\n",
            "Chunking data/output/arxiv_org_22.txt...\n",
            "Chunking data/output/arxiv_org_12_0.txt...\n",
            "Chunking data/output/arxiv_org_0.txt...\n",
            "Chunking data/output/arxiv_org_4.txt...\n",
            "Chunking data/output/arxiv_org_22_0.txt...\n",
            "Chunking data/output/arxiv_org_2.txt...\n",
            "Chunking data/output/arxiv_org_15.txt...\n",
            "Chunking data/output/arxiv_org_18_0.txt...\n",
            "Chunking data/output/arxiv_org_5_0.txt...\n",
            "Chunking data/output/arxiv_org_1_0.txt...\n",
            "Chunking data/output/arxiv_org_19.txt...\n",
            "Chunking data/output/arxiv_org_15_0.txt...\n",
            "Chunking data/output/arxiv_org_18.txt...\n",
            "Chunking data/output/arxiv_org.txt...\n",
            "Chunking data/output/arxiv_org_21_0.txt...\n",
            "Chunking data/output/arxiv_org_14.txt...\n",
            "Chunking data/output/arxiv_org_11_0.txt...\n",
            "Chunking data/output/arxiv_org_11.txt...\n",
            "Chunking data/output/arxiv_org_13_0.txt...\n",
            "Chunking data/output/arxiv_org_16.txt...\n",
            "\n",
            "Total chunks created: 69\n",
            "First 5 chunked filenames: ['data/output/arxiv_org_8_0.txt', 'data/output/arxiv_org_2_0_0.txt', 'data/output/arxiv_org_17_0.txt', 'data/output/arxiv_org_21_0.txt', 'data/output/arxiv_org_1_0.txt']\n"
          ]
        }
      ],
      "source": [
        "# Byte Latent Transformer: Patches Scale Better Than Tokens paper in HTML format\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# Your list of papers to process\n",
        "papers = [\n",
        "    \"https://arxiv.org/html/2411.02959v2\",\n",
        "    \"https://arxiv.org/html/2510.02312v1\",\n",
        "    \"https://arxiv.org/html/2510.02305v1\",\n",
        "    \"https://arxiv.org/html/2510.02180v1\",\n",
        "    \"https://arxiv.org/html/2509.21340v1\",\n",
        "    \"https://arxiv.org/html/2509.21346v1\",\n",
        "    \"https://arxiv.org/html/2510.02245v1\",\n",
        "    \"https://arxiv.org/html/2510.02227v1\",\n",
        "    \"https://arxiv.org/html/2510.02212v1\",\n",
        "    \"https://arxiv.org/html/2510.02140v1\",\n",
        "    \"https://arxiv.org/html/2510.01656v1\",\n",
        "    \"https://arxiv.org/html/2510.01544v1\",\n",
        "    \"https://arxiv.org/html/2510.01459v1\",\n",
        "    \"https://arxiv.org/html/2510.00911v1\",\n",
        "    \"https://arxiv.org/html/2510.00690v1\",\n",
        "    \"https://arxiv.org/html/2509.23967v1\"\n",
        "]\n",
        "\n",
        "# --- 1. Ingest all papers from the list ---\n",
        "print(f\"--- Ingesting {len(papers)} documents ---\")\n",
        "for paper_url in papers:\n",
        "    print(f\"Ingesting: {paper_url}\")\n",
        "    !synthetic-data-kit \\\n",
        "        -c synthetic_data_kit_config.yaml \\\n",
        "        ingest \"{paper_url}\"\n",
        "\n",
        "# --- 2. Find and chunk all ingested documents ---\n",
        "print(\"\\n--- Chunking all ingested documents ---\")\n",
        "all_chunk_filenames = []\n",
        "# Find all .txt files created by the ingest step\n",
        "ingested_files = glob.glob(\"data/output/*.txt\")\n",
        "\n",
        "if not ingested_files:\n",
        "    print(\"No ingested text files found in 'data/output/'.\")\n",
        "else:\n",
        "    for text_file in ingested_files:\n",
        "        print(f\"Chunking {text_file}...\")\n",
        "        # The generator will split the file and return the names of the new chunk files\n",
        "        chunk_names = generator.chunk_data(text_file)\n",
        "        all_chunk_filenames.extend(chunk_names)\n",
        "\n",
        "# --- 3. Print the final results ---\n",
        "print(f\"\\nTotal chunks created: {len(all_chunk_filenames)}\")\n",
        "print(f\"First 5 chunked filenames: {all_chunk_filenames[:5]}\")\n",
        "\n",
        "# Make the final list of chunks available to the next cells in the notebook\n",
        "filenames = all_chunk_filenames"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGdAXafV6S2M"
      },
      "source": [
        "We see around 37 chunks of data. We now call synthetic-data-kit to create some pairs of data for 3 of our chunks.\n",
        "\n",
        "You can process more chunks, but it'll be much slower!\n",
        "\n",
        "Using `--num-pairs` will generate **approximately** that many QA pairs. However it might be shorter or longer depending on the `max_seq_length` of the loaded up model. So if you specify 100, you might only get 10 since the model's max sequence length is capped."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYYYlMJ7ZtT7",
        "outputId": "b6692abe-3a34-441f-f4d1-0ce5fdc8f098"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vLLM STDOUT: INFO:     127.0.0.1:50806 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:50810 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from data/output/arxiv_org_8_0.txt...vLLM STDOUT: INFO 10-03 08:33:05 [chat_utils.py:444] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\n",
            "vLLM STDOUT: INFO 10-03 08:33:05 [logger.py:43] Received request chatcmpl-f44262d1fe184716be7115001f2f19df: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n}\\n1.00−0.0%\\\\scriptstyle{\\\\scriptstyle-0.0\\\\%}\\n76.7↑7.0%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 7.0\\\\%}\\n20613↓13.4%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 13.4\\\\%}\\n1.00−0.0%\\\\scriptstyle{\\\\scriptstyle-0.0\\\\%}\\n60.8↑8.2%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 8.2\\\\%}\\n18158↓6.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 6.8\\\\%}\\n0.91↓9.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 9.0\\\\%}\\n88.4↑6.6%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 6.6\\\\%}\\n2272↓14.6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 14.6\\\\%}\\n0.54↓46.3%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 46.3\\\\%}\\n+ GRPO\\n86.7↑7.2%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 7.2\\\\%}\\n17083↓19.7%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 19.7\\\\%}\\n1.00−0.0%\\\\scriptstyle{\\\\scriptstyle-0.0\\\\%}\\n79.17↑10.5%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 10.5\\\\%}\\n19869↓16.5%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 16.5\\\\%}\\n1.00−0.0%\\\\scriptstyle{\\\\scriptstyle-0.0\\\\%}\\n62.1↑10.6%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 10.6\\\\%}\\n18046↓7.3%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 7.3\\\\%}\\n0.93↓7.3%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 7.3\\\\%}\\n87.8↑5.9%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 5.9\\\\%}\\n2220↓16.6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 16.6\\\\%}\\n0.59↓40.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 40.8\\\\%}\\nAdaptThink\\n83.3↑3.1%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 3.1\\\\%}\\n16598↓21.9%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 21.9\\\\%}\\n0.93↓7.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 7.0\\\\%}\\n74.2↑3.5%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 3.5\\\\%}\\n19993↓16.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 16.0\\\\%}\\n0.84↓16.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 16.0\\\\%}\\n57.1↑1.6%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 1.6\\\\%}\\n16162↓17.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 17.0\\\\%}\\n0.78↓28.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 28.0\\\\%}\\n85.4↑3.0%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 3.0\\\\%}\\n915↓65.6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 65.6\\\\%}\\n0.16↓84.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 84.0\\\\%}\\nAutoThink\\n84.3↑3.5%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 3.5\\\\%}\\n17061↓19.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 19.8\\\\%}\\n0.95↓5.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 5.0\\\\%}\\n75.0↑4.6%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 4.6\\\\%}\\n18784↓21.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 21.0\\\\%}\\n0.88↓12.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 12.0\\\\%}\\n57.5↑<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:33:05 [engine.py:317] Added request chatcmpl-f44262d1fe184716be7115001f2f19df.\n",
            "\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from data/output/arxiv_org_8_0.txt...vLLM STDOUT: INFO:     127.0.0.1:50814 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:33:17 [logger.py:43] Received request chatcmpl-ffe04fc5d6af4825aefefdb0099e689e: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\n}\\n1.00−0.0%\\\\scriptstyle{\\\\scriptstyle-0.0\\\\%}\\n76.7↑7.0%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 7.0\\\\%}\\n20613↓13.4%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 13.4\\\\%}\\n1.00−0.0%\\\\scriptstyle{\\\\scriptstyle-0.0\\\\%}\\n60.8↑8.2%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 8.2\\\\%}\\n18158↓6.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 6.8\\\\%}\\n0.91↓9.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 9.0\\\\%}\\n88.4↑6.6%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 6.6\\\\%}\\n2272↓14.6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 14.6\\\\%}\\n0.54↓46.3%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 46.3\\\\%}\\n+ GRPO\\n86.7↑7.2%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 7.2\\\\%}\\n17083↓19.7%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 19.7\\\\%}\\n1.00−0.0%\\\\scriptstyle{\\\\scriptstyle-0.0\\\\%}\\n79.17↑10.5%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 10.5\\\\%}\\n19869↓16.5%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 16.5\\\\%}\\n1.00−0.0%\\\\scriptstyle{\\\\scriptstyle-0.0\\\\%}\\n62.1↑10.6%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 10.6\\\\%}\\n18046↓7.3%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 7.3\\\\%}\\n0.93↓7.3%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 7.3\\\\%}\\n87.8↑5.9%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 5.9\\\\%}\\n2220↓16.6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 16.6\\\\%}\\n0.59↓40.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 40.8\\\\%}\\nAdaptThink\\n83.3↑3.1%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 3.1\\\\%}\\n16598↓21.9%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 21.9\\\\%}\\n0.93↓7.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 7.0\\\\%}\\n74.2↑3.5%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 3.5\\\\%}\\n19993↓16.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 16.0\\\\%}\\n0.84↓16.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 16.0\\\\%}\\n57.1↑1.6%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 1.6\\\\%}\\n16162↓17.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 17.0\\\\%}\\n0.78↓28.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 28.0\\\\%}\\n85.4↑3.0%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 3.0\\\\%}\\n915↓65.6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 65.6\\\\%}\\n0.16↓84.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 84.0\\\\%}\\nAutoThink\\n84.3↑3.5%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 3.5\\\\%}\\n17061↓19.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 19.8\\\\%}\\n0.95↓5.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 5.0\\\\%}\\n75.0↑4.6%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 4.6\\\\%}\\n18784↓21.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 21.0\\\\%}\\n0.88↓12.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 12.0\\\\%}\\n57.5↑<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:33:17 [engine.py:317] Added request chatcmpl-ffe04fc5d6af4825aefefdb0099e689e.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from data/output/arxiv_org_8_0.txt...vLLM STDOUT: INFO:     127.0.0.1:42716 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 10 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/arxiv_org_8_0_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/arxiv_org_8_0_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from data/output/arxiv_org_8_0.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/arxiv_org_8_0_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:44436 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:44448 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:33:26 [logger.py:43] Received request chatcmpl-127255986fb84a80848ad9a0a1be3990: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nao et\\xa0al., 2023; Wei et\\xa0al., 2023) and R1-style \\xa0(DeepSeek-AI et\\xa0al., 2025) systems—have improved complex problem solving via explicit step-by-step reasoning and self-reflection but also suffer from “overthinking” \\xa0(Kumar et\\xa0al., 2025; Sui et\\xa0al., 2025; Nayab et\\xa0al., 2025), where simple queries trigger redundant chains that inflate compute, latency, and token usage, hindering interactive deployment. To address this, existing work focuses on: (i) Training-based adaptive reasoning: RL to conditionally trigger CoT, length penalties and conciseness rewards\\xa0(Aggarwal and Welleck, 2025; Arora and Zanette, 2025; Hou et\\xa0al., 2025; Luo et\\xa0al., 2025; Shen et\\xa0al., 2025; Team et\\xa0al., 2025; Lou et\\xa0al., 2025; Zhan et\\xa0al., 2025), and SFT\\xa0(Munkhdalai et\\xa0al., 2024; Ma et\\xa0al., 2025; Chen et\\xa0al., 2025a; Kang et\\xa0al., 2025) to prefer shorter yet correct reasoning; (ii) External control : prompt or instruction designs that limit steps or defer CoT\\xa0(Xu et\\xa0al., 2025; Renze and Guven, 2024; Chen et\\xa0al., 2024; Munkhbat et\\xa0al., 2025); (iii) Post-hoc Efficiency Optimization: pruning and restructuring chains after generation\\xa0(Aytes et\\xa0al., 2025; Xia et\\xa0al., 2025; Liu et\\xa0al., 2024b; Sun et\\xa0al., 2024; Yang et\\xa0al., 2025).\\nDespite progress, these methods still face coarse supervision, limited adaptation to hard cases due to monotonic shortening, and a lack of principled trade-offs between quality, token cost, and latency.\\n3 Method\\nOur HiPO framework consists of two important components: (i) a hybrid data construction pipeline that generates training data with both Think-on and Think-off responses; (ii) a hybrid reinforcement learning reward system that combines mode-specific accuracy and global average performance, along with a bias-adjustment mechanism to prevent over-reliance on the Think-on mode.\\n3.1 Hybrid Data Construction Pipeline\\nThis process begins with a novel data labeling system leveraging state-of-the-art LLMs to assess each query’s difficulty and domain characteristics. Queries are then classified into Think-on and Think-off categories based on their intrinsic complexity and the availability of verifiable answers.\\n3.1.1 Data Source\\nFigure 2:\\nStatistics of Data Sources.\\nWe construct a challenging corpus for code and mathematics by integrating diverse public and proprietary sources, as illustrated in Fig.\\xa02, including AM-Thinking-v1-Distilled\\xa0(Tian et\\xa0al., 2025), II-Thought-RL\\xa0(Internet, 2025), AceReason-Math\\xa0(Chen et\\xa0al., 2025b), and Skywork-OR1-RL-Data\\xa0(He et\\xa0al., 2025).\\n3.1.2 Data Collection\\nTo effectively enhance the performance of HiPO, we design a structured data construction pipeline aimed at exploring and guiding the model’s preference between the Think-on and Think-off reasoning modes. Our training dataset is meticulously curated to be logically rich, cross-domain, and sufficiently challenging.\\nWe adopt a multi-stage data generation process as shown in Figure\\xa01.\\nFor each query, the pipeline samples NN responses under the Think-on mode and NN responses under the Think-off mode using a dedicated reasoning model.\\nAll responses are then verified for correctness, and the reasoning mode with the higher pass rate is selected as the preferred mode for that query. Let ponp_{\\\\text{on}} and poffp_{\\\\text{off}} denote the pass rates of the Think-on and Think-off modes, respectively. If the difference in pass rates satisfies |pon−poff|<δ|p_{\\\\text{on}}-p_{\\\\text{off}}|<\\\\delta, where δ\\\\delta is a predefined threshold, the Think-off mode is selected. This tie-breaking strategy encourages the model to<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:33:26 [engine.py:317] Added request chatcmpl-127255986fb84a80848ad9a0a1be3990.\n",
            "\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from data/output/arxiv_org_2_0_0.txt...vLLM STDOUT: INFO:     127.0.0.1:44460 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:33:28 [logger.py:43] Received request chatcmpl-eafc0ccd9bcd4baeb8b0e64cc13b9138: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nao et\\xa0al., 2023; Wei et\\xa0al., 2023) and R1-style \\xa0(DeepSeek-AI et\\xa0al., 2025) systems—have improved complex problem solving via explicit step-by-step reasoning and self-reflection but also suffer from “overthinking” \\xa0(Kumar et\\xa0al., 2025; Sui et\\xa0al., 2025; Nayab et\\xa0al., 2025), where simple queries trigger redundant chains that inflate compute, latency, and token usage, hindering interactive deployment. To address this, existing work focuses on: (i) Training-based adaptive reasoning: RL to conditionally trigger CoT, length penalties and conciseness rewards\\xa0(Aggarwal and Welleck, 2025; Arora and Zanette, 2025; Hou et\\xa0al., 2025; Luo et\\xa0al., 2025; Shen et\\xa0al., 2025; Team et\\xa0al., 2025; Lou et\\xa0al., 2025; Zhan et\\xa0al., 2025), and SFT\\xa0(Munkhdalai et\\xa0al., 2024; Ma et\\xa0al., 2025; Chen et\\xa0al., 2025a; Kang et\\xa0al., 2025) to prefer shorter yet correct reasoning; (ii) External control : prompt or instruction designs that limit steps or defer CoT\\xa0(Xu et\\xa0al., 2025; Renze and Guven, 2024; Chen et\\xa0al., 2024; Munkhbat et\\xa0al., 2025); (iii) Post-hoc Efficiency Optimization: pruning and restructuring chains after generation\\xa0(Aytes et\\xa0al., 2025; Xia et\\xa0al., 2025; Liu et\\xa0al., 2024b; Sun et\\xa0al., 2024; Yang et\\xa0al., 2025).\\nDespite progress, these methods still face coarse supervision, limited adaptation to hard cases due to monotonic shortening, and a lack of principled trade-offs between quality, token cost, and latency.\\n3 Method\\nOur HiPO framework consists of two important components: (i) a hybrid data construction pipeline that generates training data with both Think-on and Think-off responses; (ii) a hybrid reinforcement learning reward system that combines mode-specific accuracy and global average performance, along with a bias-adjustment mechanism to prevent over-reliance on the Think-on mode.\\n3.1 Hybrid Data Construction Pipeline\\nThis process begins with a novel data labeling system leveraging state-of-the-art LLMs to assess each query’s difficulty and domain characteristics. Queries are then classified into Think-on and Think-off categories based on their intrinsic complexity and the availability of verifiable answers.\\n3.1.1 Data Source\\nFigure 2:\\nStatistics of Data Sources.\\nWe construct a challenging corpus for code and mathematics by integrating diverse public and proprietary sources, as illustrated in Fig.\\xa02, including AM-Thinking-v1-Distilled\\xa0(Tian et\\xa0al., 2025), II-Thought-RL\\xa0(Internet, 2025), AceReason-Math\\xa0(Chen et\\xa0al., 2025b), and Skywork-OR1-RL-Data\\xa0(He et\\xa0al., 2025).\\n3.1.2 Data Collection\\nTo effectively enhance the performance of HiPO, we design a structured data construction pipeline aimed at exploring and guiding the model’s preference between the Think-on and Think-off reasoning modes. Our training dataset is meticulously curated to be logically rich, cross-domain, and sufficiently challenging.\\nWe adopt a multi-stage data generation process as shown in Figure\\xa01.\\nFor each query, the pipeline samples NN responses under the Think-on mode and NN responses under the Think-off mode using a dedicated reasoning model.\\nAll responses are then verified for correctness, and the reasoning mode with the higher pass rate is selected as the preferred mode for that query. Let ponp_{\\\\text{on}} and poffp_{\\\\text{off}} denote the pass rates of the Think-on and Think-off modes, respectively. If the difference in pass rates satisfies |pon−poff|<δ|p_{\\\\text{on}}-p_{\\\\text{off}}|<\\\\delta, where δ\\\\delta is a predefined threshold, the Think-off mode is selected. This tie-breaking strategy encourages the model to<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:33:28 [engine.py:317] Added request chatcmpl-eafc0ccd9bcd4baeb8b0e64cc13b9138.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from data/output/arxiv_org_2_0_0.txt...vLLM STDOUT: INFO:     127.0.0.1:44462 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 9 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/arxiv_org_2_0_0_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/arxiv_org_2_0_0_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from data/output/arxiv_org_2_0_0.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/arxiv_org_2_0_0_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:49084 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:49094 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:33:37 [logger.py:43] Received request chatcmpl-64498701b596491d8d20d0a903404e7b: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nJiang, Weihao Gao, Weimin Xiong, Weiran He, Weixiao Huang, Weixin Xu, Wenhao Wu, Wenyang He, Xianghui Wei, Xianqing Jia, Xingzhe Wu, Xinran Xu, Xinxing Zu, Xinyu Zhou, Xuehai Pan, Y.\\xa0Charles, Yang Li, Yangyang Hu, Yangyang Liu, Yanru Chen, Yejie Wang, Yibo Liu, Yidao Qin, Yifeng Liu, Ying Yang, Yiping Bao, Yulun Du, Yuxin Wu, Yuzhi Wang, Zaida\\nZhou, Zhaoji Wang, Zhaowei Li, Zhen Zhu, Zheng Zhang, Zhexu Wang, Zhilin Yang, Zhiqi Huang, Zihao Huang, Ziyao Xu, Zonghan Yang, and Zongyu Lin.\\nKimi k1.5: Scaling reinforcement learning with llms, 2025.\\nURL https://arxiv.org/abs/2501.12599.\\nLou et\\xa0al. [2025]\\nChenwei Lou, Zewei Sun, Xinnian Liang, Meng Qu, Wei Shen, Wenqi Wang, Yuntao Li, Qingping Yang, and Shuangzhi Wu.\\nAdacot: Pareto-optimal adaptive chain-of-thought triggering via reinforcement learning, 2025.\\nURL https://arxiv.org/abs/2505.11896.\\nMunkhdalai et\\xa0al. [2024]\\nTsendsuren Munkhdalai, Manaal Faruqui, and Siddharth Gopal.\\nLeave no context behind: Efficient infinite context transformers with infini-attention.\\narXiv preprint arXiv:2404.07143, 2024.\\nMa et\\xa0al. [2025]\\nXinyin Ma, Guangnian Wan, Runpeng Yu, Gongfan Fang, and Xinchao Wang.\\nCot-valve: Length-compressible chain-of-thought tuning, 2025.\\nURL https://arxiv.org/abs/2502.09601.\\nChen et\\xa0al. [2025a]\\nXingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, and Dong Yu.\\nDo not think that much for 2+3=? on the overthinking of o1-like llms, 2025a.\\nURL https://arxiv.org/abs/2412.21187.\\nKang et\\xa0al. [2025]\\nYu\\xa0Kang, Xianghui Sun, Liangyu Chen, and Wei Zou.\\nC3ot: generating shorter chain-of-thought without compromising effectiveness.\\nIn Proceedings of the Thirty-Ninth AAAI Conference on Artificial Intelligence and Thirty-Seventh Conference on Innovative Applications of Artificial Intelligence and Fifteenth Symposium on Educational Advances in Artificial Intelligence, AAAI’25/IAAI’25/EAAI’25. AAAI Press, 2025.\\nISBN 978-1-57735-897-8.\\ndoi: 10.1609/aaai.v39i23.34608.\\nURL https://doi.org/10.1609/aaai.v39i23.34608.\\nXu et\\xa0al. [2025]\\nSilei Xu, Wenhao Xie, Lingxiao Zhao, and Pengcheng He.\\nChain of draft: Thinking faster by writing less, 2025.\\nURL https://arxiv.org/abs/2502.18600.\\nRenze and Guven [2024]\\nMatthew Renze and Erhan Guven.\\nThe benefits of a concise chain of thought on problem-solving in large language models.\\nIn 2024 2nd International Conference on Foundation and Large Language Models (FLLM), page 476–483. IEEE, November 2024.\\ndoi: 10.1109/fllm63129.2024.10852493.\\nURL http://dx.doi.org/10.1109/FLLM63129.2024.10852493.\\nChen et\\xa0al. [2024]\\nQiguang Chen, Lib<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:33:37 [engine.py:317] Added request chatcmpl-64498701b596491d8d20d0a903404e7b.\n",
            "\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from data/output/arxiv_org_17_0.txt...vLLM STDOUT: INFO:     127.0.0.1:49096 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[32m⠧\u001b[0m Generating qa content from data/output/arxiv_org_17_0.txt...vLLM STDOUT: INFO 10-03 08:33:40 [logger.py:43] Received request chatcmpl-ba4e271d26f34e9e97f83409e83d4b0f: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\n Jiang, Weihao Gao, Weimin Xiong, Weiran He, Weixiao Huang, Weixin Xu, Wenhao Wu, Wenyang He, Xianghui Wei, Xianqing Jia, Xingzhe Wu, Xinran Xu, Xinxing Zu, Xinyu Zhou, Xuehai Pan, Y.\\xa0Charles, Yang Li, Yangyang Hu, Yangyang Liu, Yanru Chen, Yejie Wang, Yibo Liu, Yidao Qin, Yifeng Liu, Ying Yang, Yiping Bao, Yulun Du, Yuxin Wu, Yuzhi Wang, Zaida\\nZhou, Zhaoji Wang, Zhaowei Li, Zhen Zhu, Zheng Zhang, Zhexu Wang, Zhilin Yang, Zhiqi Huang, Zihao Huang, Ziyao Xu, Zonghan Yang, and Zongyu Lin.\\nKimi k1.5: Scaling reinforcement learning with llms, 2025.\\nURL https://arxiv.org/abs/2501.12599.\\nLou et\\xa0al. [2025]\\nChenwei Lou, Zewei Sun, Xinnian Liang, Meng Qu, Wei Shen, Wenqi Wang, Yuntao Li, Qingping Yang, and Shuangzhi Wu.\\nAdacot: Pareto-optimal adaptive chain-of-thought triggering via reinforcement learning, 2025.\\nURL https://arxiv.org/abs/2505.11896.\\nMunkhdalai et\\xa0al. [2024]\\nTsendsuren Munkhdalai, Manaal Faruqui, and Siddharth Gopal.\\nLeave no context behind: Efficient infinite context transformers with infini-attention.\\narXiv preprint arXiv:2404.07143, 2024.\\nMa et\\xa0al. [2025]\\nXinyin Ma, Guangnian Wan, Runpeng Yu, Gongfan Fang, and Xinchao Wang.\\nCot-valve: Length-compressible chain-of-thought tuning, 2025.\\nURL https://arxiv.org/abs/2502.09601.\\nChen et\\xa0al. [2025a]\\nXingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, and Dong Yu.\\nDo not think that much for 2+3=? on the overthinking of o1-like llms, 2025a.\\nURL https://arxiv.org/abs/2412.21187.\\nKang et\\xa0al. [2025]\\nYu\\xa0Kang, Xianghui Sun, Liangyu Chen, and Wei Zou.\\nC3ot: generating shorter chain-of-thought without compromising effectiveness.\\nIn Proceedings of the Thirty-Ninth AAAI Conference on Artificial Intelligence and Thirty-Seventh Conference on Innovative Applications of Artificial Intelligence and Fifteenth Symposium on Educational Advances in Artificial Intelligence, AAAI’25/IAAI’25/EAAI’25. AAAI Press, 2025.\\nISBN 978-1-57735-897-8.\\ndoi: 10.1609/aaai.v39i23.34608.\\nURL https://doi.org/10.1609/aaai.v39i23.34608.\\nXu et\\xa0al. [2025]\\nSilei Xu, Wenhao Xie, Lingxiao Zhao, and Pengcheng He.\\nChain of draft: Thinking faster by writing less, 2025.\\nURL https://arxiv.org/abs/2502.18600.\\nRenze and Guven [2024]\\nMatthew Renze and Erhan Guven.\\nThe benefits of a concise chain of thought on problem-solving in large language models.\\nIn 2024 2nd International Conference on Foundation and Large Language Models (FLLM), page 476–483. IEEE, November 2024.\\ndoi: 10.1109/fllm63129.2024.10852493.\\nURL http://dx.doi.org/10.1109/FLLM63129.2024.10852493.\\nChen et\\xa0al. [2024]\\nQiguang Chen, Lib<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:33:40 [engine.py:317] Added request chatcmpl-ba4e271d26f34e9e97f83409e83d4b0f.\n",
            "\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from data/output/arxiv_org_17_0.txt...vLLM STDOUT: INFO:     127.0.0.1:49112 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 9 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/arxiv_org_17_0_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/arxiv_org_17_0_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from data/output/arxiv_org_17_0.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/arxiv_org_17_0_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:57992 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:57996 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:33:49 [logger.py:43] Received request chatcmpl-4c60110ba83244e29594045dd01b16df: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\njie Wang, Peng Cheng, Xiaoyu Zhang, Fuxiang Zhang, Jiacheng Xu, Wei Shen, Siyuan Li, Liang Zeng, Tianwen Wei, Cheng Cheng, Yang Liu, and Yahui Zhou.\\nSkywork open reasoner series.\\nhttps://capricious-hydrogen-41c.notion.site/Skywork-Open-Reaonser-Series-1d0bc9ae823a80459b46c149e4f51680, 2025.\\nNotion Blog.\\nZhang et\\xa0al. [2025]\\nJiajie Zhang, Nianyi Lin, Lei Hou, Ling Feng, and Juanzi Li.\\nAdaptthink: Reasoning models can learn when to think, 2025.\\nURL https://arxiv.org/abs/2505.13417.\\nTu et\\xa0al. [2025]\\nSongjun Tu, Jiahao Lin, Qichao Zhang, Xiangyu Tian, Linjing Li, Xiangyuan Lan, and Dongbin Zhao.\\nLearning when to think: Shaping adaptive reasoning in r1-style models via multi-stage rl, 2025.\\nURL https://arxiv.org/abs/2505.10832.\\nChen et\\xa0al. [2021]\\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique\\xa0Ponde de\\xa0Oliveira\\xa0Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe\\xa0Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William\\xa0Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew\\xa0N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba.\\nEvaluating large language models trained on code, 2021.\\nURL https://arxiv.org/abs/2107.03374.\\nJain et\\xa0al. [2024]\\nNaman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica.\\nLivecodebench: Holistic and contamination free evaluation of large language models for code, 2024.\\nURL https://arxiv.org/abs/2403.07974.\\nAustin et\\xa0al. [2021]\\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton.\\nProgram synthesis with large language models, 2021.\\nURL https://arxiv.org/abs/2108.07732.\\nLightman et\\xa0al. [2023]\\nHunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe.\\nLet’s verify step by step, 2023.\\nURL https://arxiv.org/abs/2305.20050.\\nRein et\\xa0al. [2023]\\nDavid Rein, Betty\\xa0Li Hou, Asa\\xa0Cooper Stickland, Jackson Petty, Richard\\xa0Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel\\xa0R. Bowman.\\nGpqa: A graduate-level google-proof q&a benchmark.\\nArXiv, abs/2311.12022, 2023.\\nURL https://api.semanticscholar.org/CorpusID:265295009<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:33:49 [engine.py:317] Added request chatcmpl-4c60110ba83244e29594045dd01b16df.\n",
            "\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from data/output/arxiv_org_21_0.txt...vLLM STDOUT: INFO:     127.0.0.1:58010 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:33:53 [logger.py:43] Received request chatcmpl-4c5cf18346f544aabb68f287b576a9f3: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\njie Wang, Peng Cheng, Xiaoyu Zhang, Fuxiang Zhang, Jiacheng Xu, Wei Shen, Siyuan Li, Liang Zeng, Tianwen Wei, Cheng Cheng, Yang Liu, and Yahui Zhou.\\nSkywork open reasoner series.\\nhttps://capricious-hydrogen-41c.notion.site/Skywork-Open-Reaonser-Series-1d0bc9ae823a80459b46c149e4f51680, 2025.\\nNotion Blog.\\nZhang et\\xa0al. [2025]\\nJiajie Zhang, Nianyi Lin, Lei Hou, Ling Feng, and Juanzi Li.\\nAdaptthink: Reasoning models can learn when to think, 2025.\\nURL https://arxiv.org/abs/2505.13417.\\nTu et\\xa0al. [2025]\\nSongjun Tu, Jiahao Lin, Qichao Zhang, Xiangyu Tian, Linjing Li, Xiangyuan Lan, and Dongbin Zhao.\\nLearning when to think: Shaping adaptive reasoning in r1-style models via multi-stage rl, 2025.\\nURL https://arxiv.org/abs/2505.10832.\\nChen et\\xa0al. [2021]\\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique\\xa0Ponde de\\xa0Oliveira\\xa0Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe\\xa0Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William\\xa0Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew\\xa0N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba.\\nEvaluating large language models trained on code, 2021.\\nURL https://arxiv.org/abs/2107.03374.\\nJain et\\xa0al. [2024]\\nNaman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica.\\nLivecodebench: Holistic and contamination free evaluation of large language models for code, 2024.\\nURL https://arxiv.org/abs/2403.07974.\\nAustin et\\xa0al. [2021]\\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton.\\nProgram synthesis with large language models, 2021.\\nURL https://arxiv.org/abs/2108.07732.\\nLightman et\\xa0al. [2023]\\nHunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe.\\nLet’s verify step by step, 2023.\\nURL https://arxiv.org/abs/2305.20050.\\nRein et\\xa0al. [2023]\\nDavid Rein, Betty\\xa0Li Hou, Asa\\xa0Cooper Stickland, Jackson Petty, Richard\\xa0Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel\\xa0R. Bowman.\\nGpqa: A graduate-level google-proof q&a benchmark.\\nArXiv, abs/2311.12022, 2023.\\nURL https://api.semanticscholar.org/CorpusID:265295009<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:33:53 [engine.py:317] Added request chatcmpl-4c5cf18346f544aabb68f287b576a9f3.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from data/output/arxiv_org_21_0.txt...vLLM STDOUT: INFO:     127.0.0.1:46688 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 7 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/arxiv_org_21_0_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/arxiv_org_21_0_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from data/output/arxiv_org_21_0.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/arxiv_org_21_0_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:46702 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:46716 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:34:02 [logger.py:43] Received request chatcmpl-a86b359a3da447aabfa37716501b59f2: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n) training-based adaptive reasoning, where reinforcement learning (RL)\\xa0(Aggarwal and Welleck, 2025; Arora and Zanette, 2025; Hou et\\xa0al., 2025; Luo et\\xa0al., 2025; Shen et\\xa0al., 2025; Team et\\xa0al., 2025; Lou et\\xa0al., 2025) or supervised fine-tuning (SFT)\\xa0(Munkhdalai et\\xa0al., 2024; Ma et\\xa0al., 2025; Chen et\\xa0al., 2025a; Kang et\\xa0al., 2025) encourages concise reasoning through length penalties or conciseness rewards; (ii) external control, which constrains reasoning with handcrafted prompts or dynamic instructions\\xa0(Xu et\\xa0al., 2025; Renze and Guven, 2024; Chen et\\xa0al., 2024; Munkhbat et\\xa0al., 2025). While effective to some extent, these methods suffer from important limitations: coarse supervision signals, monotonic incentives that discourage deeper reasoning on difficult problems, and a lack of principled trade-offs between accuracy, latency, and token efficiency.\\nTo address these challenges, we introduce HiPO (Hybrid Policy Optimization), a unified framework for adaptive reasoning in LLMs. HiPO is designed to enable models to decide when to “think” (i.e., Think-on)and when to skip reasoning (i.e., Think-off), thereby striking a balance between correctness and efficiency. Specifically, our approach builds on two key innovations:\\n(1) Hybrid Data Construction Pipeline. As shown in Figure\\xa01, we first collect the training data containing both Think-on and Think-off responses. Each query is automatically categorized based on its difficulty and response correctness. Then, a high-performance model (i.e., DeepSeek-V3\\xa0(Liu et\\xa0al., 2024a)) is used to produce the explicit explanations to justify its reasoning-mode decisions. Finally, for each query, the final response based on the thinking mode and the corresponding explanation construct the hybrid output.\\n(2) Hybrid Reinforcement Learning Reward System. We propose a hybrid reward design that balances Think-on and Think-off decisions. Specifically, a bias adjustment mechanism prevents the model from over-relying on verbose reasoning, while mode-aware advantage functions align reasoning-mode selection with actual performance gains. This ensures stable training and principled control over reasoning depth.\\nIn summary, our contributions are threefold:\\n•\\nWe propose HiPO for adaptive LLM reasoning, which mainly includes the hybrid data construction and hybrid reinforcement learning.\\n•\\nIn the hybrid data construction pipeline, we produce logically rich Think-on and concise Think-off responses with the justification for the thinking mode. Then, for hybrid reinforcement learning, we introduce both the judge analysis and the response reward signal to enable principled control of reasoning depth.\\n•\\nExperimental results on multiple datasets demonstrate that HiPO can consistently reduce redundant reasoning while improving or maintaining accuracy.\\nFigure 1: Framework of the hybrid data construction pipeline.\\n2 Related Works\\nRL for LLM Reasoning.\\nRecent advances in reinforcement learning (RL) have significantly enhanced LLMs’ complex reasoning capabilities, moving beyond supervised fine-tuning (SFT) limitations. State-of-the-art RL algorithms demonstrate superior performance in mathematical reasoning and multi-step problem solving: GRPO \\xa0(Shao et\\xa0al., 2024) stabilizes training through intra-group relative reward comparisons; GSPO \\xa0(Zheng et\\xa0al., 2025) defines sequence-level importance ratios and applies sequence-level clipping/rewarding/updates to improve efficiency and stabilize MoE training; VAPO \\xa0(Yue et\\xa0al., 2025) ensures reward consistency via value-aware optimization; PPO \\xa0(Schulman et\\xa0al., 2017) constrains policy updates through clipping mechanisms; and DPO \\xa0(Rafailov et\\xa0al., 2024) learns directly from human preferences without explicit reward modeling.\\nAdaptive Reasoning.\\nReasoning-oriented large language models—exemplified by Chain-of-Thought (CoT) \\xa0(Yao et\\xa0al., 2023; Wei et\\xa0al., 2023) and R1-style \\xa0(DeepSeek-AI et\\xa0al., 2025) systems—have improved complex problem solving via explicit step-by-step reasoning and self-reflection but also suffer from “overthinking”<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:34:02 [engine.py:317] Added request chatcmpl-a86b359a3da447aabfa37716501b59f2.\n",
            "\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from data/output/arxiv_org_1_0.txt...vLLM STDOUT: INFO:     127.0.0.1:46724 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from data/output/arxiv_org_1_0.txt...vLLM STDOUT: INFO 10-03 08:34:04 [logger.py:43] Received request chatcmpl-538924d9e45949169b0fa091d8b36f6f: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\n) training-based adaptive reasoning, where reinforcement learning (RL)\\xa0(Aggarwal and Welleck, 2025; Arora and Zanette, 2025; Hou et\\xa0al., 2025; Luo et\\xa0al., 2025; Shen et\\xa0al., 2025; Team et\\xa0al., 2025; Lou et\\xa0al., 2025) or supervised fine-tuning (SFT)\\xa0(Munkhdalai et\\xa0al., 2024; Ma et\\xa0al., 2025; Chen et\\xa0al., 2025a; Kang et\\xa0al., 2025) encourages concise reasoning through length penalties or conciseness rewards; (ii) external control, which constrains reasoning with handcrafted prompts or dynamic instructions\\xa0(Xu et\\xa0al., 2025; Renze and Guven, 2024; Chen et\\xa0al., 2024; Munkhbat et\\xa0al., 2025). While effective to some extent, these methods suffer from important limitations: coarse supervision signals, monotonic incentives that discourage deeper reasoning on difficult problems, and a lack of principled trade-offs between accuracy, latency, and token efficiency.\\nTo address these challenges, we introduce HiPO (Hybrid Policy Optimization), a unified framework for adaptive reasoning in LLMs. HiPO is designed to enable models to decide when to “think” (i.e., Think-on)and when to skip reasoning (i.e., Think-off), thereby striking a balance between correctness and efficiency. Specifically, our approach builds on two key innovations:\\n(1) Hybrid Data Construction Pipeline. As shown in Figure\\xa01, we first collect the training data containing both Think-on and Think-off responses. Each query is automatically categorized based on its difficulty and response correctness. Then, a high-performance model (i.e., DeepSeek-V3\\xa0(Liu et\\xa0al., 2024a)) is used to produce the explicit explanations to justify its reasoning-mode decisions. Finally, for each query, the final response based on the thinking mode and the corresponding explanation construct the hybrid output.\\n(2) Hybrid Reinforcement Learning Reward System. We propose a hybrid reward design that balances Think-on and Think-off decisions. Specifically, a bias adjustment mechanism prevents the model from over-relying on verbose reasoning, while mode-aware advantage functions align reasoning-mode selection with actual performance gains. This ensures stable training and principled control over reasoning depth.\\nIn summary, our contributions are threefold:\\n•\\nWe propose HiPO for adaptive LLM reasoning, which mainly includes the hybrid data construction and hybrid reinforcement learning.\\n•\\nIn the hybrid data construction pipeline, we produce logically rich Think-on and concise Think-off responses with the justification for the thinking mode. Then, for hybrid reinforcement learning, we introduce both the judge analysis and the response reward signal to enable principled control of reasoning depth.\\n•\\nExperimental results on multiple datasets demonstrate that HiPO can consistently reduce redundant reasoning while improving or maintaining accuracy.\\nFigure 1: Framework of the hybrid data construction pipeline.\\n2 Related Works\\nRL for LLM Reasoning.\\nRecent advances in reinforcement learning (RL) have significantly enhanced LLMs’ complex reasoning capabilities, moving beyond supervised fine-tuning (SFT) limitations. State-of-the-art RL algorithms demonstrate superior performance in mathematical reasoning and multi-step problem solving: GRPO \\xa0(Shao et\\xa0al., 2024) stabilizes training through intra-group relative reward comparisons; GSPO \\xa0(Zheng et\\xa0al., 2025) defines sequence-level importance ratios and applies sequence-level clipping/rewarding/updates to improve efficiency and stabilize MoE training; VAPO \\xa0(Yue et\\xa0al., 2025) ensures reward consistency via value-aware optimization; PPO \\xa0(Schulman et\\xa0al., 2017) constrains policy updates through clipping mechanisms; and DPO \\xa0(Rafailov et\\xa0al., 2024) learns directly from human preferences without explicit reward modeling.\\nAdaptive Reasoning.\\nReasoning-oriented large language models—exemplified by Chain-of-Thought (CoT) \\xa0(Yao et\\xa0al., 2023; Wei et\\xa0al., 2023) and R1-style \\xa0(DeepSeek-AI et\\xa0al., 2025) systems—have improved complex problem solving via explicit step-by-step reasoning and self-reflection but also suffer from “overthinking”<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:34:04 [engine.py:317] Added request chatcmpl-538924d9e45949169b0fa091d8b36f6f.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from data/output/arxiv_org_1_0.txt...vLLM STDOUT: INFO:     127.0.0.1:40544 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 8 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/arxiv_org_1_0_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/arxiv_org_1_0_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from data/output/arxiv_org_1_0.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/arxiv_org_1_0_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:53642 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:53654 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:34:14 [logger.py:43] Received request chatcmpl-c6181938fcdf48518ea20f3b4bced325: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nfor training. The parameters are set as: batch size = 16, maximum response length = 32k, NN = 16, ω\\\\omega = 0.01, and γ=0.3\\\\gamma=0.3.\\nBaselines.\\nTo demonstrate the effect of mitigating overthinking, we designed the following baselines for comparison.\\n(1) Cold-Start: We perform Cold-Star on the model using the data construction method described in Section\\xa03.1.\\n(2) Cold-Start (On): We apply the same Cold-Star procedure as in Section\\xa03.1, but only include the data collected under the Think-on mode.\\n(3) Cold-Start (On) + GRPO: We further train the Cold-Start (On) model using the GRPO algorithm.\\n(4) Cold-Start + GRPO: We further train the Cold-Start model with the GRPO algorithm.\\n(5) HiPO: We train the model following our HiPO.\\n(6) AdaptThink: We reproduced the code provided in (Zhang et\\xa0al., 2025).\\n(7) AutoThink: We reproduced the code provided in (Tu et\\xa0al., 2025).\\nEvaluation benchmarks.\\nWe conducted tests on AIME2024, AIME2025, HumanEval\\xa0(Chen et\\xa0al., 2021), LiveCodeBench V6\\xa0(Jain et\\xa0al., 2024), MBPP\\xa0(Austin et\\xa0al., 2021),\\nMATH-500\\xa0(Lightman et\\xa0al., 2023), and GPQA-Diamond\\xa0(Rein et\\xa0al., 2023).\\n4.2 Main Results\\nAIME2024\\nAIME2025\\nLiveCodeBench\\nHumanEval\\nMethod\\nAcc↑\\\\uparrow\\nLength↓\\\\downarrow\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}↓\\\\downarrow\\nAcc↑\\\\uparrow\\nLength↓\\\\downarrow\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}↓\\\\downarrow\\nAcc↑\\\\uparrow\\nLength↓\\\\downarrow\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}↓\\\\downarrow\\nAcc↑\\\\uparrow\\nLength↓\\\\downarrow\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}↓\\\\downarrow\\nCold-Start (on)\\n80.8\\n21265\\n1.00\\n71.7\\n23791\\n1.00\\n56.2\\n19473\\n1.00\\n82.9\\n2662\\n1.00\\n+ GRPO\\n82.5↑2.1%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 2.1\\\\%}\\n21045↓1.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 1.0\\\\%}\\n1.00−0.0%\\\\scriptstyle{\\\\scriptstyle-0.0\\\\%}\\n76.7↑7%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 7\\\\%}\\n22695↓4.6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 4.6\\\\%}\\n1.00−0.0%\\\\scriptstyle{\\\\scriptstyle-0.0\\\\%}\\n57.3↑2.0%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 2.0\\\\%}\\n19067↓2.1%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 2.1\\\\%}\\n1.00−0.0%\\\\scriptstyle{\\\\scriptstyle-0.0\\\\%}\\n95.1↑14.7%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 14.7\\\\%}\\n3597↑35.1%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 35.1\\\\%}\\n1.00−0.0%\\\\scriptstyle{\\\\scriptstyle-0.0\\\\%}\\nCold-Start\\n85.8↑6.2%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 6.2\\\\%}\\n18138↓14.7%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 14.7\\\\%}\\n1.00−0.0%\\\\scriptstyle{\\\\scriptstyle-0.0\\\\%}\\n76.7↑7.0%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 7.0\\\\%}\\n20613↓13.4%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 13.4\\\\<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:34:14 [engine.py:317] Added request chatcmpl-c6181938fcdf48518ea20f3b4bced325.\n",
            "\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from data/output/arxiv_org_7_0.txt...vLLM STDOUT: INFO:     127.0.0.1:53666 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:34:16 [logger.py:43] Received request chatcmpl-abdce2fd598142679fc7b0e85394f3ca: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\n for training. The parameters are set as: batch size = 16, maximum response length = 32k, NN = 16, ω\\\\omega = 0.01, and γ=0.3\\\\gamma=0.3.\\nBaselines.\\nTo demonstrate the effect of mitigating overthinking, we designed the following baselines for comparison.\\n(1) Cold-Start: We perform Cold-Star on the model using the data construction method described in Section\\xa03.1.\\n(2) Cold-Start (On): We apply the same Cold-Star procedure as in Section\\xa03.1, but only include the data collected under the Think-on mode.\\n(3) Cold-Start (On) + GRPO: We further train the Cold-Start (On) model using the GRPO algorithm.\\n(4) Cold-Start + GRPO: We further train the Cold-Start model with the GRPO algorithm.\\n(5) HiPO: We train the model following our HiPO.\\n(6) AdaptThink: We reproduced the code provided in (Zhang et\\xa0al., 2025).\\n(7) AutoThink: We reproduced the code provided in (Tu et\\xa0al., 2025).\\nEvaluation benchmarks.\\nWe conducted tests on AIME2024, AIME2025, HumanEval\\xa0(Chen et\\xa0al., 2021), LiveCodeBench V6\\xa0(Jain et\\xa0al., 2024), MBPP\\xa0(Austin et\\xa0al., 2021),\\nMATH-500\\xa0(Lightman et\\xa0al., 2023), and GPQA-Diamond\\xa0(Rein et\\xa0al., 2023).\\n4.2 Main Results\\nAIME2024\\nAIME2025\\nLiveCodeBench\\nHumanEval\\nMethod\\nAcc↑\\\\uparrow\\nLength↓\\\\downarrow\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}↓\\\\downarrow\\nAcc↑\\\\uparrow\\nLength↓\\\\downarrow\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}↓\\\\downarrow\\nAcc↑\\\\uparrow\\nLength↓\\\\downarrow\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}↓\\\\downarrow\\nAcc↑\\\\uparrow\\nLength↓\\\\downarrow\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}↓\\\\downarrow\\nCold-Start (on)\\n80.8\\n21265\\n1.00\\n71.7\\n23791\\n1.00\\n56.2\\n19473\\n1.00\\n82.9\\n2662\\n1.00\\n+ GRPO\\n82.5↑2.1%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 2.1\\\\%}\\n21045↓1.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 1.0\\\\%}\\n1.00−0.0%\\\\scriptstyle{\\\\scriptstyle-0.0\\\\%}\\n76.7↑7%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 7\\\\%}\\n22695↓4.6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 4.6\\\\%}\\n1.00−0.0%\\\\scriptstyle{\\\\scriptstyle-0.0\\\\%}\\n57.3↑2.0%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 2.0\\\\%}\\n19067↓2.1%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 2.1\\\\%}\\n1.00−0.0%\\\\scriptstyle{\\\\scriptstyle-0.0\\\\%}\\n95.1↑14.7%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 14.7\\\\%}\\n3597↑35.1%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 35.1\\\\%}\\n1.00−0.0%\\\\scriptstyle{\\\\scriptstyle-0.0\\\\%}\\nCold-Start\\n85.8↑6.2%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 6.2\\\\%}\\n18138↓14.7%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 14.7\\\\%}\\n1.00−0.0%\\\\scriptstyle{\\\\scriptstyle-0.0\\\\%}\\n76.7↑7.0%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 7.0\\\\%}\\n20613↓13.4%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 13.4\\\\<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:34:16 [engine.py:317] Added request chatcmpl-abdce2fd598142679fc7b0e85394f3ca.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from data/output/arxiv_org_7_0.txt...vLLM STDOUT: INFO:     127.0.0.1:53672 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 16 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/arxiv_org_7_0_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/arxiv_org_7_0_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from data/output/arxiv_org_7_0.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/arxiv_org_7_0_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:52298 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:52302 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:34:25 [logger.py:43] Received request chatcmpl-a269fd3721fe483b88ce503162e0dc95: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nJiang, Weihao Gao, Weimin Xiong, Weiran He, Weixiao Huang, Weixin Xu, Wenhao Wu, Wenyang He, Xianghui Wei, Xianqing Jia, Xingzhe Wu, Xinran Xu, Xinxing Zu, Xinyu Zhou, Xuehai Pan, Y.\\xa0Charles, Yang Li, Yangyang Hu, Yangyang Liu, Yanru Chen, Yejie Wang, Yibo Liu, Yidao Qin, Yifeng Liu, Ying Yang, Yiping Bao, Yulun Du, Yuxin Wu, Yuzhi Wang, Zaida\\nZhou, Zhaoji Wang, Zhaowei Li, Zhen Zhu, Zheng Zhang, Zhexu Wang, Zhilin Yang, Zhiqi Huang, Zihao Huang, Ziyao Xu, Zonghan Yang, and Zongyu Lin.\\nKimi k1.5: Scaling reinforcement learning with llms, 2025.\\nURL https://arxiv.org/abs/2501.12599.\\nLou et\\xa0al. [2025]\\nChenwei Lou, Zewei Sun, Xinnian Liang, Meng Qu, Wei Shen, Wenqi Wang, Yuntao Li, Qingping Yang, and Shuangzhi Wu.\\nAdacot: Pareto-optimal adaptive chain-of-thought triggering via reinforcement learning, 2025.\\nURL https://arxiv.org/abs/2505.11896.\\nMunkhdalai et\\xa0al. [2024]\\nTsendsuren Munkhdalai, Manaal Faruqui, and Siddharth Gopal.\\nLeave no context behind: Efficient infinite context transformers with infini-attention.\\narXiv preprint arXiv:2404.07143, 2024.\\nMa et\\xa0al. [2025]\\nXinyin Ma, Guangnian Wan, Runpeng Yu, Gongfan Fang, and Xinchao Wang.\\nCot-valve: Length-compressible chain-of-thought tuning, 2025.\\nURL https://arxiv.org/abs/2502.09601.\\nChen et\\xa0al. [2025a]\\nXingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, and Dong Yu.\\nDo not think that much for 2+3=? on the overthinking of o1-like llms, 2025a.\\nURL https://arxiv.org/abs/2412.21187.\\nKang et\\xa0al. [2025]\\nYu\\xa0Kang, Xianghui Sun, Liangyu Chen, and Wei Zou.\\nC3ot: generating shorter chain-of-thought without compromising effectiveness.\\nIn Proceedings of the Thirty-Ninth AAAI Conference on Artificial Intelligence and Thirty-Seventh Conference on Innovative Applications of Artificial Intelligence and Fifteenth Symposium on Educational Advances in Artificial Intelligence, AAAI’25/IAAI’25/EAAI’25. AAAI Press, 2025.\\nISBN 978-1-57735-897-8.\\ndoi: 10.1609/aaai.v39i23.34608.\\nURL https://doi.org/10.1609/aaai.v39i23.34608.\\nXu et\\xa0al. [2025]\\nSilei Xu, Wenhao Xie, Lingxiao Zhao, and Pengcheng He.\\nChain of draft: Thinking faster by writing less, 2025.\\nURL https://arxiv.org/abs/2502.18600.\\nRenze and Guven [2024]\\nMatthew Renze and Erhan Guven.\\nThe benefits of a concise chain of thought on problem-solving in large language models.\\nIn 2024 2nd International Conference on Foundation and Large Language Models (FLLM), page 476–483. IEEE, November 2024.\\ndoi: 10.1109/fllm63129.2024.10852493.\\nURL http://dx.doi.org/10.1109/FLLM63129.2024.10852493.\\nChen et\\xa0al. [2024]\\nQiguang Chen, Lib<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:34:25 [engine.py:317] Added request chatcmpl-a269fd3721fe483b88ce503162e0dc95.\n",
            "\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from data/output/arxiv_org_17_0_0.txt...vLLM STDOUT: INFO:     127.0.0.1:52304 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:34:28 [logger.py:43] Received request chatcmpl-49b86a0921574b47b327fe08e2018e1f: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\n Jiang, Weihao Gao, Weimin Xiong, Weiran He, Weixiao Huang, Weixin Xu, Wenhao Wu, Wenyang He, Xianghui Wei, Xianqing Jia, Xingzhe Wu, Xinran Xu, Xinxing Zu, Xinyu Zhou, Xuehai Pan, Y.\\xa0Charles, Yang Li, Yangyang Hu, Yangyang Liu, Yanru Chen, Yejie Wang, Yibo Liu, Yidao Qin, Yifeng Liu, Ying Yang, Yiping Bao, Yulun Du, Yuxin Wu, Yuzhi Wang, Zaida\\nZhou, Zhaoji Wang, Zhaowei Li, Zhen Zhu, Zheng Zhang, Zhexu Wang, Zhilin Yang, Zhiqi Huang, Zihao Huang, Ziyao Xu, Zonghan Yang, and Zongyu Lin.\\nKimi k1.5: Scaling reinforcement learning with llms, 2025.\\nURL https://arxiv.org/abs/2501.12599.\\nLou et\\xa0al. [2025]\\nChenwei Lou, Zewei Sun, Xinnian Liang, Meng Qu, Wei Shen, Wenqi Wang, Yuntao Li, Qingping Yang, and Shuangzhi Wu.\\nAdacot: Pareto-optimal adaptive chain-of-thought triggering via reinforcement learning, 2025.\\nURL https://arxiv.org/abs/2505.11896.\\nMunkhdalai et\\xa0al. [2024]\\nTsendsuren Munkhdalai, Manaal Faruqui, and Siddharth Gopal.\\nLeave no context behind: Efficient infinite context transformers with infini-attention.\\narXiv preprint arXiv:2404.07143, 2024.\\nMa et\\xa0al. [2025]\\nXinyin Ma, Guangnian Wan, Runpeng Yu, Gongfan Fang, and Xinchao Wang.\\nCot-valve: Length-compressible chain-of-thought tuning, 2025.\\nURL https://arxiv.org/abs/2502.09601.\\nChen et\\xa0al. [2025a]\\nXingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, and Dong Yu.\\nDo not think that much for 2+3=? on the overthinking of o1-like llms, 2025a.\\nURL https://arxiv.org/abs/2412.21187.\\nKang et\\xa0al. [2025]\\nYu\\xa0Kang, Xianghui Sun, Liangyu Chen, and Wei Zou.\\nC3ot: generating shorter chain-of-thought without compromising effectiveness.\\nIn Proceedings of the Thirty-Ninth AAAI Conference on Artificial Intelligence and Thirty-Seventh Conference on Innovative Applications of Artificial Intelligence and Fifteenth Symposium on Educational Advances in Artificial Intelligence, AAAI’25/IAAI’25/EAAI’25. AAAI Press, 2025.\\nISBN 978-1-57735-897-8.\\ndoi: 10.1609/aaai.v39i23.34608.\\nURL https://doi.org/10.1609/aaai.v39i23.34608.\\nXu et\\xa0al. [2025]\\nSilei Xu, Wenhao Xie, Lingxiao Zhao, and Pengcheng He.\\nChain of draft: Thinking faster by writing less, 2025.\\nURL https://arxiv.org/abs/2502.18600.\\nRenze and Guven [2024]\\nMatthew Renze and Erhan Guven.\\nThe benefits of a concise chain of thought on problem-solving in large language models.\\nIn 2024 2nd International Conference on Foundation and Large Language Models (FLLM), page 476–483. IEEE, November 2024.\\ndoi: 10.1109/fllm63129.2024.10852493.\\nURL http://dx.doi.org/10.1109/FLLM63129.2024.10852493.\\nChen et\\xa0al. [2024]\\nQiguang Chen, Lib<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:34:28 [engine.py:317] Added request chatcmpl-49b86a0921574b47b327fe08e2018e1f.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from data/output/arxiv_org_17_0_0.txt...vLLM STDOUT: INFO:     127.0.0.1:52316 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 6 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/arxiv_org_17_0_0_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/arxiv_org_17_0_0_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from data/output/arxiv_org_17_0_0.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/arxiv_org_17_0_0_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:44406 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:44416 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:34:37 [logger.py:43] Received request chatcmpl-34d3a30b5e454835971d540643f04d4a: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang.\\nDeepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025.\\nURL https://arxiv.org/abs/2501.12948.\\nZhan et\\xa0al. [2025]\\nZizheng Zhan, Ken Deng, Huaixi Tang, Wen Xiang, Kun Wu, Weihao Li, Wenqiang Zhu, Jingxuan Xu, Lecheng Huang, Zongxian Feng, Shaojie Wang, Shangpeng Yan, Xuxing Chen, Jiaheng Liu, Zhongyuan Peng, Zuchen Gao, Haoyang Huang, Xiaojiang Zhang, Jinghui Wang, Zheng Lin, Mengtong Li, Huiming Wang, Ziqi Zhan, Yanan Wu, Yuanxing Zhang, Jian Yang, Guang Chen, Haotian Zhang, Bin Chen, and Bing Yu.\\nKat-v1: Kwai-autothink technical report, 2025.\\nURL https://arxiv.org/abs/2507.08297.\\nAytes et\\xa0al. [2025]\\nSimon\\xa0A. Aytes, Jinheon Baek, and Sung\\xa0Ju Hwang.\\nSketch-of-thought: Efficient llm reasoning with adaptive cognitive-inspired sketching, 2025.\\nURL https://arxiv.org/abs/2503.05179.\\nXia et\\xa0al. [2025]\\nHeming Xia, Chak\\xa0Tou Leong, Wenjie Wang, Yongqi Li, and Wenjie Li.\\nTokenskip: Controllable chain-of-thought compression in llms, 2025.\\nURL https://arxiv.org/abs/2502.12067.\\nLiu et\\xa0al. [2024b]\\nTengxiao Liu, Qipeng Guo, Xiangkun Hu, Cheng Jiayang, Yue Zhang, Xipeng Qiu, and Zheng Zhang.\\nCan language models learn to skip steps?, 2024b.\\nURL https://arxiv.org/abs/2411.01855.\\nSun et\\xa0al. [2024]\\nHanshi Sun, Momin Haider, Ruiqi Zhang, Huitao Yang, Jiahao Qiu, Ming Yin, Mengdi Wang, Peter Bartlett, and Andrea Zanette.\\nFast best-of-n decoding via speculative rejection, 2024.\\nURL https://arxiv.org/abs/2410.20290.\\nYang et\\xa0al. [2025]\\nChenxu Yang, Qingyi Si, Yongjie Duan, Zheliang Zhu, Chenyu Zhu, Qiaowei Li, Zheng Lin, Li\\xa0Cao, and Weiping Wang.\\nDynamic early exit in reasoning models, 2025.\\nURL https://arxiv.org/abs/2504.15895.\\nTian et\\xa0al. [2025]\\nXiaoyu Tian, Yunjie Ji, Haotian Wang, Shuaiting Chen, Sitong Zhao, Yiping Peng, Han Zhao, and Xiangang Li.\\nNot all correct answers are equal: Why your distillation source matters.\\narXiv preprint arXiv:2505.14464, 2025.\\nInternet [2025]\\nIntelligent Internet.\\nIi-thought : A large-scale, high-quality reasoning dataset, 2025.\\nChen et\\xa0al. [2025b]\\nYang Chen, Zhuolin Yang, Zihan Liu, Chankyu Lee, Peng Xu, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping.\\nAcereason-nemotron: Advancing math and code reasoning through reinforcement learning.\\narXiv preprint arXiv:2505.16400, 2025b.\\nHe et\\xa0al. [2025]\\nJujie He, Jiacai Liu, Chris\\xa0Yuhao Liu, Rui Yan, Chaojie Wang, Peng Cheng, Xiaoyu Zhang, Fuxiang Zhang, Jiacheng Xu, Wei Shen, Siyuan Li, Liang Zeng, Tianwen Wei, Cheng Cheng, Yang Liu, and Yahui Zhou.\\nSkywork open reasoner series.\\nhttps://capricious-hydrogen<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:34:37 [engine.py:317] Added request chatcmpl-34d3a30b5e454835971d540643f04d4a.\n",
            "\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from data/output/arxiv_org_20_0_0.txt...vLLM STDOUT: INFO:     127.0.0.1:44428 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:34:40 [logger.py:43] Received request chatcmpl-ba53788dcc5b47c481641fbbe1ea08e0: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang.\\nDeepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025.\\nURL https://arxiv.org/abs/2501.12948.\\nZhan et\\xa0al. [2025]\\nZizheng Zhan, Ken Deng, Huaixi Tang, Wen Xiang, Kun Wu, Weihao Li, Wenqiang Zhu, Jingxuan Xu, Lecheng Huang, Zongxian Feng, Shaojie Wang, Shangpeng Yan, Xuxing Chen, Jiaheng Liu, Zhongyuan Peng, Zuchen Gao, Haoyang Huang, Xiaojiang Zhang, Jinghui Wang, Zheng Lin, Mengtong Li, Huiming Wang, Ziqi Zhan, Yanan Wu, Yuanxing Zhang, Jian Yang, Guang Chen, Haotian Zhang, Bin Chen, and Bing Yu.\\nKat-v1: Kwai-autothink technical report, 2025.\\nURL https://arxiv.org/abs/2507.08297.\\nAytes et\\xa0al. [2025]\\nSimon\\xa0A. Aytes, Jinheon Baek, and Sung\\xa0Ju Hwang.\\nSketch-of-thought: Efficient llm reasoning with adaptive cognitive-inspired sketching, 2025.\\nURL https://arxiv.org/abs/2503.05179.\\nXia et\\xa0al. [2025]\\nHeming Xia, Chak\\xa0Tou Leong, Wenjie Wang, Yongqi Li, and Wenjie Li.\\nTokenskip: Controllable chain-of-thought compression in llms, 2025.\\nURL https://arxiv.org/abs/2502.12067.\\nLiu et\\xa0al. [2024b]\\nTengxiao Liu, Qipeng Guo, Xiangkun Hu, Cheng Jiayang, Yue Zhang, Xipeng Qiu, and Zheng Zhang.\\nCan language models learn to skip steps?, 2024b.\\nURL https://arxiv.org/abs/2411.01855.\\nSun et\\xa0al. [2024]\\nHanshi Sun, Momin Haider, Ruiqi Zhang, Huitao Yang, Jiahao Qiu, Ming Yin, Mengdi Wang, Peter Bartlett, and Andrea Zanette.\\nFast best-of-n decoding via speculative rejection, 2024.\\nURL https://arxiv.org/abs/2410.20290.\\nYang et\\xa0al. [2025]\\nChenxu Yang, Qingyi Si, Yongjie Duan, Zheliang Zhu, Chenyu Zhu, Qiaowei Li, Zheng Lin, Li\\xa0Cao, and Weiping Wang.\\nDynamic early exit in reasoning models, 2025.\\nURL https://arxiv.org/abs/2504.15895.\\nTian et\\xa0al. [2025]\\nXiaoyu Tian, Yunjie Ji, Haotian Wang, Shuaiting Chen, Sitong Zhao, Yiping Peng, Han Zhao, and Xiangang Li.\\nNot all correct answers are equal: Why your distillation source matters.\\narXiv preprint arXiv:2505.14464, 2025.\\nInternet [2025]\\nIntelligent Internet.\\nIi-thought : A large-scale, high-quality reasoning dataset, 2025.\\nChen et\\xa0al. [2025b]\\nYang Chen, Zhuolin Yang, Zihan Liu, Chankyu Lee, Peng Xu, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping.\\nAcereason-nemotron: Advancing math and code reasoning through reinforcement learning.\\narXiv preprint arXiv:2505.16400, 2025b.\\nHe et\\xa0al. [2025]\\nJujie He, Jiacai Liu, Chris\\xa0Yuhao Liu, Rui Yan, Chaojie Wang, Peng Cheng, Xiaoyu Zhang, Fuxiang Zhang, Jiacheng Xu, Wei Shen, Siyuan Li, Liang Zeng, Tianwen Wei, Cheng Cheng, Yang Liu, and Yahui Zhou.\\nSkywork open reasoner series.\\nhttps://capricious-hydrogen<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:34:40 [engine.py:317] Added request chatcmpl-ba53788dcc5b47c481641fbbe1ea08e0.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from data/output/arxiv_org_20_0_0.txt...vLLM STDOUT: INFO:     127.0.0.1:44442 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 10 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/arxiv_org_20_0_0_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/arxiv_org_20_0_0_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from data/output/arxiv_org_20_0_0.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/arxiv_org_20_0_0_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:42288 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:42294 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:34:49 [logger.py:43] Received request chatcmpl-ab911e984aae4423a13b6fe89521db90: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nNaseh, Marzena Karpinska, Mohit Iyyer, Amir Houmansadr, and Eugene Bagdasarian.\\nOverthink: Slowdown attacks on reasoning llms, 2025.\\nURL https://arxiv.org/abs/2502.02542.\\nSui et\\xa0al. [2025]\\nYang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu, Andrew Wen, Shaochen Zhong, Na\\xa0Zou, Hanjie Chen, and Xia Hu.\\nStop overthinking: A survey on efficient reasoning for large language models, 2025.\\nURL https://arxiv.org/abs/2503.16419.\\nNayab et\\xa0al. [2025]\\nSania Nayab, Giulio Rossolini, Marco Simoni, Andrea Saracino, Giorgio Buttazzo, Nicolamaria Manes, and Fabrizio Giacomelli.\\nConcise thoughts: Impact of output length on llm reasoning and cost, 2025.\\nURL https://arxiv.org/abs/2407.19825.\\nAggarwal and Welleck [2025]\\nPranjal Aggarwal and Sean Welleck.\\nL1: Controlling how long a reasoning model thinks with reinforcement learning, 2025.\\nURL https://arxiv.org/abs/2503.04697.\\nArora and Zanette [2025]\\nDaman Arora and Andrea Zanette.\\nTraining language models to reason efficiently, 2025.\\nURL https://arxiv.org/abs/2502.04463.\\nHou et\\xa0al. [2025]\\nBairu Hou, Yang Zhang, Jiabao Ji, Yujian Liu, Kaizhi Qian, Jacob Andreas, and Shiyu Chang.\\nThinkprune: Pruning long chain-of-thought of llms via reinforcement learning, 2025.\\nURL https://arxiv.org/abs/2504.01296.\\nLuo et\\xa0al. [2025]\\nHaotian Luo, Li\\xa0Shen, Haiying He, Yibo Wang, Shiwei Liu, Wei Li, Naiqiang Tan, Xiaochun Cao, and Dacheng Tao.\\nO1-pruner: Length-harmonizing fine-tuning for o1-like reasoning pruning, 2025.\\nURL https://arxiv.org/abs/2501.12570.\\nShen et\\xa0al. [2025]\\nYi\\xa0Shen, Jian Zhang, Jieyun Huang, Shuming Shi, Wenjing Zhang, Jiangze Yan, Ning Wang, Kai Wang, Zhaoxiang Liu, and Shiguo Lian.\\nDast: Difficulty-adaptive slow-thinking for large reasoning models, 2025.\\nURL https://arxiv.org/abs/2503.04472.\\nTeam et\\xa0al. [2025]\\nKimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, Chuning Tang, Congcong Wang, Dehao Zhang, Enming Yuan, Enzhe Lu, Fengxiang Tang, Flood Sung, Guangda Wei, Guokun Lai, Haiqing Guo, Han Zhu, Hao Ding, Hao Hu, Hao Yang, Hao Zhang, Haotian Yao, Haotian Zhao, Haoyu Lu, Haoze Li, Haozhen Yu, Hongcheng Gao, Huabin Zheng, Huan Yuan, Jia Chen, Jianhang Guo, Jianlin Su, Jianzhou Wang, Jie Zhao, Jin Zhang, Jingyuan Liu, Junjie Yan, Junyan Wu, Lidong Shi, Ling Ye, Longhui Yu, Mengnan Dong, Neo Zhang, Ningchen Ma, Qiwei Pan, Qucheng Gong, Shaowei Liu, Shengling Ma, Shupeng Wei, Sihan Cao, Siying Huang, Tao Jiang, Weihao Gao, Weimin Xiong, Weiran He, Weixiao Huang, Weixin Xu, Wenhao Wu, Wenyang He, Xianghui Wei, Xianqing Jia, Xingzhe Wu, Xinran Xu, Xinxing Zu,<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:34:49 [engine.py:317] Added request chatcmpl-ab911e984aae4423a13b6fe89521db90.\n",
            "\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from data/output/arxiv_org_16_0_0.txt...vLLM STDOUT: INFO:     127.0.0.1:42302 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:34:51 [logger.py:43] Received request chatcmpl-bc23a4e18f8346e68df1143f508cdb85: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\n Naseh, Marzena Karpinska, Mohit Iyyer, Amir Houmansadr, and Eugene Bagdasarian.\\nOverthink: Slowdown attacks on reasoning llms, 2025.\\nURL https://arxiv.org/abs/2502.02542.\\nSui et\\xa0al. [2025]\\nYang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu, Andrew Wen, Shaochen Zhong, Na\\xa0Zou, Hanjie Chen, and Xia Hu.\\nStop overthinking: A survey on efficient reasoning for large language models, 2025.\\nURL https://arxiv.org/abs/2503.16419.\\nNayab et\\xa0al. [2025]\\nSania Nayab, Giulio Rossolini, Marco Simoni, Andrea Saracino, Giorgio Buttazzo, Nicolamaria Manes, and Fabrizio Giacomelli.\\nConcise thoughts: Impact of output length on llm reasoning and cost, 2025.\\nURL https://arxiv.org/abs/2407.19825.\\nAggarwal and Welleck [2025]\\nPranjal Aggarwal and Sean Welleck.\\nL1: Controlling how long a reasoning model thinks with reinforcement learning, 2025.\\nURL https://arxiv.org/abs/2503.04697.\\nArora and Zanette [2025]\\nDaman Arora and Andrea Zanette.\\nTraining language models to reason efficiently, 2025.\\nURL https://arxiv.org/abs/2502.04463.\\nHou et\\xa0al. [2025]\\nBairu Hou, Yang Zhang, Jiabao Ji, Yujian Liu, Kaizhi Qian, Jacob Andreas, and Shiyu Chang.\\nThinkprune: Pruning long chain-of-thought of llms via reinforcement learning, 2025.\\nURL https://arxiv.org/abs/2504.01296.\\nLuo et\\xa0al. [2025]\\nHaotian Luo, Li\\xa0Shen, Haiying He, Yibo Wang, Shiwei Liu, Wei Li, Naiqiang Tan, Xiaochun Cao, and Dacheng Tao.\\nO1-pruner: Length-harmonizing fine-tuning for o1-like reasoning pruning, 2025.\\nURL https://arxiv.org/abs/2501.12570.\\nShen et\\xa0al. [2025]\\nYi\\xa0Shen, Jian Zhang, Jieyun Huang, Shuming Shi, Wenjing Zhang, Jiangze Yan, Ning Wang, Kai Wang, Zhaoxiang Liu, and Shiguo Lian.\\nDast: Difficulty-adaptive slow-thinking for large reasoning models, 2025.\\nURL https://arxiv.org/abs/2503.04472.\\nTeam et\\xa0al. [2025]\\nKimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, Chuning Tang, Congcong Wang, Dehao Zhang, Enming Yuan, Enzhe Lu, Fengxiang Tang, Flood Sung, Guangda Wei, Guokun Lai, Haiqing Guo, Han Zhu, Hao Ding, Hao Hu, Hao Yang, Hao Zhang, Haotian Yao, Haotian Zhao, Haoyu Lu, Haoze Li, Haozhen Yu, Hongcheng Gao, Huabin Zheng, Huan Yuan, Jia Chen, Jianhang Guo, Jianlin Su, Jianzhou Wang, Jie Zhao, Jin Zhang, Jingyuan Liu, Junjie Yan, Junyan Wu, Lidong Shi, Ling Ye, Longhui Yu, Mengnan Dong, Neo Zhang, Ningchen Ma, Qiwei Pan, Qucheng Gong, Shaowei Liu, Shengling Ma, Shupeng Wei, Sihan Cao, Siying Huang, Tao Jiang, Weihao Gao, Weimin Xiong, Weiran He, Weixiao Huang, Weixin Xu, Wenhao Wu, Wenyang He, Xianghui Wei, Xianqing Jia, Xingzhe Wu, Xinran Xu, Xinxing Zu,<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:34:51 [engine.py:317] Added request chatcmpl-bc23a4e18f8346e68df1143f508cdb85.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from data/output/arxiv_org_16_0_0.txt...vLLM STDOUT: INFO:     127.0.0.1:42318 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 0 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/arxiv_org_16_0_0_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/arxiv_org_16_0_0_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from data/output/arxiv_org_16_0_0.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/arxiv_org_16_0_0_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:37810 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:37820 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:35:00 [logger.py:43] Received request chatcmpl-99efc094ed3a4c4bb145bdb0c9a1712c: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n}\\n1.00−0.0%\\\\scriptstyle{\\\\scriptstyle-0.0\\\\%}\\n76.7↑7.0%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 7.0\\\\%}\\n20613↓13.4%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 13.4\\\\%}\\n1.00−0.0%\\\\scriptstyle{\\\\scriptstyle-0.0\\\\%}\\n60.8↑8.2%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 8.2\\\\%}\\n18158↓6.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 6.8\\\\%}\\n0.91↓9.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 9.0\\\\%}\\n88.4↑6.6%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 6.6\\\\%}\\n2272↓14.6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 14.6\\\\%}\\n0.54↓46.3%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 46.3\\\\%}\\n+ GRPO\\n86.7↑7.2%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 7.2\\\\%}\\n17083↓19.7%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 19.7\\\\%}\\n1.00−0.0%\\\\scriptstyle{\\\\scriptstyle-0.0\\\\%}\\n79.17↑10.5%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 10.5\\\\%}\\n19869↓16.5%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 16.5\\\\%}\\n1.00−0.0%\\\\scriptstyle{\\\\scriptstyle-0.0\\\\%}\\n62.1↑10.6%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 10.6\\\\%}\\n18046↓7.3%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 7.3\\\\%}\\n0.93↓7.3%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 7.3\\\\%}\\n87.8↑5.9%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 5.9\\\\%}\\n2220↓16.6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 16.6\\\\%}\\n0.59↓40.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 40.8\\\\%}\\nAdaptThink\\n83.3↑3.1%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 3.1\\\\%}\\n16598↓21.9%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 21.9\\\\%}\\n0.93↓7.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 7.0\\\\%}\\n74.2↑3.5%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 3.5\\\\%}\\n19993↓16.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 16.0\\\\%}\\n0.84↓16.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 16.0\\\\%}\\n57.1↑1.6%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 1.6\\\\%}\\n16162↓17.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 17.0\\\\%}\\n0.78↓28.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 28.0\\\\%}\\n85.4↑3.0%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 3.0\\\\%}\\n915↓65.6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 65.6\\\\%}\\n0.16↓84.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 84.0\\\\%}\\nAutoThink\\n84.3↑3.5%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 3.5\\\\%}\\n17061↓19.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 19.8\\\\%}\\n0.95↓5.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 5.0\\\\%}\\n75.0↑4.6%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 4.6\\\\%}\\n18784↓21.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 21.0\\\\%}\\n0.88↓12.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 12.0\\\\%}\\n57.5↑<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:35:00 [engine.py:317] Added request chatcmpl-99efc094ed3a4c4bb145bdb0c9a1712c.\n",
            "\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from data/output/arxiv_org_8_0_0.txt...vLLM STDOUT: INFO:     127.0.0.1:37826 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:35:03 [logger.py:43] Received request chatcmpl-cbdfe072606a4fa8babf86670e2361ce: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\n}\\n1.00−0.0%\\\\scriptstyle{\\\\scriptstyle-0.0\\\\%}\\n76.7↑7.0%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 7.0\\\\%}\\n20613↓13.4%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 13.4\\\\%}\\n1.00−0.0%\\\\scriptstyle{\\\\scriptstyle-0.0\\\\%}\\n60.8↑8.2%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 8.2\\\\%}\\n18158↓6.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 6.8\\\\%}\\n0.91↓9.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 9.0\\\\%}\\n88.4↑6.6%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 6.6\\\\%}\\n2272↓14.6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 14.6\\\\%}\\n0.54↓46.3%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 46.3\\\\%}\\n+ GRPO\\n86.7↑7.2%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 7.2\\\\%}\\n17083↓19.7%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 19.7\\\\%}\\n1.00−0.0%\\\\scriptstyle{\\\\scriptstyle-0.0\\\\%}\\n79.17↑10.5%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 10.5\\\\%}\\n19869↓16.5%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 16.5\\\\%}\\n1.00−0.0%\\\\scriptstyle{\\\\scriptstyle-0.0\\\\%}\\n62.1↑10.6%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 10.6\\\\%}\\n18046↓7.3%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 7.3\\\\%}\\n0.93↓7.3%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 7.3\\\\%}\\n87.8↑5.9%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 5.9\\\\%}\\n2220↓16.6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 16.6\\\\%}\\n0.59↓40.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 40.8\\\\%}\\nAdaptThink\\n83.3↑3.1%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 3.1\\\\%}\\n16598↓21.9%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 21.9\\\\%}\\n0.93↓7.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 7.0\\\\%}\\n74.2↑3.5%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 3.5\\\\%}\\n19993↓16.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 16.0\\\\%}\\n0.84↓16.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 16.0\\\\%}\\n57.1↑1.6%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 1.6\\\\%}\\n16162↓17.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 17.0\\\\%}\\n0.78↓28.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 28.0\\\\%}\\n85.4↑3.0%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 3.0\\\\%}\\n915↓65.6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 65.6\\\\%}\\n0.16↓84.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 84.0\\\\%}\\nAutoThink\\n84.3↑3.5%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 3.5\\\\%}\\n17061↓19.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 19.8\\\\%}\\n0.95↓5.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 5.0\\\\%}\\n75.0↑4.6%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 4.6\\\\%}\\n18784↓21.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 21.0\\\\%}\\n0.88↓12.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 12.0\\\\%}\\n57.5↑<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:35:03 [engine.py:317] Added request chatcmpl-cbdfe072606a4fa8babf86670e2361ce.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from data/output/arxiv_org_8_0_0.txt...vLLM STDOUT: INFO:     127.0.0.1:37840 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 17 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/arxiv_org_8_0_0_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/arxiv_org_8_0_0_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from data/output/arxiv_org_8_0_0.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/arxiv_org_8_0_0_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:43732 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:43740 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:35:12 [logger.py:43] Received request chatcmpl-880fb8b948e74c6a806ca75d0db49c6c: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nmodes, respectively. If the difference in pass rates satisfies |pon−poff|<δ|p_{\\\\text{on}}-p_{\\\\text{off}}|<\\\\delta, where δ\\\\delta is a predefined threshold, the Think-off mode is selected. This tie-breaking strategy encourages the model to prefer more concise responses when deeper reasoning does not lead to a significant improvement in correctness.\\nFor the winning mode,\\nthe shortest correct response is retained as the final sample.\\nTo expose the model to diverse reasoning scenarios and encourage adaptive behavior, we randomly assign a mode to 1% of the queries, forcing the model to encounter diverse reasoning scenarios. This forces the model to engage with both reasoning styles in varying contexts, which is essential for learning when to switch modes dynamically during inference.\\nAdditionally, we incorporate an auxiliary explanation signal to enhance the model’s mode alignment capabilities. For each query-response pair, we prompt DeepSeek-V3\\xa0(Liu et\\xa0al., 2024a) to generate a justification explaining why the selected mode is appropriate. This explanation provides a valuable training signal for aligning mode decisions with the underlying reasoning complexity.\\n3.1.3 Data format\\nThe training samples follow a unified structure encompassing justification and answer generation. As shown in Table\\xa01, this design guides the model to decide when reasoning is needed and to generate answers consistent with it. The special tokens are detailed in Table\\xa01, ensuring a clear separation between reasoning and final response for better alignment.\\nTable 1: Formatting templates (left) and special tokens with their descriptions (right).\\nThink-on Mode\\nThink-off Mode\\n<judge>\\n<judge>\\n{judge_analysis}\\n{judge_analysis}\\n</judge>\\n</judge>\\n<think_on>\\n<think_off>\\n<think>\\n<answer>\\n{thinking_content}\\n{response}\\n</think>\\n</answer>\\n<answer>\\n{response}\\n</answer>\\nSpecial Token\\nDescription\\n<judge>\\nAnalyzes input query to determine whether reasoning is required.\\n<think_on/off>\\nSpecifies whether reasoning should be activated (\"on\") or skipped (\"off\").\\n<think>\\nMarks the beginning of reasoning in Think-on mode.\\n<answer>\\nMarks the beginning of the model’s answer.\\n3.2 Hybrid RL Reward System\\nThis section details the reinforcement learning process used to teach the model how to effectively balance Think-on and Think-off reasoning modes. The approach is built on a hybrid RL reward system that guides the model’s optimization.\\n3.2.1 Basic Reward Formulation\\nConsider a group of NN sampled responses, for each response i∈{1,…,N}i\\\\in\\\\{1,\\\\dots,N\\\\}, we denote its answer correctness by ACCi∈{0,1}\\\\mathrm{ACC}_{i}\\\\in\\\\{0,1\\\\}, its format correctness by FORMATi∈{0,1}\\\\mathrm{FORMAT}_{i}\\\\in\\\\{0,1\\\\}, its basic reward by ri\\u200b=\\u200bACCi+0.2⋅FORMATi∈ℝr_{i}\\\\text{=}\\\\mathrm{ACC}_{i}+0.2\\\\cdot\\\\mathrm{FORMAT}_{i}\\\\in\\\\mathbb{R}, and its reasoning mode by Mi∈{on,off}M_{i}\\\\in\\\\{\\\\text{on},\\\\text{off}\\\\}, where Mi\\u200b=onM_{i}\\\\text{=}\\\\text{on} indicates the Think-on mode and Mi\\u200b=offM_{i}\\\\text{=}\\\\text{off} indicates the Think-off mode.\\n3.2.2 Bias Adjustment Mechanism\\nA potential risk of the hybrid reward design is that the model may overfit to the more accurate Think-on mode, favoring deep reasoning even when it is unnecessary. This tendency can reduce response efficiency and hinder the intended flexibility in reasoning behavior.\\nTo mitigate this issue, we introduce a bias adjustment mechanism that dynamically regularizes the contribution of mode-specific accuracies.\\nLet mean\\u200b(𝐫on)\\u200b=\\u200b1Non\\u200b∑i:Mi=onri\\\\text{mean}(\\\\mathbf{r}_{\\\\text{on}})\\\\text{=}\\\\frac{1}{N_{\\\\text{on}}}\\\\sum_{i:M_{i}=\\\\text{on}}r_{i} denote the average reward of responses generated under the Think-on mode, and let mean\\u200b(𝐫off)\\\\text{mean}(\\\\mathbf{r}_{\\\\<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:35:12 [engine.py:317] Added request chatcmpl-880fb8b948e74c6a806ca75d0db49c6c.\n",
            "\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from data/output/arxiv_org_3_0_0.txt...vLLM STDOUT: INFO:     127.0.0.1:43756 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:35:14 [logger.py:43] Received request chatcmpl-b0b784eeaad64a4a8bc5bf8b2ff248ac: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\n modes, respectively. If the difference in pass rates satisfies |pon−poff|<δ|p_{\\\\text{on}}-p_{\\\\text{off}}|<\\\\delta, where δ\\\\delta is a predefined threshold, the Think-off mode is selected. This tie-breaking strategy encourages the model to prefer more concise responses when deeper reasoning does not lead to a significant improvement in correctness.\\nFor the winning mode,\\nthe shortest correct response is retained as the final sample.\\nTo expose the model to diverse reasoning scenarios and encourage adaptive behavior, we randomly assign a mode to 1% of the queries, forcing the model to encounter diverse reasoning scenarios. This forces the model to engage with both reasoning styles in varying contexts, which is essential for learning when to switch modes dynamically during inference.\\nAdditionally, we incorporate an auxiliary explanation signal to enhance the model’s mode alignment capabilities. For each query-response pair, we prompt DeepSeek-V3\\xa0(Liu et\\xa0al., 2024a) to generate a justification explaining why the selected mode is appropriate. This explanation provides a valuable training signal for aligning mode decisions with the underlying reasoning complexity.\\n3.1.3 Data format\\nThe training samples follow a unified structure encompassing justification and answer generation. As shown in Table\\xa01, this design guides the model to decide when reasoning is needed and to generate answers consistent with it. The special tokens are detailed in Table\\xa01, ensuring a clear separation between reasoning and final response for better alignment.\\nTable 1: Formatting templates (left) and special tokens with their descriptions (right).\\nThink-on Mode\\nThink-off Mode\\n<judge>\\n<judge>\\n{judge_analysis}\\n{judge_analysis}\\n</judge>\\n</judge>\\n<think_on>\\n<think_off>\\n<think>\\n<answer>\\n{thinking_content}\\n{response}\\n</think>\\n</answer>\\n<answer>\\n{response}\\n</answer>\\nSpecial Token\\nDescription\\n<judge>\\nAnalyzes input query to determine whether reasoning is required.\\n<think_on/off>\\nSpecifies whether reasoning should be activated (\"on\") or skipped (\"off\").\\n<think>\\nMarks the beginning of reasoning in Think-on mode.\\n<answer>\\nMarks the beginning of the model’s answer.\\n3.2 Hybrid RL Reward System\\nThis section details the reinforcement learning process used to teach the model how to effectively balance Think-on and Think-off reasoning modes. The approach is built on a hybrid RL reward system that guides the model’s optimization.\\n3.2.1 Basic Reward Formulation\\nConsider a group of NN sampled responses, for each response i∈{1,…,N}i\\\\in\\\\{1,\\\\dots,N\\\\}, we denote its answer correctness by ACCi∈{0,1}\\\\mathrm{ACC}_{i}\\\\in\\\\{0,1\\\\}, its format correctness by FORMATi∈{0,1}\\\\mathrm{FORMAT}_{i}\\\\in\\\\{0,1\\\\}, its basic reward by ri\\u200b=\\u200bACCi+0.2⋅FORMATi∈ℝr_{i}\\\\text{=}\\\\mathrm{ACC}_{i}+0.2\\\\cdot\\\\mathrm{FORMAT}_{i}\\\\in\\\\mathbb{R}, and its reasoning mode by Mi∈{on,off}M_{i}\\\\in\\\\{\\\\text{on},\\\\text{off}\\\\}, where Mi\\u200b=onM_{i}\\\\text{=}\\\\text{on} indicates the Think-on mode and Mi\\u200b=offM_{i}\\\\text{=}\\\\text{off} indicates the Think-off mode.\\n3.2.2 Bias Adjustment Mechanism\\nA potential risk of the hybrid reward design is that the model may overfit to the more accurate Think-on mode, favoring deep reasoning even when it is unnecessary. This tendency can reduce response efficiency and hinder the intended flexibility in reasoning behavior.\\nTo mitigate this issue, we introduce a bias adjustment mechanism that dynamically regularizes the contribution of mode-specific accuracies.\\nLet mean\\u200b(𝐫on)\\u200b=\\u200b1Non\\u200b∑i:Mi=onri\\\\text{mean}(\\\\mathbf{r}_{\\\\text{on}})\\\\text{=}\\\\frac{1}{N_{\\\\text{on}}}\\\\sum_{i:M_{i}=\\\\text{on}}r_{i} denote the average reward of responses generated under the Think-on mode, and let mean\\u200b(𝐫off)\\\\text{mean}(\\\\mathbf{r}_{\\\\<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:35:14 [engine.py:317] Added request chatcmpl-b0b784eeaad64a4a8bc5bf8b2ff248ac.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from data/output/arxiv_org_3_0_0.txt...vLLM STDOUT: INFO:     127.0.0.1:54904 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 10 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/arxiv_org_3_0_0_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/arxiv_org_3_0_0_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from data/output/arxiv_org_3_0_0.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/arxiv_org_3_0_0_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:47860 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:47874 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:35:23 [logger.py:43] Received request chatcmpl-c1b87a31f81746c4810aa4c312f1062d: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nmodes, respectively. If the difference in pass rates satisfies |pon−poff|<δ|p_{\\\\text{on}}-p_{\\\\text{off}}|<\\\\delta, where δ\\\\delta is a predefined threshold, the Think-off mode is selected. This tie-breaking strategy encourages the model to prefer more concise responses when deeper reasoning does not lead to a significant improvement in correctness.\\nFor the winning mode,\\nthe shortest correct response is retained as the final sample.\\nTo expose the model to diverse reasoning scenarios and encourage adaptive behavior, we randomly assign a mode to 1% of the queries, forcing the model to encounter diverse reasoning scenarios. This forces the model to engage with both reasoning styles in varying contexts, which is essential for learning when to switch modes dynamically during inference.\\nAdditionally, we incorporate an auxiliary explanation signal to enhance the model’s mode alignment capabilities. For each query-response pair, we prompt DeepSeek-V3\\xa0(Liu et\\xa0al., 2024a) to generate a justification explaining why the selected mode is appropriate. This explanation provides a valuable training signal for aligning mode decisions with the underlying reasoning complexity.\\n3.1.3 Data format\\nThe training samples follow a unified structure encompassing justification and answer generation. As shown in Table\\xa01, this design guides the model to decide when reasoning is needed and to generate answers consistent with it. The special tokens are detailed in Table\\xa01, ensuring a clear separation between reasoning and final response for better alignment.\\nTable 1: Formatting templates (left) and special tokens with their descriptions (right).\\nThink-on Mode\\nThink-off Mode\\n<judge>\\n<judge>\\n{judge_analysis}\\n{judge_analysis}\\n</judge>\\n</judge>\\n<think_on>\\n<think_off>\\n<think>\\n<answer>\\n{thinking_content}\\n{response}\\n</think>\\n</answer>\\n<answer>\\n{response}\\n</answer>\\nSpecial Token\\nDescription\\n<judge>\\nAnalyzes input query to determine whether reasoning is required.\\n<think_on/off>\\nSpecifies whether reasoning should be activated (\"on\") or skipped (\"off\").\\n<think>\\nMarks the beginning of reasoning in Think-on mode.\\n<answer>\\nMarks the beginning of the model’s answer.\\n3.2 Hybrid RL Reward System\\nThis section details the reinforcement learning process used to teach the model how to effectively balance Think-on and Think-off reasoning modes. The approach is built on a hybrid RL reward system that guides the model’s optimization.\\n3.2.1 Basic Reward Formulation\\nConsider a group of NN sampled responses, for each response i∈{1,…,N}i\\\\in\\\\{1,\\\\dots,N\\\\}, we denote its answer correctness by ACCi∈{0,1}\\\\mathrm{ACC}_{i}\\\\in\\\\{0,1\\\\}, its format correctness by FORMATi∈{0,1}\\\\mathrm{FORMAT}_{i}\\\\in\\\\{0,1\\\\}, its basic reward by ri\\u200b=\\u200bACCi+0.2⋅FORMATi∈ℝr_{i}\\\\text{=}\\\\mathrm{ACC}_{i}+0.2\\\\cdot\\\\mathrm{FORMAT}_{i}\\\\in\\\\mathbb{R}, and its reasoning mode by Mi∈{on,off}M_{i}\\\\in\\\\{\\\\text{on},\\\\text{off}\\\\}, where Mi\\u200b=onM_{i}\\\\text{=}\\\\text{on} indicates the Think-on mode and Mi\\u200b=offM_{i}\\\\text{=}\\\\text{off} indicates the Think-off mode.\\n3.2.2 Bias Adjustment Mechanism\\nA potential risk of the hybrid reward design is that the model may overfit to the more accurate Think-on mode, favoring deep reasoning even when it is unnecessary. This tendency can reduce response efficiency and hinder the intended flexibility in reasoning behavior.\\nTo mitigate this issue, we introduce a bias adjustment mechanism that dynamically regularizes the contribution of mode-specific accuracies.\\nLet mean\\u200b(𝐫on)\\u200b=\\u200b1Non\\u200b∑i:Mi=onri\\\\text{mean}(\\\\mathbf{r}_{\\\\text{on}})\\\\text{=}\\\\frac{1}{N_{\\\\text{on}}}\\\\sum_{i:M_{i}=\\\\text{on}}r_{i} denote the average reward of responses generated under the Think-on mode, and let mean\\u200b(𝐫off)\\\\text{mean}(\\\\mathbf{r}_{\\\\<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:35:23 [engine.py:317] Added request chatcmpl-c1b87a31f81746c4810aa4c312f1062d.\n",
            "\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from data/output/arxiv_org_3_0.txt...vLLM STDOUT: INFO:     127.0.0.1:47876 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:35:25 [logger.py:43] Received request chatcmpl-d34054b32f7040eda120253fcdf5b9ed: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\n modes, respectively. If the difference in pass rates satisfies |pon−poff|<δ|p_{\\\\text{on}}-p_{\\\\text{off}}|<\\\\delta, where δ\\\\delta is a predefined threshold, the Think-off mode is selected. This tie-breaking strategy encourages the model to prefer more concise responses when deeper reasoning does not lead to a significant improvement in correctness.\\nFor the winning mode,\\nthe shortest correct response is retained as the final sample.\\nTo expose the model to diverse reasoning scenarios and encourage adaptive behavior, we randomly assign a mode to 1% of the queries, forcing the model to encounter diverse reasoning scenarios. This forces the model to engage with both reasoning styles in varying contexts, which is essential for learning when to switch modes dynamically during inference.\\nAdditionally, we incorporate an auxiliary explanation signal to enhance the model’s mode alignment capabilities. For each query-response pair, we prompt DeepSeek-V3\\xa0(Liu et\\xa0al., 2024a) to generate a justification explaining why the selected mode is appropriate. This explanation provides a valuable training signal for aligning mode decisions with the underlying reasoning complexity.\\n3.1.3 Data format\\nThe training samples follow a unified structure encompassing justification and answer generation. As shown in Table\\xa01, this design guides the model to decide when reasoning is needed and to generate answers consistent with it. The special tokens are detailed in Table\\xa01, ensuring a clear separation between reasoning and final response for better alignment.\\nTable 1: Formatting templates (left) and special tokens with their descriptions (right).\\nThink-on Mode\\nThink-off Mode\\n<judge>\\n<judge>\\n{judge_analysis}\\n{judge_analysis}\\n</judge>\\n</judge>\\n<think_on>\\n<think_off>\\n<think>\\n<answer>\\n{thinking_content}\\n{response}\\n</think>\\n</answer>\\n<answer>\\n{response}\\n</answer>\\nSpecial Token\\nDescription\\n<judge>\\nAnalyzes input query to determine whether reasoning is required.\\n<think_on/off>\\nSpecifies whether reasoning should be activated (\"on\") or skipped (\"off\").\\n<think>\\nMarks the beginning of reasoning in Think-on mode.\\n<answer>\\nMarks the beginning of the model’s answer.\\n3.2 Hybrid RL Reward System\\nThis section details the reinforcement learning process used to teach the model how to effectively balance Think-on and Think-off reasoning modes. The approach is built on a hybrid RL reward system that guides the model’s optimization.\\n3.2.1 Basic Reward Formulation\\nConsider a group of NN sampled responses, for each response i∈{1,…,N}i\\\\in\\\\{1,\\\\dots,N\\\\}, we denote its answer correctness by ACCi∈{0,1}\\\\mathrm{ACC}_{i}\\\\in\\\\{0,1\\\\}, its format correctness by FORMATi∈{0,1}\\\\mathrm{FORMAT}_{i}\\\\in\\\\{0,1\\\\}, its basic reward by ri\\u200b=\\u200bACCi+0.2⋅FORMATi∈ℝr_{i}\\\\text{=}\\\\mathrm{ACC}_{i}+0.2\\\\cdot\\\\mathrm{FORMAT}_{i}\\\\in\\\\mathbb{R}, and its reasoning mode by Mi∈{on,off}M_{i}\\\\in\\\\{\\\\text{on},\\\\text{off}\\\\}, where Mi\\u200b=onM_{i}\\\\text{=}\\\\text{on} indicates the Think-on mode and Mi\\u200b=offM_{i}\\\\text{=}\\\\text{off} indicates the Think-off mode.\\n3.2.2 Bias Adjustment Mechanism\\nA potential risk of the hybrid reward design is that the model may overfit to the more accurate Think-on mode, favoring deep reasoning even when it is unnecessary. This tendency can reduce response efficiency and hinder the intended flexibility in reasoning behavior.\\nTo mitigate this issue, we introduce a bias adjustment mechanism that dynamically regularizes the contribution of mode-specific accuracies.\\nLet mean\\u200b(𝐫on)\\u200b=\\u200b1Non\\u200b∑i:Mi=onri\\\\text{mean}(\\\\mathbf{r}_{\\\\text{on}})\\\\text{=}\\\\frac{1}{N_{\\\\text{on}}}\\\\sum_{i:M_{i}=\\\\text{on}}r_{i} denote the average reward of responses generated under the Think-on mode, and let mean\\u200b(𝐫off)\\\\text{mean}(\\\\mathbf{r}_{\\\\<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:35:25 [engine.py:317] Added request chatcmpl-d34054b32f7040eda120253fcdf5b9ed.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from data/output/arxiv_org_3_0.txt...vLLM STDOUT: INFO:     127.0.0.1:47882 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 12 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/arxiv_org_3_0_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/arxiv_org_3_0_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from data/output/arxiv_org_3_0.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/arxiv_org_3_0_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:35374 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:35390 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:35:34 [logger.py:43] Received request chatcmpl-ef4dd38302494e0b84f1275be6f710c7: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nfor training. The parameters are set as: batch size = 16, maximum response length = 32k, NN = 16, ω\\\\omega = 0.01, and γ=0.3\\\\gamma=0.3.\\nBaselines.\\nTo demonstrate the effect of mitigating overthinking, we designed the following baselines for comparison.\\n(1) Cold-Start: We perform Cold-Star on the model using the data construction method described in Section\\xa03.1.\\n(2) Cold-Start (On): We apply the same Cold-Star procedure as in Section\\xa03.1, but only include the data collected under the Think-on mode.\\n(3) Cold-Start (On) + GRPO: We further train the Cold-Start (On) model using the GRPO algorithm.\\n(4) Cold-Start + GRPO: We further train the Cold-Start model with the GRPO algorithm.\\n(5) HiPO: We train the model following our HiPO.\\n(6) AdaptThink: We reproduced the code provided in (Zhang et\\xa0al., 2025).\\n(7) AutoThink: We reproduced the code provided in (Tu et\\xa0al., 2025).\\nEvaluation benchmarks.\\nWe conducted tests on AIME2024, AIME2025, HumanEval\\xa0(Chen et\\xa0al., 2021), LiveCodeBench V6\\xa0(Jain et\\xa0al., 2024), MBPP\\xa0(Austin et\\xa0al., 2021),\\nMATH-500\\xa0(Lightman et\\xa0al., 2023), and GPQA-Diamond\\xa0(Rein et\\xa0al., 2023).\\n4.2 Main Results\\nAIME2024\\nAIME2025\\nLiveCodeBench\\nHumanEval\\nMethod\\nAcc↑\\\\uparrow\\nLength↓\\\\downarrow\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}↓\\\\downarrow\\nAcc↑\\\\uparrow\\nLength↓\\\\downarrow\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}↓\\\\downarrow\\nAcc↑\\\\uparrow\\nLength↓\\\\downarrow\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}↓\\\\downarrow\\nAcc↑\\\\uparrow\\nLength↓\\\\downarrow\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}↓\\\\downarrow\\nCold-Start (on)\\n80.8\\n21265\\n1.00\\n71.7\\n23791\\n1.00\\n56.2\\n19473\\n1.00\\n82.9\\n2662\\n1.00\\n+ GRPO\\n82.5↑2.1%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 2.1\\\\%}\\n21045↓1.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 1.0\\\\%}\\n1.00−0.0%\\\\scriptstyle{\\\\scriptstyle-0.0\\\\%}\\n76.7↑7%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 7\\\\%}\\n22695↓4.6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 4.6\\\\%}\\n1.00−0.0%\\\\scriptstyle{\\\\scriptstyle-0.0\\\\%}\\n57.3↑2.0%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 2.0\\\\%}\\n19067↓2.1%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 2.1\\\\%}\\n1.00−0.0%\\\\scriptstyle{\\\\scriptstyle-0.0\\\\%}\\n95.1↑14.7%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 14.7\\\\%}\\n3597↑35.1%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 35.1\\\\%}\\n1.00−0.0%\\\\scriptstyle{\\\\scriptstyle-0.0\\\\%}\\nCold-Start\\n85.8↑6.2%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 6.2\\\\%}\\n18138↓14.7%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 14.7\\\\%}\\n1.00−0.0%\\\\scriptstyle{\\\\scriptstyle-0.0\\\\%}\\n76.7↑7.0%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 7.0\\\\%}\\n20613↓13.4%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 13.4\\\\<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:35:34 [engine.py:317] Added request chatcmpl-ef4dd38302494e0b84f1275be6f710c7.\n",
            "\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from data/output/arxiv_org_7_0_0.txt...vLLM STDOUT: INFO:     127.0.0.1:35396 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:35:37 [logger.py:43] Received request chatcmpl-32859a13e5a64e9dbc6e7acf96d67e95: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\n for training. The parameters are set as: batch size = 16, maximum response length = 32k, NN = 16, ω\\\\omega = 0.01, and γ=0.3\\\\gamma=0.3.\\nBaselines.\\nTo demonstrate the effect of mitigating overthinking, we designed the following baselines for comparison.\\n(1) Cold-Start: We perform Cold-Star on the model using the data construction method described in Section\\xa03.1.\\n(2) Cold-Start (On): We apply the same Cold-Star procedure as in Section\\xa03.1, but only include the data collected under the Think-on mode.\\n(3) Cold-Start (On) + GRPO: We further train the Cold-Start (On) model using the GRPO algorithm.\\n(4) Cold-Start + GRPO: We further train the Cold-Start model with the GRPO algorithm.\\n(5) HiPO: We train the model following our HiPO.\\n(6) AdaptThink: We reproduced the code provided in (Zhang et\\xa0al., 2025).\\n(7) AutoThink: We reproduced the code provided in (Tu et\\xa0al., 2025).\\nEvaluation benchmarks.\\nWe conducted tests on AIME2024, AIME2025, HumanEval\\xa0(Chen et\\xa0al., 2021), LiveCodeBench V6\\xa0(Jain et\\xa0al., 2024), MBPP\\xa0(Austin et\\xa0al., 2021),\\nMATH-500\\xa0(Lightman et\\xa0al., 2023), and GPQA-Diamond\\xa0(Rein et\\xa0al., 2023).\\n4.2 Main Results\\nAIME2024\\nAIME2025\\nLiveCodeBench\\nHumanEval\\nMethod\\nAcc↑\\\\uparrow\\nLength↓\\\\downarrow\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}↓\\\\downarrow\\nAcc↑\\\\uparrow\\nLength↓\\\\downarrow\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}↓\\\\downarrow\\nAcc↑\\\\uparrow\\nLength↓\\\\downarrow\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}↓\\\\downarrow\\nAcc↑\\\\uparrow\\nLength↓\\\\downarrow\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}↓\\\\downarrow\\nCold-Start (on)\\n80.8\\n21265\\n1.00\\n71.7\\n23791\\n1.00\\n56.2\\n19473\\n1.00\\n82.9\\n2662\\n1.00\\n+ GRPO\\n82.5↑2.1%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 2.1\\\\%}\\n21045↓1.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 1.0\\\\%}\\n1.00−0.0%\\\\scriptstyle{\\\\scriptstyle-0.0\\\\%}\\n76.7↑7%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 7\\\\%}\\n22695↓4.6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 4.6\\\\%}\\n1.00−0.0%\\\\scriptstyle{\\\\scriptstyle-0.0\\\\%}\\n57.3↑2.0%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 2.0\\\\%}\\n19067↓2.1%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 2.1\\\\%}\\n1.00−0.0%\\\\scriptstyle{\\\\scriptstyle-0.0\\\\%}\\n95.1↑14.7%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 14.7\\\\%}\\n3597↑35.1%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 35.1\\\\%}\\n1.00−0.0%\\\\scriptstyle{\\\\scriptstyle-0.0\\\\%}\\nCold-Start\\n85.8↑6.2%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 6.2\\\\%}\\n18138↓14.7%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 14.7\\\\%}\\n1.00−0.0%\\\\scriptstyle{\\\\scriptstyle-0.0\\\\%}\\n76.7↑7.0%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 7.0\\\\%}\\n20613↓13.4%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 13.4\\\\<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[32m⠇\u001b[0m Generating qa content from data/output/arxiv_org_7_0_0.txt...vLLM STDOUT: INFO 10-03 08:35:37 [engine.py:317] Added request chatcmpl-32859a13e5a64e9dbc6e7acf96d67e95.\n",
            "\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from data/output/arxiv_org_7_0_0.txt...vLLM STDOUT: INFO:     127.0.0.1:35412 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 18 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/arxiv_org_7_0_0_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/arxiv_org_7_0_0_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from data/output/arxiv_org_7_0_0.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/arxiv_org_7_0_0_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:46602 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:46618 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:35:46 [logger.py:43] Received request chatcmpl-fddcb46d06314cf68362873c20dcf611: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{\\\\scriptstyle 0.0\\\\%}\\n76.3↑3.3%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 3.3\\\\%}\\n12628↓0.3%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 0.3\\\\%}\\n1.00−0.0%\\\\scriptstyle{\\\\scriptstyle-0.0\\\\%}\\nCold-Start\\n93.0↑1.1%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 1.1\\\\%}\\n5215↓16.4%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 16.4\\\\%}\\n0.65↓35.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 35.0\\\\%}\\n61.6↑0.8%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 0.8\\\\%}\\n11172↑3.1%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 3.1\\\\%}\\n0.95↓4.5%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 4.5\\\\%}\\n71.4↓0.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 0.8\\\\%}\\n3561↓19.3%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 19.3\\\\%}\\n0.42↓58.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 58.0\\\\%}\\n76.8↑4.1%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 4.1\\\\%}\\n11304↓10.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 10.8\\\\%}\\n0.78↓21.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 21.8\\\\%}\\n+ GRPO\\n92.8↑0.9%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 0.9\\\\%}\\n5204↓16.6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 16.6\\\\%}\\n0.68↓31.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 31.8\\\\%}\\n58.6↓4.1%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 4.1\\\\%}\\n10581↓2.3%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 2.3\\\\%}\\n0.98↓2.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 2.0\\\\%}\\n72.0−0.0%\\\\scriptstyle{\\\\scriptstyle-0.0\\\\%}\\n4341↓1.6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 1.6\\\\%}\\n0.38↓62.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 62.0\\\\%}\\n77.0↑4.3%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 4.3\\\\%}\\n11049 ↓12.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 12.8\\\\%}\\n0.79↓20.6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 20.6\\\\%}\\nAdaptThink\\n92.8↑0.9%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 0.9\\\\%}\\n4213↓32.5%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 32.5\\\\%}\\n0.55↓45.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 45.0\\\\%}\\n56.1↓8.2%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 8.2\\\\%}\\n10242↓5.4%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 5.4\\\\%}\\n0.91↓9.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 9.0\\\\%}\\n68.0↓5.6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 5.6\\\\%}\\n4165↓5.6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 5.6\\\\%}\\n0.33↓67.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 67.0\\\\%}\\n73.8−0.0%\\\\scriptstyle-{\\\\scriptstyle 0.0\\\\%}\\n10327↓18.5%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 18.5\\\\%}\\n0.64↓36.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 36.0\\\\%}\\nAutoThink\\n92.8↑0.9%\\\\scriptstyle<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:35:46 [engine.py:317] Added request chatcmpl-fddcb46d06314cf68362873c20dcf611.\n",
            "\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from data/output/arxiv_org_10_0.txt...vLLM STDOUT: INFO:     127.0.0.1:46622 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:35:48 [logger.py:43] Received request chatcmpl-f563c73e22aa49cc8a84a3f8e4678de3: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\n{\\\\scriptstyle 0.0\\\\%}\\n76.3↑3.3%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 3.3\\\\%}\\n12628↓0.3%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 0.3\\\\%}\\n1.00−0.0%\\\\scriptstyle{\\\\scriptstyle-0.0\\\\%}\\nCold-Start\\n93.0↑1.1%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 1.1\\\\%}\\n5215↓16.4%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 16.4\\\\%}\\n0.65↓35.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 35.0\\\\%}\\n61.6↑0.8%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 0.8\\\\%}\\n11172↑3.1%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 3.1\\\\%}\\n0.95↓4.5%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 4.5\\\\%}\\n71.4↓0.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 0.8\\\\%}\\n3561↓19.3%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 19.3\\\\%}\\n0.42↓58.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 58.0\\\\%}\\n76.8↑4.1%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 4.1\\\\%}\\n11304↓10.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 10.8\\\\%}\\n0.78↓21.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 21.8\\\\%}\\n+ GRPO\\n92.8↑0.9%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 0.9\\\\%}\\n5204↓16.6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 16.6\\\\%}\\n0.68↓31.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 31.8\\\\%}\\n58.6↓4.1%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 4.1\\\\%}\\n10581↓2.3%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 2.3\\\\%}\\n0.98↓2.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 2.0\\\\%}\\n72.0−0.0%\\\\scriptstyle{\\\\scriptstyle-0.0\\\\%}\\n4341↓1.6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 1.6\\\\%}\\n0.38↓62.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 62.0\\\\%}\\n77.0↑4.3%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 4.3\\\\%}\\n11049 ↓12.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 12.8\\\\%}\\n0.79↓20.6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 20.6\\\\%}\\nAdaptThink\\n92.8↑0.9%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 0.9\\\\%}\\n4213↓32.5%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 32.5\\\\%}\\n0.55↓45.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 45.0\\\\%}\\n56.1↓8.2%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 8.2\\\\%}\\n10242↓5.4%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 5.4\\\\%}\\n0.91↓9.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 9.0\\\\%}\\n68.0↓5.6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 5.6\\\\%}\\n4165↓5.6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 5.6\\\\%}\\n0.33↓67.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 67.0\\\\%}\\n73.8−0.0%\\\\scriptstyle-{\\\\scriptstyle 0.0\\\\%}\\n10327↓18.5%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 18.5\\\\%}\\n0.64↓36.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 36.0\\\\%}\\nAutoThink\\n92.8↑0.9%\\\\scriptstyle<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:35:48 [engine.py:317] Added request chatcmpl-f563c73e22aa49cc8a84a3f8e4678de3.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from data/output/arxiv_org_10_0.txt...vLLM STDOUT: INFO:     127.0.0.1:46634 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 14 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/arxiv_org_10_0_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/arxiv_org_10_0_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from data/output/arxiv_org_10_0.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/arxiv_org_10_0_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:54724 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:54738 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:35:57 [logger.py:43] Received request chatcmpl-56f04d672ab8427793834ec14ce1de8b: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nXiao Bi, Xiaokang Zhang, Xingkai Yu, Yu\\xa0Wu, Z.\\xa0F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H.\\xa0Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J.\\xa0L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong\\nZhang, Ruizhe Pan, Runji Wang, R.\\xa0J. Chen, R.\\xa0L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S.\\xa0S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T.\\xa0Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W.\\xa0L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X.\\xa0Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y.\\xa0K. Li, Y.\\xa0Q. Wang, Y.\\xa0X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi\\xa0Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y.\\xa0X. Zhu,\\nYanhong Xu, Yanping Huang, Yaohui Li, Yi\\xa0Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z.\\xa0Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang.\\nDeepseek-r1: Incentivizing reasoning capability in llms via reinforcement<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:35:57 [engine.py:317] Added request chatcmpl-56f04d672ab8427793834ec14ce1de8b.\n",
            "\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from data/output/arxiv_org_19_0_0.txt...vLLM STDOUT: INFO:     127.0.0.1:54744 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:36:00 [logger.py:43] Received request chatcmpl-4e54d3baa7da4dc9b8c4535de5f4814e: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\n Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu\\xa0Wu, Z.\\xa0F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H.\\xa0Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J.\\xa0L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong\\nZhang, Ruizhe Pan, Runji Wang, R.\\xa0J. Chen, R.\\xa0L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S.\\xa0S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T.\\xa0Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W.\\xa0L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X.\\xa0Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y.\\xa0K. Li, Y.\\xa0Q. Wang, Y.\\xa0X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi\\xa0Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y.\\xa0X. Zhu,\\nYanhong Xu, Yanping Huang, Yaohui Li, Yi\\xa0Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z.\\xa0Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang.\\nDeepseek-r1: Incentivizing reasoning capability in llms via reinforcement<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[32m⠼\u001b[0m Generating qa content from data/output/arxiv_org_19_0_0.txt...vLLM STDOUT: INFO 10-03 08:36:00 [engine.py:317] Added request chatcmpl-4e54d3baa7da4dc9b8c4535de5f4814e.\n",
            "\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from data/output/arxiv_org_19_0_0.txt...vLLM STDOUT: INFO:     127.0.0.1:54752 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 13 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/arxiv_org_19_0_0_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/arxiv_org_19_0_0_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from data/output/arxiv_org_19_0_0.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/arxiv_org_19_0_0_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:37888 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:37898 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:36:09 [logger.py:43] Received request chatcmpl-fe25f367a596407a9e4c31a55f42767d: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nHiPO: Hybrid Policy Optimization for Dynamic Reasoning in LLMs\\n1 Introduction\\n2 Related Works\\n3 Method\\n3.1 Hybrid Data Construction Pipeline\\n3.1.1 Data Source\\n3.1.2 Data Collection\\n3.1.3 Data format\\n3.2 Hybrid RL Reward System\\n3.2.1 Basic Reward Formulation\\n3.2.2 Bias Adjustment Mechanism\\n3.2.3 Supervision RL with HiPO\\n3.3 Training Paradigm\\n4 Experiments\\n4.1 Experimental setup\\n4.2 Main Results\\n4.3 Ablation Study\\n4.4 Further Analysis\\nThink-on vs. Think-off Dynamics During Training and Inference\\nToken Count Dynamics During Training and Inference\\nGeneralization on More Models\\n5 Conclusion\\nA Appendix\\nA.1 Use of LLMs\\nA.2 The decline in Qwen3’s performance on the test set.\\nA.3 Data Source\\nA.4 Prompt Templates\\nHiPO: Hybrid Policy Optimization for Dynamic Reasoning in LLMs\\nKen Deng∗,\\nZizheng Zhan∗,\\nWen Xiang∗,\\nWenqiang Zhu∗,\\nTianhao Peng,\\nXinping Lei,\\nWeihao Li,\\nJingxuan Xu,\\nKun Wu,\\nYifan Yao,\\nHaoyang Huang,\\nHuaixi Tang,\\nKepeng Lei,\\nZhiyi Lai,\\nSongwei Yu,\\nZongxian Feng,\\nZuchen Gao,\\nWeihao Xie,\\nChenchen Zhang,\\nYanan Wu,\\nYuanxing Zhang,\\nLecheng Huang,\\nYuqun Zhang,\\nJie Liu,\\nZhaoxiang Zhang,\\nHaotian Zhang,\\nBin Chen,\\nJiaheng Liu†\\nKuaishou Technology, Nanjing University\\ndengken@kuaishou.com, liujiaheng@nju.edu.cn\\n††footnotetext: *\\xa0Equal Contribution. \\xa0\\xa0†\\xa0Corresponding Author.\\nAbstract\\nLarge Language Models (LLMs) increasingly rely on chain-of-thought (CoT) reasoning to improve accuracy on complex tasks. However, always generating lengthy reasoning traces is inefficient, leading to excessive token usage and higher inference costs. This paper introduces the Hybrid Policy Optimization (i.e., HiPO), a framework for adaptive reasoning control that enables LLMs to selectively decide when to engage in detailed reasoning (Think-on) and when to respond directly (Think-off).\\nSpecifically, HiPO combines a hybrid data pipeline—providing paired Think-on and Think-off responses—with a hybrid reinforcement learning reward system that balances accuracy and efficiency while avoiding over-reliance on detailed reasoning.\\nExperiments across mathematics and coding benchmarks demonstrate that HiPO can substantially reduce token length while maintaining or improving accuracy.\\nFinally, we hope HiPO\\xa0111https://huggingface.co/Kwaipilot/HiPO-8B can be a principled approach for efficient adaptive reasoning, advancing the deployment of reasoning-oriented LLMs in real-world, resource-sensitive settings.\\n1 Introduction\\nLarge Language Models (LLMs) have achieved unprecedented success across diverse cognitive tasks, from code generation and mathematical reasoning to scientific problem-solving. A key driver of this progress is the integration of Chain-of-Thought (CoT)\\xa0(Yao et\\xa0al., 2023; Wei et\\xa0al., 2023) reasoning—a paradigm where models decompose complex queries into sequential, interpretable steps to derive accurate outputs.\\nThese approaches enhance accuracy on challenging problems but also introduce a persistent drawback: overthinking\\xa0(Kumar et\\xa0al., 2025; Sui et\\xa0al., 2025; Nayab et\\xa0al., 2025). Even for trivial queries, models often generate unnecessarily long reasoning chains, leading to inflated token usage, higher latency, and reduced efficiency in interactive applications. This inefficiency creates a fundamental tension between reasoning quality and computational cost, raising the need for mechanisms that can adaptively regulate reasoning depth.\\nRecently, recent work has explored adaptive reasoning control to mitigate overthinking,\\nand can be divided into two categories: (i) training-based adaptive reasoning, where reinforcement learning (RL)\\xa0(Aggarwal and Welleck, 2025; Arora and Zanette, 2025; Hou et\\xa0al., 2025; Luo et\\xa0al., 2025; Shen et\\xa0al., 2025;<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:36:09 [engine.py:317] Added request chatcmpl-fe25f367a596407a9e4c31a55f42767d.\n",
            "\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from data/output/arxiv_org_0_0_0.txt...vLLM STDOUT: INFO:     127.0.0.1:37910 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:36:11 [logger.py:43] Received request chatcmpl-1b139f1e1a4848989b7593b3bc330a8b: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nHiPO: Hybrid Policy Optimization for Dynamic Reasoning in LLMs\\n1 Introduction\\n2 Related Works\\n3 Method\\n3.1 Hybrid Data Construction Pipeline\\n3.1.1 Data Source\\n3.1.2 Data Collection\\n3.1.3 Data format\\n3.2 Hybrid RL Reward System\\n3.2.1 Basic Reward Formulation\\n3.2.2 Bias Adjustment Mechanism\\n3.2.3 Supervision RL with HiPO\\n3.3 Training Paradigm\\n4 Experiments\\n4.1 Experimental setup\\n4.2 Main Results\\n4.3 Ablation Study\\n4.4 Further Analysis\\nThink-on vs. Think-off Dynamics During Training and Inference\\nToken Count Dynamics During Training and Inference\\nGeneralization on More Models\\n5 Conclusion\\nA Appendix\\nA.1 Use of LLMs\\nA.2 The decline in Qwen3’s performance on the test set.\\nA.3 Data Source\\nA.4 Prompt Templates\\nHiPO: Hybrid Policy Optimization for Dynamic Reasoning in LLMs\\nKen Deng∗,\\nZizheng Zhan∗,\\nWen Xiang∗,\\nWenqiang Zhu∗,\\nTianhao Peng,\\nXinping Lei,\\nWeihao Li,\\nJingxuan Xu,\\nKun Wu,\\nYifan Yao,\\nHaoyang Huang,\\nHuaixi Tang,\\nKepeng Lei,\\nZhiyi Lai,\\nSongwei Yu,\\nZongxian Feng,\\nZuchen Gao,\\nWeihao Xie,\\nChenchen Zhang,\\nYanan Wu,\\nYuanxing Zhang,\\nLecheng Huang,\\nYuqun Zhang,\\nJie Liu,\\nZhaoxiang Zhang,\\nHaotian Zhang,\\nBin Chen,\\nJiaheng Liu†\\nKuaishou Technology, Nanjing University\\ndengken@kuaishou.com, liujiaheng@nju.edu.cn\\n††footnotetext: *\\xa0Equal Contribution. \\xa0\\xa0†\\xa0Corresponding Author.\\nAbstract\\nLarge Language Models (LLMs) increasingly rely on chain-of-thought (CoT) reasoning to improve accuracy on complex tasks. However, always generating lengthy reasoning traces is inefficient, leading to excessive token usage and higher inference costs. This paper introduces the Hybrid Policy Optimization (i.e., HiPO), a framework for adaptive reasoning control that enables LLMs to selectively decide when to engage in detailed reasoning (Think-on) and when to respond directly (Think-off).\\nSpecifically, HiPO combines a hybrid data pipeline—providing paired Think-on and Think-off responses—with a hybrid reinforcement learning reward system that balances accuracy and efficiency while avoiding over-reliance on detailed reasoning.\\nExperiments across mathematics and coding benchmarks demonstrate that HiPO can substantially reduce token length while maintaining or improving accuracy.\\nFinally, we hope HiPO\\xa0111https://huggingface.co/Kwaipilot/HiPO-8B can be a principled approach for efficient adaptive reasoning, advancing the deployment of reasoning-oriented LLMs in real-world, resource-sensitive settings.\\n1 Introduction\\nLarge Language Models (LLMs) have achieved unprecedented success across diverse cognitive tasks, from code generation and mathematical reasoning to scientific problem-solving. A key driver of this progress is the integration of Chain-of-Thought (CoT)\\xa0(Yao et\\xa0al., 2023; Wei et\\xa0al., 2023) reasoning—a paradigm where models decompose complex queries into sequential, interpretable steps to derive accurate outputs.\\nThese approaches enhance accuracy on challenging problems but also introduce a persistent drawback: overthinking\\xa0(Kumar et\\xa0al., 2025; Sui et\\xa0al., 2025; Nayab et\\xa0al., 2025). Even for trivial queries, models often generate unnecessarily long reasoning chains, leading to inflated token usage, higher latency, and reduced efficiency in interactive applications. This inefficiency creates a fundamental tension between reasoning quality and computational cost, raising the need for mechanisms that can adaptively regulate reasoning depth.\\nRecently, recent work has explored adaptive reasoning control to mitigate overthinking,\\nand can be divided into two categories: (i) training-based adaptive reasoning, where reinforcement learning (RL)\\xa0(Aggarwal and Welleck, 2025; Arora and Zanette, 2025; Hou et\\xa0al., 2025; Luo et\\xa0al., 2025; Shen et\\xa0al., 2025;<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:36:11 [engine.py:317] Added request chatcmpl-1b139f1e1a4848989b7593b3bc330a8b.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from data/output/arxiv_org_0_0_0.txt...vLLM STDOUT: INFO:     127.0.0.1:37912 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 10 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/arxiv_org_0_0_0_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/arxiv_org_0_0_0_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from data/output/arxiv_org_0_0_0.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/arxiv_org_0_0_0_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:55708 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:55720 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:36:20 [logger.py:43] Received request chatcmpl-fd1181c73dd841ef80997dec3e16cb8c: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\ndownarrow{\\\\scriptstyle 2.0\\\\%}\\n81.7↑5.2%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 5.2\\\\%}\\n3084↓26.1%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 26.1\\\\%}\\n0.39↓61.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 61.0\\\\%}\\n54.4↓0.4%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 0.4\\\\%}\\n6398↓25.5%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 25.5\\\\%}\\n0.64↓36.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 36.0\\\\%}\\nHiPO\\n68.3↑7.9%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 7.9\\\\%}\\n17614↓27.3%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 27.3\\\\%}\\n0.98↓6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 6\\\\%}\\n44.3↑31.4%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 31.4\\\\%}\\n19358↓24.4%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 24.4\\\\%}\\n0.92↓8.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 8.0\\\\%}\\n86.0↑11.1%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 11.1\\\\%}\\n1973↓52.7%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 52.7\\\\%}\\n0.28↓62.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 62.0\\\\%}\\n62.8↑15.0%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 15.0\\\\%}\\n4330↓49.6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 49.6\\\\%}\\n0.47↓53.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 53.0\\\\%}\\nQwen3-32B\\nCold-Start (On)\\n81.7\\n19551\\n1.00\\n65.4\\n17885\\n1.00\\n87.8\\n4298\\n1.00\\n76.2\\n4753\\n1.00\\nCold-Start\\n85.0↑4.3%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 4.3\\\\%}\\n16542↓15.4%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 15.4\\\\%}\\n1.00−0.0%\\\\scriptstyle{\\\\scriptstyle-0.0\\\\%}\\n65.9↑0.8%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 0.8\\\\%}\\n14935↓16.5%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 16.5\\\\%}\\n0.87↓13.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 13.0\\\\%}\\n92.1↑4.9%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 4.9\\\\%}\\n2785↓35.2%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 35.2\\\\%}\\n0.47↓53.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 53.0\\\\%}\\n78.4↑2.2%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 2.2\\\\%}\\n3991↓16.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 16.0\\\\%}\\n0.51↓49.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 49.0\\\\%}\\nHiPO\\n88.3↑8.1%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 8.1\\\\%}\\n14873↓23.9%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 23.9\\\\%}\\n0.98↓2.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 2.0\\\\%}\\n68.5↑4.5%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 4.5\\\\%}\\n12721↓28.9%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 28.9\\\\%}\\n0.82↓18.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 18.0\\\\%}\\n92.7↑5.6%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 5.6\\\\%<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:36:20 [engine.py:317] Added request chatcmpl-fd1181c73dd841ef80997dec3e16cb8c.\n",
            "\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from data/output/arxiv_org_14_0_0.txt...vLLM STDOUT: INFO:     127.0.0.1:55724 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:36:22 [logger.py:43] Received request chatcmpl-7c28a8dc9ee9488a93b8cd40772237ee: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\ndownarrow{\\\\scriptstyle 2.0\\\\%}\\n81.7↑5.2%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 5.2\\\\%}\\n3084↓26.1%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 26.1\\\\%}\\n0.39↓61.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 61.0\\\\%}\\n54.4↓0.4%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 0.4\\\\%}\\n6398↓25.5%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 25.5\\\\%}\\n0.64↓36.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 36.0\\\\%}\\nHiPO\\n68.3↑7.9%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 7.9\\\\%}\\n17614↓27.3%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 27.3\\\\%}\\n0.98↓6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 6\\\\%}\\n44.3↑31.4%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 31.4\\\\%}\\n19358↓24.4%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 24.4\\\\%}\\n0.92↓8.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 8.0\\\\%}\\n86.0↑11.1%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 11.1\\\\%}\\n1973↓52.7%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 52.7\\\\%}\\n0.28↓62.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 62.0\\\\%}\\n62.8↑15.0%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 15.0\\\\%}\\n4330↓49.6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 49.6\\\\%}\\n0.47↓53.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 53.0\\\\%}\\nQwen3-32B\\nCold-Start (On)\\n81.7\\n19551\\n1.00\\n65.4\\n17885\\n1.00\\n87.8\\n4298\\n1.00\\n76.2\\n4753\\n1.00\\nCold-Start\\n85.0↑4.3%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 4.3\\\\%}\\n16542↓15.4%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 15.4\\\\%}\\n1.00−0.0%\\\\scriptstyle{\\\\scriptstyle-0.0\\\\%}\\n65.9↑0.8%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 0.8\\\\%}\\n14935↓16.5%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 16.5\\\\%}\\n0.87↓13.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 13.0\\\\%}\\n92.1↑4.9%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 4.9\\\\%}\\n2785↓35.2%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 35.2\\\\%}\\n0.47↓53.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 53.0\\\\%}\\n78.4↑2.2%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 2.2\\\\%}\\n3991↓16.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 16.0\\\\%}\\n0.51↓49.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 49.0\\\\%}\\nHiPO\\n88.3↑8.1%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 8.1\\\\%}\\n14873↓23.9%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 23.9\\\\%}\\n0.98↓2.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 2.0\\\\%}\\n68.5↑4.5%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 4.5\\\\%}\\n12721↓28.9%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 28.9\\\\%}\\n0.82↓18.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 18.0\\\\%}\\n92.7↑5.6%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 5.6\\\\%<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:36:22 [engine.py:317] Added request chatcmpl-7c28a8dc9ee9488a93b8cd40772237ee.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from data/output/arxiv_org_14_0_0.txt...vLLM STDOUT: INFO:     127.0.0.1:55734 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 15 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/arxiv_org_14_0_0_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/arxiv_org_14_0_0_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from data/output/arxiv_org_14_0_0.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/arxiv_org_14_0_0_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:46478 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:46484 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:36:31 [logger.py:43] Received request chatcmpl-5bffdd56684b46189a4130215c454b91: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang.\\nDeepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025.\\nURL https://arxiv.org/abs/2501.12948.\\nZhan et\\xa0al. [2025]\\nZizheng Zhan, Ken Deng, Huaixi Tang, Wen Xiang, Kun Wu, Weihao Li, Wenqiang Zhu, Jingxuan Xu, Lecheng Huang, Zongxian Feng, Shaojie Wang, Shangpeng Yan, Xuxing Chen, Jiaheng Liu, Zhongyuan Peng, Zuchen Gao, Haoyang Huang, Xiaojiang Zhang, Jinghui Wang, Zheng Lin, Mengtong Li, Huiming Wang, Ziqi Zhan, Yanan Wu, Yuanxing Zhang, Jian Yang, Guang Chen, Haotian Zhang, Bin Chen, and Bing Yu.\\nKat-v1: Kwai-autothink technical report, 2025.\\nURL https://arxiv.org/abs/2507.08297.\\nAytes et\\xa0al. [2025]\\nSimon\\xa0A. Aytes, Jinheon Baek, and Sung\\xa0Ju Hwang.\\nSketch-of-thought: Efficient llm reasoning with adaptive cognitive-inspired sketching, 2025.\\nURL https://arxiv.org/abs/2503.05179.\\nXia et\\xa0al. [2025]\\nHeming Xia, Chak\\xa0Tou Leong, Wenjie Wang, Yongqi Li, and Wenjie Li.\\nTokenskip: Controllable chain-of-thought compression in llms, 2025.\\nURL https://arxiv.org/abs/2502.12067.\\nLiu et\\xa0al. [2024b]\\nTengxiao Liu, Qipeng Guo, Xiangkun Hu, Cheng Jiayang, Yue Zhang, Xipeng Qiu, and Zheng Zhang.\\nCan language models learn to skip steps?, 2024b.\\nURL https://arxiv.org/abs/2411.01855.\\nSun et\\xa0al. [2024]\\nHanshi Sun, Momin Haider, Ruiqi Zhang, Huitao Yang, Jiahao Qiu, Ming Yin, Mengdi Wang, Peter Bartlett, and Andrea Zanette.\\nFast best-of-n decoding via speculative rejection, 2024.\\nURL https://arxiv.org/abs/2410.20290.\\nYang et\\xa0al. [2025]\\nChenxu Yang, Qingyi Si, Yongjie Duan, Zheliang Zhu, Chenyu Zhu, Qiaowei Li, Zheng Lin, Li\\xa0Cao, and Weiping Wang.\\nDynamic early exit in reasoning models, 2025.\\nURL https://arxiv.org/abs/2504.15895.\\nTian et\\xa0al. [2025]\\nXiaoyu Tian, Yunjie Ji, Haotian Wang, Shuaiting Chen, Sitong Zhao, Yiping Peng, Han Zhao, and Xiangang Li.\\nNot all correct answers are equal: Why your distillation source matters.\\narXiv preprint arXiv:2505.14464, 2025.\\nInternet [2025]\\nIntelligent Internet.\\nIi-thought : A large-scale, high-quality reasoning dataset, 2025.\\nChen et\\xa0al. [2025b]\\nYang Chen, Zhuolin Yang, Zihan Liu, Chankyu Lee, Peng Xu, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping.\\nAcereason-nemotron: Advancing math and code reasoning through reinforcement learning.\\narXiv preprint arXiv:2505.16400, 2025b.\\nHe et\\xa0al. [2025]\\nJujie He, Jiacai Liu, Chris\\xa0Yuhao Liu, Rui Yan, Chaojie Wang, Peng Cheng, Xiaoyu Zhang, Fuxiang Zhang, Jiacheng Xu, Wei Shen, Siyuan Li, Liang Zeng, Tianwen Wei, Cheng Cheng, Yang Liu, and Yahui Zhou.\\nSkywork open reasoner series.\\nhttps://capricious-hydrogen<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:36:31 [engine.py:317] Added request chatcmpl-5bffdd56684b46189a4130215c454b91.\n",
            "\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from data/output/arxiv_org_20_0.txt...vLLM STDOUT: INFO:     127.0.0.1:46490 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:36:35 [logger.py:43] Received request chatcmpl-f0d8ba10ea2749bf9ea810257aaae259: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang.\\nDeepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025.\\nURL https://arxiv.org/abs/2501.12948.\\nZhan et\\xa0al. [2025]\\nZizheng Zhan, Ken Deng, Huaixi Tang, Wen Xiang, Kun Wu, Weihao Li, Wenqiang Zhu, Jingxuan Xu, Lecheng Huang, Zongxian Feng, Shaojie Wang, Shangpeng Yan, Xuxing Chen, Jiaheng Liu, Zhongyuan Peng, Zuchen Gao, Haoyang Huang, Xiaojiang Zhang, Jinghui Wang, Zheng Lin, Mengtong Li, Huiming Wang, Ziqi Zhan, Yanan Wu, Yuanxing Zhang, Jian Yang, Guang Chen, Haotian Zhang, Bin Chen, and Bing Yu.\\nKat-v1: Kwai-autothink technical report, 2025.\\nURL https://arxiv.org/abs/2507.08297.\\nAytes et\\xa0al. [2025]\\nSimon\\xa0A. Aytes, Jinheon Baek, and Sung\\xa0Ju Hwang.\\nSketch-of-thought: Efficient llm reasoning with adaptive cognitive-inspired sketching, 2025.\\nURL https://arxiv.org/abs/2503.05179.\\nXia et\\xa0al. [2025]\\nHeming Xia, Chak\\xa0Tou Leong, Wenjie Wang, Yongqi Li, and Wenjie Li.\\nTokenskip: Controllable chain-of-thought compression in llms, 2025.\\nURL https://arxiv.org/abs/2502.12067.\\nLiu et\\xa0al. [2024b]\\nTengxiao Liu, Qipeng Guo, Xiangkun Hu, Cheng Jiayang, Yue Zhang, Xipeng Qiu, and Zheng Zhang.\\nCan language models learn to skip steps?, 2024b.\\nURL https://arxiv.org/abs/2411.01855.\\nSun et\\xa0al. [2024]\\nHanshi Sun, Momin Haider, Ruiqi Zhang, Huitao Yang, Jiahao Qiu, Ming Yin, Mengdi Wang, Peter Bartlett, and Andrea Zanette.\\nFast best-of-n decoding via speculative rejection, 2024.\\nURL https://arxiv.org/abs/2410.20290.\\nYang et\\xa0al. [2025]\\nChenxu Yang, Qingyi Si, Yongjie Duan, Zheliang Zhu, Chenyu Zhu, Qiaowei Li, Zheng Lin, Li\\xa0Cao, and Weiping Wang.\\nDynamic early exit in reasoning models, 2025.\\nURL https://arxiv.org/abs/2504.15895.\\nTian et\\xa0al. [2025]\\nXiaoyu Tian, Yunjie Ji, Haotian Wang, Shuaiting Chen, Sitong Zhao, Yiping Peng, Han Zhao, and Xiangang Li.\\nNot all correct answers are equal: Why your distillation source matters.\\narXiv preprint arXiv:2505.14464, 2025.\\nInternet [2025]\\nIntelligent Internet.\\nIi-thought : A large-scale, high-quality reasoning dataset, 2025.\\nChen et\\xa0al. [2025b]\\nYang Chen, Zhuolin Yang, Zihan Liu, Chankyu Lee, Peng Xu, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping.\\nAcereason-nemotron: Advancing math and code reasoning through reinforcement learning.\\narXiv preprint arXiv:2505.16400, 2025b.\\nHe et\\xa0al. [2025]\\nJujie He, Jiacai Liu, Chris\\xa0Yuhao Liu, Rui Yan, Chaojie Wang, Peng Cheng, Xiaoyu Zhang, Fuxiang Zhang, Jiacheng Xu, Wei Shen, Siyuan Li, Liang Zeng, Tianwen Wei, Cheng Cheng, Yang Liu, and Yahui Zhou.\\nSkywork open reasoner series.\\nhttps://capricious-hydrogen<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:36:35 [engine.py:317] Added request chatcmpl-f0d8ba10ea2749bf9ea810257aaae259.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from data/output/arxiv_org_20_0.txt...vLLM STDOUT: INFO:     127.0.0.1:46110 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 12 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/arxiv_org_20_0_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/arxiv_org_20_0_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from data/output/arxiv_org_20_0.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/arxiv_org_20_0_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:43874 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:43888 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:36:44 [logger.py:43] Received request chatcmpl-7cbacf3699a54349b0097f976bcedfae: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nscriptstyle\\\\uparrow{\\\\scriptstyle 4.6\\\\%}\\n18784↓21.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 21.0\\\\%}\\n0.88↓12.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 12.0\\\\%}\\n57.5↑2.3%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 2.3\\\\%}\\n15672↓19.5%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 19.5\\\\%}\\n0.80↓20.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 20.0\\\\%}\\n82.3↓0.7%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 0.7\\\\%}\\n1050↓60.6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 60.6\\\\%}\\n0.18↓82.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 82.0\\\\%}\\nHiPO\\n87.5↑8.3%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 8.3\\\\%}\\n15107↓29.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 29.0\\\\%}\\n0.98↓1.7%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 1.7\\\\%}\\n82.5↑15.1%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 15.1\\\\%}\\n17655↓25.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 25.8\\\\%}\\n0.95↓5.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 5.0\\\\%}\\n63.0↑12.2%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 12.2\\\\%}\\n13558↓30.4%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 30.4\\\\%}\\n0.82↓18.5%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 18.5\\\\%}\\n90.2↑8.8%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 8.8\\\\%}\\n776↓70.9%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 70.9\\\\%}\\n0.12↓88.4%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 88.4\\\\%}\\nMATH-500\\nGPQA-Diamond\\nMBPP\\nAverage\\nMethod\\nAcc↑\\\\uparrow\\nLength↓\\\\downarrow\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}↓\\\\downarrow\\nAcc↑\\\\uparrow\\nLength↓\\\\downarrow\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}↓\\\\downarrow\\nAcc↑\\\\uparrow\\nLength↓\\\\downarrow\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}↓\\\\downarrow\\nAcc↑\\\\uparrow\\nLength↓\\\\downarrow\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}↓\\\\downarrow\\nCold-Start (on)\\n92.0\\n6237\\n1.00\\n61.1\\n10832\\n1.00\\n72.0\\n4411\\n1.00\\n73.8\\n12667\\n1.00\\n+ GRPO\\n93.2↑0.0%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 0.0\\\\%}\\n6256↑1.3%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 1.3\\\\%}\\n1.00−0.0%\\\\scriptstyle{\\\\scriptstyle-0.0\\\\%}\\n57.6↓5.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 5.8\\\\%}\\n10633↓1.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 1.8\\\\%}\\n1.00−0.0%\\\\scriptstyle{\\\\scriptstyle-0.0\\\\%}\\n71.8↓0.3%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 0.3\\\\%}\\n5103↑15.7%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 15.7\\\\%}\\n1.00−0.0%\\\\scriptstyle-{\\\\scriptstyle 0.0\\\\%}\\n76.3↑3.3%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 3.3\\\\%}\\n12628↓0.3%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 0.3\\\\%}\\n1.00−0.0%\\\\script<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:36:44 [engine.py:317] Added request chatcmpl-7cbacf3699a54349b0097f976bcedfae.\n",
            "\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from data/output/arxiv_org_9_0_0.txt...vLLM STDOUT: INFO:     127.0.0.1:43898 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:36:47 [logger.py:43] Received request chatcmpl-005f5297a77c4ac984c0c20ed6ddb800: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nscriptstyle\\\\uparrow{\\\\scriptstyle 4.6\\\\%}\\n18784↓21.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 21.0\\\\%}\\n0.88↓12.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 12.0\\\\%}\\n57.5↑2.3%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 2.3\\\\%}\\n15672↓19.5%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 19.5\\\\%}\\n0.80↓20.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 20.0\\\\%}\\n82.3↓0.7%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 0.7\\\\%}\\n1050↓60.6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 60.6\\\\%}\\n0.18↓82.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 82.0\\\\%}\\nHiPO\\n87.5↑8.3%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 8.3\\\\%}\\n15107↓29.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 29.0\\\\%}\\n0.98↓1.7%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 1.7\\\\%}\\n82.5↑15.1%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 15.1\\\\%}\\n17655↓25.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 25.8\\\\%}\\n0.95↓5.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 5.0\\\\%}\\n63.0↑12.2%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 12.2\\\\%}\\n13558↓30.4%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 30.4\\\\%}\\n0.82↓18.5%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 18.5\\\\%}\\n90.2↑8.8%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 8.8\\\\%}\\n776↓70.9%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 70.9\\\\%}\\n0.12↓88.4%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 88.4\\\\%}\\nMATH-500\\nGPQA-Diamond\\nMBPP\\nAverage\\nMethod\\nAcc↑\\\\uparrow\\nLength↓\\\\downarrow\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}↓\\\\downarrow\\nAcc↑\\\\uparrow\\nLength↓\\\\downarrow\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}↓\\\\downarrow\\nAcc↑\\\\uparrow\\nLength↓\\\\downarrow\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}↓\\\\downarrow\\nAcc↑\\\\uparrow\\nLength↓\\\\downarrow\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}↓\\\\downarrow\\nCold-Start (on)\\n92.0\\n6237\\n1.00\\n61.1\\n10832\\n1.00\\n72.0\\n4411\\n1.00\\n73.8\\n12667\\n1.00\\n+ GRPO\\n93.2↑0.0%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 0.0\\\\%}\\n6256↑1.3%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 1.3\\\\%}\\n1.00−0.0%\\\\scriptstyle{\\\\scriptstyle-0.0\\\\%}\\n57.6↓5.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 5.8\\\\%}\\n10633↓1.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 1.8\\\\%}\\n1.00−0.0%\\\\scriptstyle{\\\\scriptstyle-0.0\\\\%}\\n71.8↓0.3%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 0.3\\\\%}\\n5103↑15.7%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 15.7\\\\%}\\n1.00−0.0%\\\\scriptstyle-{\\\\scriptstyle 0.0\\\\%}\\n76.3↑3.3%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 3.3\\\\%}\\n12628↓0.3%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 0.3\\\\%}\\n1.00−0.0%\\\\script<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:36:47 [engine.py:317] Added request chatcmpl-005f5297a77c4ac984c0c20ed6ddb800.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from data/output/arxiv_org_9_0_0.txt...vLLM STDOUT: INFO:     127.0.0.1:43912 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 16 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/arxiv_org_9_0_0_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/arxiv_org_9_0_0_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from data/output/arxiv_org_9_0_0.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/arxiv_org_9_0_0_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:36110 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:36112 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:36:56 [logger.py:43] Received request chatcmpl-a2210846f8cd4bdc80302a4109716a12: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n=\\\\frac{\\\\pi_{\\\\theta}\\\\!\\\\left(y_{i,t}\\\\,\\\\middle|\\\\,h_{i,t}\\\\right)}{\\\\pi_{\\\\theta_{\\\\text{old}}}\\\\!\\\\left(y_{i,t}\\\\,\\\\middle|\\\\,h_{i,t}\\\\right)},\\nwhere yi,ty_{i,t} is the tt-th generated token in oio_{i} and hi,th_{i,t} is its conditioning context.\\nThe policy πθ\\\\pi_{\\\\theta} is optimized by maximizing the following token-level objective:\\n𝒥\\u200b(θ)\\\\displaystyle\\\\mathcal{J}(\\\\theta)\\n=𝔼[q∼P(Q),{oi}i=1G∼πθold(⋅|q)]\\\\displaystyle=\\\\mathbb{E}\\\\Big[q\\\\sim P(Q),\\\\{o_{i}\\\\}_{i=1}^{G}\\\\sim\\\\pi_{\\\\theta_{\\\\text{old}}}(\\\\cdot\\\\,|\\\\,q)\\\\Big]\\n(5)\\n⋅1G∑i=1G1|𝒯i|∑t∈𝒯i(min(ρi,tAi,t,clip(ρi,t,\\u20091−ϵ,\\u20091+ϵ)Ai,t)\\\\displaystyle\\\\quad\\\\cdot\\\\frac{1}{G}\\\\sum_{i=1}^{G}\\\\frac{1}{\\\\lvert\\\\mathcal{T}_{i}\\\\rvert}\\\\sum_{t\\\\in\\\\mathcal{T}_{i}}\\\\Big(\\\\min\\\\big(\\\\rho_{i,t}\\\\,A_{i,t},\\\\;\\\\text{clip}(\\\\rho_{i,t},1-\\\\epsilon,1+\\\\epsilon)\\\\,A_{i,t}\\\\big)\\n−β𝔻K\\u200bL(πθ(⋅|hi,t)∥πref(⋅|hi,t))).\\\\displaystyle\\\\qquad-\\\\beta\\\\,\\\\mathbb{D}_{KL}\\\\!\\\\big(\\\\pi_{\\\\theta}(\\\\cdot\\\\,|\\\\,h_{i,t})\\\\,\\\\big\\\\|\\\\,\\\\pi_{\\\\text{ref}}(\\\\cdot\\\\,|\\\\,h_{i,t})\\\\big)\\\\Big).\\nHere Ai,tA_{i,t} is the token-level advantage defined in Eq. (4) via segment-wise assignment, and 𝔻K\\u200bL\\\\mathbb{D}_{KL} is the token-level KL between the current policy and the reference policy at context hi,th_{i,t}.\\n3.3 Training Paradigm\\nOur HiPO framework adopts a two-stage training paradigm, consisting of a cold-start stage and a RL stage.\\nIn the code-start stage, the model is initialized with high-quality, hybrid training data that contains both Think-on and Think-off responses.\\nThis stage enables the model to acquire fundamental reasoning and answering capabilities, while establishing an initial balance between analytical reasoning and concise responses.\\nIn the RL stage, the model is further optimized using our hybrid reward system, which integrates mode-specific accuracy and global average performance.\\nTogether, these two stages ensure that HiPO achieves both strong factual accuracy and robust reasoning ability across diverse domains.\\n4 Experiments\\n4.1 Experimental setup\\nImplementation details.\\nSince the Qwen3 model can freely switch between inference modes, we chose it for our experiment. However, when the training data is insufficient, training the Qwen3 model can easily lead to a decline in performance on the test set (details can be found in the appendix A.2).\\nTo address this, we conducted Cold-Start tuning to stabilize its performance with relatively large datasets.\\nFor the Cold-Start stage, we use the “AM-Thinking-v1-Distilled”, “AceReason-Math”, “AM-Thinking”, “II-Thought-RL(math)” dataset for training. The parameters are set as: maximum learning rate is 8e-5, minimum learning rate is 8e-6\\nand batch size is 512. For the RL stage, we use the “II-Thought-RL(code)”, “Skywork-OR1-RL-Data” dataset for training. The parameters are set as: batch size = 16, maximum response length = 32k, NN = 16, ω\\\\omega = 0.01, and γ=0.3\\\\gamma=0.3.\\nBaselines.\\nTo demonstrate the effect of mitigating overthinking, we<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:36:56 [engine.py:317] Added request chatcmpl-a2210846f8cd4bdc80302a4109716a12.\n",
            "\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from data/output/arxiv_org_6_0.txt...vLLM STDOUT: INFO:     127.0.0.1:36124 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:36:58 [logger.py:43] Received request chatcmpl-edd38e90e0ce4dd69f7fb165008f8d1d: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\n=\\\\frac{\\\\pi_{\\\\theta}\\\\!\\\\left(y_{i,t}\\\\,\\\\middle|\\\\,h_{i,t}\\\\right)}{\\\\pi_{\\\\theta_{\\\\text{old}}}\\\\!\\\\left(y_{i,t}\\\\,\\\\middle|\\\\,h_{i,t}\\\\right)},\\nwhere yi,ty_{i,t} is the tt-th generated token in oio_{i} and hi,th_{i,t} is its conditioning context.\\nThe policy πθ\\\\pi_{\\\\theta} is optimized by maximizing the following token-level objective:\\n𝒥\\u200b(θ)\\\\displaystyle\\\\mathcal{J}(\\\\theta)\\n=𝔼[q∼P(Q),{oi}i=1G∼πθold(⋅|q)]\\\\displaystyle=\\\\mathbb{E}\\\\Big[q\\\\sim P(Q),\\\\{o_{i}\\\\}_{i=1}^{G}\\\\sim\\\\pi_{\\\\theta_{\\\\text{old}}}(\\\\cdot\\\\,|\\\\,q)\\\\Big]\\n(5)\\n⋅1G∑i=1G1|𝒯i|∑t∈𝒯i(min(ρi,tAi,t,clip(ρi,t,\\u20091−ϵ,\\u20091+ϵ)Ai,t)\\\\displaystyle\\\\quad\\\\cdot\\\\frac{1}{G}\\\\sum_{i=1}^{G}\\\\frac{1}{\\\\lvert\\\\mathcal{T}_{i}\\\\rvert}\\\\sum_{t\\\\in\\\\mathcal{T}_{i}}\\\\Big(\\\\min\\\\big(\\\\rho_{i,t}\\\\,A_{i,t},\\\\;\\\\text{clip}(\\\\rho_{i,t},1-\\\\epsilon,1+\\\\epsilon)\\\\,A_{i,t}\\\\big)\\n−β𝔻K\\u200bL(πθ(⋅|hi,t)∥πref(⋅|hi,t))).\\\\displaystyle\\\\qquad-\\\\beta\\\\,\\\\mathbb{D}_{KL}\\\\!\\\\big(\\\\pi_{\\\\theta}(\\\\cdot\\\\,|\\\\,h_{i,t})\\\\,\\\\big\\\\|\\\\,\\\\pi_{\\\\text{ref}}(\\\\cdot\\\\,|\\\\,h_{i,t})\\\\big)\\\\Big).\\nHere Ai,tA_{i,t} is the token-level advantage defined in Eq. (4) via segment-wise assignment, and 𝔻K\\u200bL\\\\mathbb{D}_{KL} is the token-level KL between the current policy and the reference policy at context hi,th_{i,t}.\\n3.3 Training Paradigm\\nOur HiPO framework adopts a two-stage training paradigm, consisting of a cold-start stage and a RL stage.\\nIn the code-start stage, the model is initialized with high-quality, hybrid training data that contains both Think-on and Think-off responses.\\nThis stage enables the model to acquire fundamental reasoning and answering capabilities, while establishing an initial balance between analytical reasoning and concise responses.\\nIn the RL stage, the model is further optimized using our hybrid reward system, which integrates mode-specific accuracy and global average performance.\\nTogether, these two stages ensure that HiPO achieves both strong factual accuracy and robust reasoning ability across diverse domains.\\n4 Experiments\\n4.1 Experimental setup\\nImplementation details.\\nSince the Qwen3 model can freely switch between inference modes, we chose it for our experiment. However, when the training data is insufficient, training the Qwen3 model can easily lead to a decline in performance on the test set (details can be found in the appendix A.2).\\nTo address this, we conducted Cold-Start tuning to stabilize its performance with relatively large datasets.\\nFor the Cold-Start stage, we use the “AM-Thinking-v1-Distilled”, “AceReason-Math”, “AM-Thinking”, “II-Thought-RL(math)” dataset for training. The parameters are set as: maximum learning rate is 8e-5, minimum learning rate is 8e-6\\nand batch size is 512. For the RL stage, we use the “II-Thought-RL(code)”, “Skywork-OR1-RL-Data” dataset for training. The parameters are set as: batch size = 16, maximum response length = 32k, NN = 16, ω\\\\omega = 0.01, and γ=0.3\\\\gamma=0.3.\\nBaselines.\\nTo demonstrate the effect of mitigating overthinking, we<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:36:58 [engine.py:317] Added request chatcmpl-edd38e90e0ce4dd69f7fb165008f8d1d.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from data/output/arxiv_org_6_0.txt...vLLM STDOUT: INFO:     127.0.0.1:36128 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 4 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/arxiv_org_6_0_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/arxiv_org_6_0_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from data/output/arxiv_org_6_0.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/arxiv_org_6_0_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:59578 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:59588 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:37:07 [logger.py:43] Received request chatcmpl-4694ab04aa484a2cb0b0926deb28a349: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{\\\\scriptstyle 0.0\\\\%}\\n76.3↑3.3%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 3.3\\\\%}\\n12628↓0.3%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 0.3\\\\%}\\n1.00−0.0%\\\\scriptstyle{\\\\scriptstyle-0.0\\\\%}\\nCold-Start\\n93.0↑1.1%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 1.1\\\\%}\\n5215↓16.4%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 16.4\\\\%}\\n0.65↓35.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 35.0\\\\%}\\n61.6↑0.8%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 0.8\\\\%}\\n11172↑3.1%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 3.1\\\\%}\\n0.95↓4.5%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 4.5\\\\%}\\n71.4↓0.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 0.8\\\\%}\\n3561↓19.3%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 19.3\\\\%}\\n0.42↓58.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 58.0\\\\%}\\n76.8↑4.1%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 4.1\\\\%}\\n11304↓10.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 10.8\\\\%}\\n0.78↓21.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 21.8\\\\%}\\n+ GRPO\\n92.8↑0.9%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 0.9\\\\%}\\n5204↓16.6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 16.6\\\\%}\\n0.68↓31.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 31.8\\\\%}\\n58.6↓4.1%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 4.1\\\\%}\\n10581↓2.3%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 2.3\\\\%}\\n0.98↓2.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 2.0\\\\%}\\n72.0−0.0%\\\\scriptstyle{\\\\scriptstyle-0.0\\\\%}\\n4341↓1.6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 1.6\\\\%}\\n0.38↓62.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 62.0\\\\%}\\n77.0↑4.3%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 4.3\\\\%}\\n11049 ↓12.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 12.8\\\\%}\\n0.79↓20.6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 20.6\\\\%}\\nAdaptThink\\n92.8↑0.9%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 0.9\\\\%}\\n4213↓32.5%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 32.5\\\\%}\\n0.55↓45.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 45.0\\\\%}\\n56.1↓8.2%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 8.2\\\\%}\\n10242↓5.4%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 5.4\\\\%}\\n0.91↓9.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 9.0\\\\%}\\n68.0↓5.6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 5.6\\\\%}\\n4165↓5.6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 5.6\\\\%}\\n0.33↓67.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 67.0\\\\%}\\n73.8−0.0%\\\\scriptstyle-{\\\\scriptstyle 0.0\\\\%}\\n10327↓18.5%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 18.5\\\\%}\\n0.64↓36.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 36.0\\\\%}\\nAutoThink\\n92.8↑0.9%\\\\scriptstyle<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:37:07 [engine.py:317] Added request chatcmpl-4694ab04aa484a2cb0b0926deb28a349.\n",
            "\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from data/output/arxiv_org_10_0_0.txt...vLLM STDOUT: INFO:     127.0.0.1:59604 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:37:10 [logger.py:43] Received request chatcmpl-3fce46ccc54244b5a2e540ed3927fa54: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\n{\\\\scriptstyle 0.0\\\\%}\\n76.3↑3.3%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 3.3\\\\%}\\n12628↓0.3%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 0.3\\\\%}\\n1.00−0.0%\\\\scriptstyle{\\\\scriptstyle-0.0\\\\%}\\nCold-Start\\n93.0↑1.1%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 1.1\\\\%}\\n5215↓16.4%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 16.4\\\\%}\\n0.65↓35.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 35.0\\\\%}\\n61.6↑0.8%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 0.8\\\\%}\\n11172↑3.1%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 3.1\\\\%}\\n0.95↓4.5%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 4.5\\\\%}\\n71.4↓0.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 0.8\\\\%}\\n3561↓19.3%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 19.3\\\\%}\\n0.42↓58.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 58.0\\\\%}\\n76.8↑4.1%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 4.1\\\\%}\\n11304↓10.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 10.8\\\\%}\\n0.78↓21.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 21.8\\\\%}\\n+ GRPO\\n92.8↑0.9%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 0.9\\\\%}\\n5204↓16.6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 16.6\\\\%}\\n0.68↓31.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 31.8\\\\%}\\n58.6↓4.1%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 4.1\\\\%}\\n10581↓2.3%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 2.3\\\\%}\\n0.98↓2.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 2.0\\\\%}\\n72.0−0.0%\\\\scriptstyle{\\\\scriptstyle-0.0\\\\%}\\n4341↓1.6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 1.6\\\\%}\\n0.38↓62.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 62.0\\\\%}\\n77.0↑4.3%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 4.3\\\\%}\\n11049 ↓12.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 12.8\\\\%}\\n0.79↓20.6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 20.6\\\\%}\\nAdaptThink\\n92.8↑0.9%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 0.9\\\\%}\\n4213↓32.5%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 32.5\\\\%}\\n0.55↓45.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 45.0\\\\%}\\n56.1↓8.2%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 8.2\\\\%}\\n10242↓5.4%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 5.4\\\\%}\\n0.91↓9.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 9.0\\\\%}\\n68.0↓5.6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 5.6\\\\%}\\n4165↓5.6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 5.6\\\\%}\\n0.33↓67.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 67.0\\\\%}\\n73.8−0.0%\\\\scriptstyle-{\\\\scriptstyle 0.0\\\\%}\\n10327↓18.5%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 18.5\\\\%}\\n0.64↓36.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 36.0\\\\%}\\nAutoThink\\n92.8↑0.9%\\\\scriptstyle<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:37:10 [engine.py:317] Added request chatcmpl-3fce46ccc54244b5a2e540ed3927fa54.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from data/output/arxiv_org_10_0_0.txt...vLLM STDOUT: INFO:     127.0.0.1:59610 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 10 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/arxiv_org_10_0_0_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/arxiv_org_10_0_0_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from data/output/arxiv_org_10_0_0.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/arxiv_org_10_0_0_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:47320 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:47332 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:37:19 [logger.py:43] Received request chatcmpl-dd2f97800b2f4dc3a3d11de99cc3fe04: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nscriptstyle\\\\uparrow{\\\\scriptstyle 4.6\\\\%}\\n18784↓21.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 21.0\\\\%}\\n0.88↓12.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 12.0\\\\%}\\n57.5↑2.3%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 2.3\\\\%}\\n15672↓19.5%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 19.5\\\\%}\\n0.80↓20.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 20.0\\\\%}\\n82.3↓0.7%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 0.7\\\\%}\\n1050↓60.6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 60.6\\\\%}\\n0.18↓82.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 82.0\\\\%}\\nHiPO\\n87.5↑8.3%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 8.3\\\\%}\\n15107↓29.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 29.0\\\\%}\\n0.98↓1.7%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 1.7\\\\%}\\n82.5↑15.1%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 15.1\\\\%}\\n17655↓25.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 25.8\\\\%}\\n0.95↓5.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 5.0\\\\%}\\n63.0↑12.2%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 12.2\\\\%}\\n13558↓30.4%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 30.4\\\\%}\\n0.82↓18.5%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 18.5\\\\%}\\n90.2↑8.8%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 8.8\\\\%}\\n776↓70.9%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 70.9\\\\%}\\n0.12↓88.4%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 88.4\\\\%}\\nMATH-500\\nGPQA-Diamond\\nMBPP\\nAverage\\nMethod\\nAcc↑\\\\uparrow\\nLength↓\\\\downarrow\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}↓\\\\downarrow\\nAcc↑\\\\uparrow\\nLength↓\\\\downarrow\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}↓\\\\downarrow\\nAcc↑\\\\uparrow\\nLength↓\\\\downarrow\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}↓\\\\downarrow\\nAcc↑\\\\uparrow\\nLength↓\\\\downarrow\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}↓\\\\downarrow\\nCold-Start (on)\\n92.0\\n6237\\n1.00\\n61.1\\n10832\\n1.00\\n72.0\\n4411\\n1.00\\n73.8\\n12667\\n1.00\\n+ GRPO\\n93.2↑0.0%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 0.0\\\\%}\\n6256↑1.3%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 1.3\\\\%}\\n1.00−0.0%\\\\scriptstyle{\\\\scriptstyle-0.0\\\\%}\\n57.6↓5.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 5.8\\\\%}\\n10633↓1.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 1.8\\\\%}\\n1.00−0.0%\\\\scriptstyle{\\\\scriptstyle-0.0\\\\%}\\n71.8↓0.3%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 0.3\\\\%}\\n5103↑15.7%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 15.7\\\\%}\\n1.00−0.0%\\\\scriptstyle-{\\\\scriptstyle 0.0\\\\%}\\n76.3↑3.3%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 3.3\\\\%}\\n12628↓0.3%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 0.3\\\\%}\\n1.00−0.0%\\\\script<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:37:19 [engine.py:317] Added request chatcmpl-dd2f97800b2f4dc3a3d11de99cc3fe04.\n",
            "\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from data/output/arxiv_org_9_0.txt...vLLM STDOUT: INFO:     127.0.0.1:47338 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:37:22 [logger.py:43] Received request chatcmpl-f77f29aad8704b63bf13a9a14f55e5a4: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nscriptstyle\\\\uparrow{\\\\scriptstyle 4.6\\\\%}\\n18784↓21.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 21.0\\\\%}\\n0.88↓12.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 12.0\\\\%}\\n57.5↑2.3%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 2.3\\\\%}\\n15672↓19.5%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 19.5\\\\%}\\n0.80↓20.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 20.0\\\\%}\\n82.3↓0.7%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 0.7\\\\%}\\n1050↓60.6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 60.6\\\\%}\\n0.18↓82.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 82.0\\\\%}\\nHiPO\\n87.5↑8.3%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 8.3\\\\%}\\n15107↓29.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 29.0\\\\%}\\n0.98↓1.7%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 1.7\\\\%}\\n82.5↑15.1%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 15.1\\\\%}\\n17655↓25.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 25.8\\\\%}\\n0.95↓5.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 5.0\\\\%}\\n63.0↑12.2%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 12.2\\\\%}\\n13558↓30.4%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 30.4\\\\%}\\n0.82↓18.5%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 18.5\\\\%}\\n90.2↑8.8%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 8.8\\\\%}\\n776↓70.9%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 70.9\\\\%}\\n0.12↓88.4%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 88.4\\\\%}\\nMATH-500\\nGPQA-Diamond\\nMBPP\\nAverage\\nMethod\\nAcc↑\\\\uparrow\\nLength↓\\\\downarrow\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}↓\\\\downarrow\\nAcc↑\\\\uparrow\\nLength↓\\\\downarrow\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}↓\\\\downarrow\\nAcc↑\\\\uparrow\\nLength↓\\\\downarrow\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}↓\\\\downarrow\\nAcc↑\\\\uparrow\\nLength↓\\\\downarrow\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}↓\\\\downarrow\\nCold-Start (on)\\n92.0\\n6237\\n1.00\\n61.1\\n10832\\n1.00\\n72.0\\n4411\\n1.00\\n73.8\\n12667\\n1.00\\n+ GRPO\\n93.2↑0.0%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 0.0\\\\%}\\n6256↑1.3%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 1.3\\\\%}\\n1.00−0.0%\\\\scriptstyle{\\\\scriptstyle-0.0\\\\%}\\n57.6↓5.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 5.8\\\\%}\\n10633↓1.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 1.8\\\\%}\\n1.00−0.0%\\\\scriptstyle{\\\\scriptstyle-0.0\\\\%}\\n71.8↓0.3%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 0.3\\\\%}\\n5103↑15.7%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 15.7\\\\%}\\n1.00−0.0%\\\\scriptstyle-{\\\\scriptstyle 0.0\\\\%}\\n76.3↑3.3%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 3.3\\\\%}\\n12628↓0.3%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 0.3\\\\%}\\n1.00−0.0%\\\\script<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:37:22 [engine.py:317] Added request chatcmpl-f77f29aad8704b63bf13a9a14f55e5a4.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from data/output/arxiv_org_9_0.txt...vLLM STDOUT: INFO:     127.0.0.1:47344 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 12 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/arxiv_org_9_0_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/arxiv_org_9_0_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from data/output/arxiv_org_9_0.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/arxiv_org_9_0_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:54128 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:54138 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:37:31 [logger.py:43] Received request chatcmpl-e7d8a78eefde4478ad9d1fe29b6069fe: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n=\\\\frac{\\\\pi_{\\\\theta}\\\\!\\\\left(y_{i,t}\\\\,\\\\middle|\\\\,h_{i,t}\\\\right)}{\\\\pi_{\\\\theta_{\\\\text{old}}}\\\\!\\\\left(y_{i,t}\\\\,\\\\middle|\\\\,h_{i,t}\\\\right)},\\nwhere yi,ty_{i,t} is the tt-th generated token in oio_{i} and hi,th_{i,t} is its conditioning context.\\nThe policy πθ\\\\pi_{\\\\theta} is optimized by maximizing the following token-level objective:\\n𝒥\\u200b(θ)\\\\displaystyle\\\\mathcal{J}(\\\\theta)\\n=𝔼[q∼P(Q),{oi}i=1G∼πθold(⋅|q)]\\\\displaystyle=\\\\mathbb{E}\\\\Big[q\\\\sim P(Q),\\\\{o_{i}\\\\}_{i=1}^{G}\\\\sim\\\\pi_{\\\\theta_{\\\\text{old}}}(\\\\cdot\\\\,|\\\\,q)\\\\Big]\\n(5)\\n⋅1G∑i=1G1|𝒯i|∑t∈𝒯i(min(ρi,tAi,t,clip(ρi,t,\\u20091−ϵ,\\u20091+ϵ)Ai,t)\\\\displaystyle\\\\quad\\\\cdot\\\\frac{1}{G}\\\\sum_{i=1}^{G}\\\\frac{1}{\\\\lvert\\\\mathcal{T}_{i}\\\\rvert}\\\\sum_{t\\\\in\\\\mathcal{T}_{i}}\\\\Big(\\\\min\\\\big(\\\\rho_{i,t}\\\\,A_{i,t},\\\\;\\\\text{clip}(\\\\rho_{i,t},1-\\\\epsilon,1+\\\\epsilon)\\\\,A_{i,t}\\\\big)\\n−β𝔻K\\u200bL(πθ(⋅|hi,t)∥πref(⋅|hi,t))).\\\\displaystyle\\\\qquad-\\\\beta\\\\,\\\\mathbb{D}_{KL}\\\\!\\\\big(\\\\pi_{\\\\theta}(\\\\cdot\\\\,|\\\\,h_{i,t})\\\\,\\\\big\\\\|\\\\,\\\\pi_{\\\\text{ref}}(\\\\cdot\\\\,|\\\\,h_{i,t})\\\\big)\\\\Big).\\nHere Ai,tA_{i,t} is the token-level advantage defined in Eq. (4) via segment-wise assignment, and 𝔻K\\u200bL\\\\mathbb{D}_{KL} is the token-level KL between the current policy and the reference policy at context hi,th_{i,t}.\\n3.3 Training Paradigm\\nOur HiPO framework adopts a two-stage training paradigm, consisting of a cold-start stage and a RL stage.\\nIn the code-start stage, the model is initialized with high-quality, hybrid training data that contains both Think-on and Think-off responses.\\nThis stage enables the model to acquire fundamental reasoning and answering capabilities, while establishing an initial balance between analytical reasoning and concise responses.\\nIn the RL stage, the model is further optimized using our hybrid reward system, which integrates mode-specific accuracy and global average performance.\\nTogether, these two stages ensure that HiPO achieves both strong factual accuracy and robust reasoning ability across diverse domains.\\n4 Experiments\\n4.1 Experimental setup\\nImplementation details.\\nSince the Qwen3 model can freely switch between inference modes, we chose it for our experiment. However, when the training data is insufficient, training the Qwen3 model can easily lead to a decline in performance on the test set (details can be found in the appendix A.2).\\nTo address this, we conducted Cold-Start tuning to stabilize its performance with relatively large datasets.\\nFor the Cold-Start stage, we use the “AM-Thinking-v1-Distilled”, “AceReason-Math”, “AM-Thinking”, “II-Thought-RL(math)” dataset for training. The parameters are set as: maximum learning rate is 8e-5, minimum learning rate is 8e-6\\nand batch size is 512. For the RL stage, we use the “II-Thought-RL(code)”, “Skywork-OR1-RL-Data” dataset for training. The parameters are set as: batch size = 16, maximum response length = 32k, NN = 16, ω\\\\omega = 0.01, and γ=0.3\\\\gamma=0.3.\\nBaselines.\\nTo demonstrate the effect of mitigating overthinking, we<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:37:31 [engine.py:317] Added request chatcmpl-e7d8a78eefde4478ad9d1fe29b6069fe.\n",
            "\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from data/output/arxiv_org_6_0_0.txt...vLLM STDOUT: INFO:     127.0.0.1:54142 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:37:33 [logger.py:43] Received request chatcmpl-f69b59ea7ba34ae2b10bdf8c6df36f72: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\n=\\\\frac{\\\\pi_{\\\\theta}\\\\!\\\\left(y_{i,t}\\\\,\\\\middle|\\\\,h_{i,t}\\\\right)}{\\\\pi_{\\\\theta_{\\\\text{old}}}\\\\!\\\\left(y_{i,t}\\\\,\\\\middle|\\\\,h_{i,t}\\\\right)},\\nwhere yi,ty_{i,t} is the tt-th generated token in oio_{i} and hi,th_{i,t} is its conditioning context.\\nThe policy πθ\\\\pi_{\\\\theta} is optimized by maximizing the following token-level objective:\\n𝒥\\u200b(θ)\\\\displaystyle\\\\mathcal{J}(\\\\theta)\\n=𝔼[q∼P(Q),{oi}i=1G∼πθold(⋅|q)]\\\\displaystyle=\\\\mathbb{E}\\\\Big[q\\\\sim P(Q),\\\\{o_{i}\\\\}_{i=1}^{G}\\\\sim\\\\pi_{\\\\theta_{\\\\text{old}}}(\\\\cdot\\\\,|\\\\,q)\\\\Big]\\n(5)\\n⋅1G∑i=1G1|𝒯i|∑t∈𝒯i(min(ρi,tAi,t,clip(ρi,t,\\u20091−ϵ,\\u20091+ϵ)Ai,t)\\\\displaystyle\\\\quad\\\\cdot\\\\frac{1}{G}\\\\sum_{i=1}^{G}\\\\frac{1}{\\\\lvert\\\\mathcal{T}_{i}\\\\rvert}\\\\sum_{t\\\\in\\\\mathcal{T}_{i}}\\\\Big(\\\\min\\\\big(\\\\rho_{i,t}\\\\,A_{i,t},\\\\;\\\\text{clip}(\\\\rho_{i,t},1-\\\\epsilon,1+\\\\epsilon)\\\\,A_{i,t}\\\\big)\\n−β𝔻K\\u200bL(πθ(⋅|hi,t)∥πref(⋅|hi,t))).\\\\displaystyle\\\\qquad-\\\\beta\\\\,\\\\mathbb{D}_{KL}\\\\!\\\\big(\\\\pi_{\\\\theta}(\\\\cdot\\\\,|\\\\,h_{i,t})\\\\,\\\\big\\\\|\\\\,\\\\pi_{\\\\text{ref}}(\\\\cdot\\\\,|\\\\,h_{i,t})\\\\big)\\\\Big).\\nHere Ai,tA_{i,t} is the token-level advantage defined in Eq. (4) via segment-wise assignment, and 𝔻K\\u200bL\\\\mathbb{D}_{KL} is the token-level KL between the current policy and the reference policy at context hi,th_{i,t}.\\n3.3 Training Paradigm\\nOur HiPO framework adopts a two-stage training paradigm, consisting of a cold-start stage and a RL stage.\\nIn the code-start stage, the model is initialized with high-quality, hybrid training data that contains both Think-on and Think-off responses.\\nThis stage enables the model to acquire fundamental reasoning and answering capabilities, while establishing an initial balance between analytical reasoning and concise responses.\\nIn the RL stage, the model is further optimized using our hybrid reward system, which integrates mode-specific accuracy and global average performance.\\nTogether, these two stages ensure that HiPO achieves both strong factual accuracy and robust reasoning ability across diverse domains.\\n4 Experiments\\n4.1 Experimental setup\\nImplementation details.\\nSince the Qwen3 model can freely switch between inference modes, we chose it for our experiment. However, when the training data is insufficient, training the Qwen3 model can easily lead to a decline in performance on the test set (details can be found in the appendix A.2).\\nTo address this, we conducted Cold-Start tuning to stabilize its performance with relatively large datasets.\\nFor the Cold-Start stage, we use the “AM-Thinking-v1-Distilled”, “AceReason-Math”, “AM-Thinking”, “II-Thought-RL(math)” dataset for training. The parameters are set as: maximum learning rate is 8e-5, minimum learning rate is 8e-6\\nand batch size is 512. For the RL stage, we use the “II-Thought-RL(code)”, “Skywork-OR1-RL-Data” dataset for training. The parameters are set as: batch size = 16, maximum response length = 32k, NN = 16, ω\\\\omega = 0.01, and γ=0.3\\\\gamma=0.3.\\nBaselines.\\nTo demonstrate the effect of mitigating overthinking, we<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:37:33 [engine.py:317] Added request chatcmpl-f69b59ea7ba34ae2b10bdf8c6df36f72.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from data/output/arxiv_org_6_0_0.txt...vLLM STDOUT: INFO:     127.0.0.1:54144 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 10 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/arxiv_org_6_0_0_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/arxiv_org_6_0_0_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from data/output/arxiv_org_6_0_0.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/arxiv_org_6_0_0_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:49614 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:49624 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:37:42 [logger.py:43] Received request chatcmpl-bddac090803f453f84a26198227da221: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n=}\\\\frac{1}{N_{\\\\text{on}}}\\\\sum_{i:M_{i}=\\\\text{on}}r_{i} denote the average reward of responses generated under the Think-on mode, and let mean\\u200b(𝐫off)\\\\text{mean}(\\\\mathbf{r}_{\\\\text{off}}) denote the corresponding average reward for the Think-off mode.\\nBased on this, we define a bias term for the Think-off mode as a fraction of the Think-on average reward:\\nbiasoff\\u200b=\\u200bω⋅mean\\u200b(𝐫on)\\\\mathrm{bias}_{\\\\text{off}}\\\\text{=}\\\\omega\\\\cdot\\\\text{mean}(\\\\mathbf{r}_{\\\\text{on}}), where ω\\\\omega controls the ratio.\\nThe adjustment is applied only when the performance of the Think-off mode does not exceed that of the Think-on mode, but the difference between the two remains within the bias threshold. Formally, the adjustment mechanism is as follows:\\nmean\\u200b(𝐫off)={mean\\u200b(𝐫off)+biasoff,0≤mean\\u200b(𝐫on)−mean\\u200b(𝐫off)≤biasoff,mean\\u200b(𝐫off),otherwise.\\\\text{mean}(\\\\mathbf{r}_{\\\\text{off}})=\\\\begin{cases}\\\\text{mean}(\\\\mathbf{r}_{\\\\text{off}})+\\\\mathrm{bias}_{\\\\text{off}},&0\\\\leq\\\\text{mean}(\\\\mathbf{r}_{\\\\text{on}})-\\\\text{mean}(\\\\mathbf{r}_{\\\\text{off}})\\\\leq\\\\mathrm{bias}_{\\\\text{off}},\\\\\\\\[6.0pt]\\n\\\\text{mean}(\\\\mathbf{r}_{\\\\text{off}}),&\\\\text{otherwise}.\\\\end{cases}\\n(1)\\nThis mechanism prevents the model from gaining an unfair advantage by overfitting to the more verbose but more accurate Think-on mode. Moreover, it ensures that the adjusted accuracies remain faithful to the true relative performance between reasoning modes, thereby improving training stability and preserving the intended balance between depth and efficiency.\\n3.2.3 Supervision RL with HiPO\\nThe final advantage function is formulated as a hybrid signal that integrates both judge analysis and model response. Each response ii receives two distinct scalar advantage, including judge advantage based on the quality of the mode justification, and answer advantage based on correctness and format.\\nThe judge advantage Aijudge\\\\mathrm{A}^{\\\\text{judge}}_{i} captures the broader decision-level utility of selecting a particular mode. The first term, mean\\u200b(𝐫Mi)−mean\\u200b(𝐫)\\\\text{mean}(\\\\mathbf{r}_{M_{i}})-\\\\text{mean}(\\\\mathbf{r}), quantifies the global advantage of the chosen mode over the full group average, guiding the model toward choosing modes that lead to higher expected rewards. The second term, γ⋅(ri−mean\\u200b(𝐫))\\\\gamma\\\\cdot(r_{i}-\\\\text{mean}(\\\\mathbf{r})), ensures that the justification content is also responsible for the quality of the response under that mode, thereby aligning the explanation with actual performance. The use of a global normalization factor std\\u200b(𝐫)\\\\text{std}(\\\\mathbf{r}) stabilizes the reward signal across groups.\\nThe judge advantage function for response ii is then given by:\\nAijudge={(mean\\u200b(𝐫on)−mean\\u200b(𝐫))+γ⋅(ri−mean\\u200b(𝐫))std\\u200b(𝐫),if\\xa0\\u200bMi=on,(mean\\u200b(𝐫off)−mean\\u200b(𝐫))+γ⋅(ri−mean\\u200b(𝐫))std\\u200b(𝐫),if\\xa0\\u200bMi=off.\\\\mathrm{A}^{\\\\text{judge}}_{i}=\\\\begin{cases}\\\\frac{(\\\\text{mean}(\\\\mathbf{r}_{\\\\text{on}})-\\\\text{mean}(\\\\mathbf{r}))+\\\\gamma\\\\cdot(r_{i}-\\\\text{mean}(\\\\mathbf{r}))}{\\\\text{std}(\\\\mathbf{r})},&\\\\text{if }<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:37:42 [engine.py:317] Added request chatcmpl-bddac090803f453f84a26198227da221.\n",
            "\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from data/output/arxiv_org_4_0_0.txt...vLLM STDOUT: INFO:     127.0.0.1:49632 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:37:44 [logger.py:43] Received request chatcmpl-d0b25053f60f470da6b60ce00f0f46c7: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\n=}\\\\frac{1}{N_{\\\\text{on}}}\\\\sum_{i:M_{i}=\\\\text{on}}r_{i} denote the average reward of responses generated under the Think-on mode, and let mean\\u200b(𝐫off)\\\\text{mean}(\\\\mathbf{r}_{\\\\text{off}}) denote the corresponding average reward for the Think-off mode.\\nBased on this, we define a bias term for the Think-off mode as a fraction of the Think-on average reward:\\nbiasoff\\u200b=\\u200bω⋅mean\\u200b(𝐫on)\\\\mathrm{bias}_{\\\\text{off}}\\\\text{=}\\\\omega\\\\cdot\\\\text{mean}(\\\\mathbf{r}_{\\\\text{on}}), where ω\\\\omega controls the ratio.\\nThe adjustment is applied only when the performance of the Think-off mode does not exceed that of the Think-on mode, but the difference between the two remains within the bias threshold. Formally, the adjustment mechanism is as follows:\\nmean\\u200b(𝐫off)={mean\\u200b(𝐫off)+biasoff,0≤mean\\u200b(𝐫on)−mean\\u200b(𝐫off)≤biasoff,mean\\u200b(𝐫off),otherwise.\\\\text{mean}(\\\\mathbf{r}_{\\\\text{off}})=\\\\begin{cases}\\\\text{mean}(\\\\mathbf{r}_{\\\\text{off}})+\\\\mathrm{bias}_{\\\\text{off}},&0\\\\leq\\\\text{mean}(\\\\mathbf{r}_{\\\\text{on}})-\\\\text{mean}(\\\\mathbf{r}_{\\\\text{off}})\\\\leq\\\\mathrm{bias}_{\\\\text{off}},\\\\\\\\[6.0pt]\\n\\\\text{mean}(\\\\mathbf{r}_{\\\\text{off}}),&\\\\text{otherwise}.\\\\end{cases}\\n(1)\\nThis mechanism prevents the model from gaining an unfair advantage by overfitting to the more verbose but more accurate Think-on mode. Moreover, it ensures that the adjusted accuracies remain faithful to the true relative performance between reasoning modes, thereby improving training stability and preserving the intended balance between depth and efficiency.\\n3.2.3 Supervision RL with HiPO\\nThe final advantage function is formulated as a hybrid signal that integrates both judge analysis and model response. Each response ii receives two distinct scalar advantage, including judge advantage based on the quality of the mode justification, and answer advantage based on correctness and format.\\nThe judge advantage Aijudge\\\\mathrm{A}^{\\\\text{judge}}_{i} captures the broader decision-level utility of selecting a particular mode. The first term, mean\\u200b(𝐫Mi)−mean\\u200b(𝐫)\\\\text{mean}(\\\\mathbf{r}_{M_{i}})-\\\\text{mean}(\\\\mathbf{r}), quantifies the global advantage of the chosen mode over the full group average, guiding the model toward choosing modes that lead to higher expected rewards. The second term, γ⋅(ri−mean\\u200b(𝐫))\\\\gamma\\\\cdot(r_{i}-\\\\text{mean}(\\\\mathbf{r})), ensures that the justification content is also responsible for the quality of the response under that mode, thereby aligning the explanation with actual performance. The use of a global normalization factor std\\u200b(𝐫)\\\\text{std}(\\\\mathbf{r}) stabilizes the reward signal across groups.\\nThe judge advantage function for response ii is then given by:\\nAijudge={(mean\\u200b(𝐫on)−mean\\u200b(𝐫))+γ⋅(ri−mean\\u200b(𝐫))std\\u200b(𝐫),if\\xa0\\u200bMi=on,(mean\\u200b(𝐫off)−mean\\u200b(𝐫))+γ⋅(ri−mean\\u200b(𝐫))std\\u200b(𝐫),if\\xa0\\u200bMi=off.\\\\mathrm{A}^{\\\\text{judge}}_{i}=\\\\begin{cases}\\\\frac{(\\\\text{mean}(\\\\mathbf{r}_{\\\\text{on}})-\\\\text{mean}(\\\\mathbf{r}))+\\\\gamma\\\\cdot(r_{i}-\\\\text{mean}(\\\\mathbf{r}))}{\\\\text{std}(\\\\mathbf{r})},&\\\\text{if }<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:37:44 [engine.py:317] Added request chatcmpl-d0b25053f60f470da6b60ce00f0f46c7.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from data/output/arxiv_org_4_0_0.txt...vLLM STDOUT: INFO:     127.0.0.1:55428 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 6 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/arxiv_org_4_0_0_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/arxiv_org_4_0_0_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from data/output/arxiv_org_4_0_0.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/arxiv_org_4_0_0_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:45356 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:45362 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:37:53 [logger.py:43] Received request chatcmpl-c253a01abac14272b934b7fcd8a2663b: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nin performance. However, when applying our method HiPO to train the Cold-Start model, the accuracy improves by 6.2%, while the token length and thinking rate decrease dramatically by 30% and 39%, respectively. Moreover, experimental results show that our HiPO outperforms existing methods on both efficiency and accuracy.\\n4.3 Ablation Study\\nFigure 3: Performance of different response selection strategies.\\nEffect of selecting the shortest response.\\nIn the data construction pipeline, we select the shortest response (Cold-Start (Shortest)) as the final sample.\\nTo analyze the effect of this strategy,\\nwe additionally propose two variants called (Cold-Start (Longest) and Cold-Start (Random)) by selecting the longest responses and randomly selecting the responses,\\nrespectively.\\nIn Figure\\xa03,\\nCold-Start (Shortest) shows an improvement in accuracy compared to both Cold-Start (Longest) and Cold-Start (Random), with a decrease in both the Thinking ratio and Token length.\\nTherefore,\\nwe adopt this Cold-Start (Shortest) strategy for the Cold-Start stage.\\nEffect of design strategies for Aijudge\\\\mathrm{A}^{\\\\text{judge}}_{i} and Aianswer\\\\mathrm{A}^{\\\\text{answer}}_{i}.\\nIn the reinforcement learning stage, first, we utilize the term mean\\u200b(𝐫Mi)−mean\\u200b(𝐫)\\\\text{mean}(\\\\mathbf{r}_{M_{i}})-\\\\text{mean}(\\\\mathbf{r}) to quantify the global advantage of the chosen mode over the full group average. Second, the local normalization based on the mode-specific mean and standard deviation is used for Aianswer\\\\mathrm{A}^{\\\\text{answer}}_{i}.\\nTo demonstrate the effect of these strategies, as shown in Table\\xa03, we design two variants (i.e., HiPO (w/o global adv) and HiPO (w/o local norm)). For HiPO (w/o global adv), we directly remove the global advantage for Aijudge\\\\mathrm{A}^{\\\\text{judge}}_{i}. For HiPO (w/o local norm), we just use the global normalization across the responses in a group.\\nIn Table\\xa03, we observe that HiPO achieves significant improvements in performance and efficiency when compared to these two variants.\\nAIME2024\\nLiveCodeBench\\nHumanEval\\nMethod\\nAcc\\nLength\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}\\nAcc\\nLength\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}\\nAcc\\nLength\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}\\nHiPO\\n87.50\\n15107\\n0.98\\n63.00\\n13558\\n0.82\\n90.2\\n776\\n0.12\\nHiPO (w/o global adv)\\n85.83↓1.9%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 1.9\\\\%}\\n18064↑19.6%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 19.6\\\\%}\\n1.00↑2.0%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 2.0\\\\%}\\n56.83↓9.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 9.8\\\\%}\\n14561↑7.4%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 7.4\\\\%}\\n0.86↑4.9%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 4.9\\\\%}\\n89.63↓4.9%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 4.9\\\\%}\\n1660↑114.9%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 114.9\\\\%}\\n0.27↑125.0%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 125.0\\\\%}\\nHiPO (w/o local norm)\\n85.00↓2.9%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 2.9\\\\%}\\n18268↑20.9%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 20.9\\\\%}\\n1.00↑2.0%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 2.0\\\\%}\\n58.37↓7.3%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 7.3\\\\%}\\n16029↑18.2%\\\\scriptstyle\\\\up<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:37:53 [engine.py:317] Added request chatcmpl-c253a01abac14272b934b7fcd8a2663b.\n",
            "\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from data/output/arxiv_org_12_0.txt...vLLM STDOUT: INFO:     127.0.0.1:45366 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:37:56 [logger.py:43] Received request chatcmpl-16cd2306e872461bb200008e05d472a6: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\n in performance. However, when applying our method HiPO to train the Cold-Start model, the accuracy improves by 6.2%, while the token length and thinking rate decrease dramatically by 30% and 39%, respectively. Moreover, experimental results show that our HiPO outperforms existing methods on both efficiency and accuracy.\\n4.3 Ablation Study\\nFigure 3: Performance of different response selection strategies.\\nEffect of selecting the shortest response.\\nIn the data construction pipeline, we select the shortest response (Cold-Start (Shortest)) as the final sample.\\nTo analyze the effect of this strategy,\\nwe additionally propose two variants called (Cold-Start (Longest) and Cold-Start (Random)) by selecting the longest responses and randomly selecting the responses,\\nrespectively.\\nIn Figure\\xa03,\\nCold-Start (Shortest) shows an improvement in accuracy compared to both Cold-Start (Longest) and Cold-Start (Random), with a decrease in both the Thinking ratio and Token length.\\nTherefore,\\nwe adopt this Cold-Start (Shortest) strategy for the Cold-Start stage.\\nEffect of design strategies for Aijudge\\\\mathrm{A}^{\\\\text{judge}}_{i} and Aianswer\\\\mathrm{A}^{\\\\text{answer}}_{i}.\\nIn the reinforcement learning stage, first, we utilize the term mean\\u200b(𝐫Mi)−mean\\u200b(𝐫)\\\\text{mean}(\\\\mathbf{r}_{M_{i}})-\\\\text{mean}(\\\\mathbf{r}) to quantify the global advantage of the chosen mode over the full group average. Second, the local normalization based on the mode-specific mean and standard deviation is used for Aianswer\\\\mathrm{A}^{\\\\text{answer}}_{i}.\\nTo demonstrate the effect of these strategies, as shown in Table\\xa03, we design two variants (i.e., HiPO (w/o global adv) and HiPO (w/o local norm)). For HiPO (w/o global adv), we directly remove the global advantage for Aijudge\\\\mathrm{A}^{\\\\text{judge}}_{i}. For HiPO (w/o local norm), we just use the global normalization across the responses in a group.\\nIn Table\\xa03, we observe that HiPO achieves significant improvements in performance and efficiency when compared to these two variants.\\nAIME2024\\nLiveCodeBench\\nHumanEval\\nMethod\\nAcc\\nLength\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}\\nAcc\\nLength\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}\\nAcc\\nLength\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}\\nHiPO\\n87.50\\n15107\\n0.98\\n63.00\\n13558\\n0.82\\n90.2\\n776\\n0.12\\nHiPO (w/o global adv)\\n85.83↓1.9%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 1.9\\\\%}\\n18064↑19.6%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 19.6\\\\%}\\n1.00↑2.0%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 2.0\\\\%}\\n56.83↓9.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 9.8\\\\%}\\n14561↑7.4%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 7.4\\\\%}\\n0.86↑4.9%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 4.9\\\\%}\\n89.63↓4.9%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 4.9\\\\%}\\n1660↑114.9%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 114.9\\\\%}\\n0.27↑125.0%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 125.0\\\\%}\\nHiPO (w/o local norm)\\n85.00↓2.9%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 2.9\\\\%}\\n18268↑20.9%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 20.9\\\\%}\\n1.00↑2.0%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 2.0\\\\%}\\n58.37↓7.3%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 7.3\\\\%}\\n16029↑18.2%\\\\scriptstyle\\\\up<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:37:56 [engine.py:317] Added request chatcmpl-16cd2306e872461bb200008e05d472a6.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from data/output/arxiv_org_12_0.txt...vLLM STDOUT: INFO:     127.0.0.1:45374 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 14 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/arxiv_org_12_0_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/arxiv_org_12_0_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from data/output/arxiv_org_12_0.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/arxiv_org_12_0_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:46424 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:46430 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:38:05 [logger.py:43] Received request chatcmpl-6e79a837076a4c9ca2818f3749915cfb: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nmean}(\\\\mathbf{r}_{\\\\text{on}})-\\\\text{mean}(\\\\mathbf{r}))+\\\\gamma\\\\cdot(r_{i}-\\\\text{mean}(\\\\mathbf{r}))}{\\\\text{std}(\\\\mathbf{r})},&\\\\text{if }M_{i}=\\\\text{on},\\\\\\\\[6.0pt]\\n\\\\frac{(\\\\text{mean}(\\\\mathbf{r}_{\\\\text{off}})-\\\\text{mean}(\\\\mathbf{r}))+\\\\gamma\\\\cdot(r_{i}-\\\\text{mean}(\\\\mathbf{r}))}{\\\\text{std}(\\\\mathbf{r})},&\\\\text{if }M_{i}=\\\\text{off}.\\\\end{cases}\\n(2)\\nIn contrast to the judge advantage function, the advantage AianswerA^{\\\\text{answer}}_{i} is computed within the context of the selected reasoning mode. Since the mode MiM_{i} has already been determined prior to response generation, it is natural to assess the response quality relative to other responses within the same mode. This local normalization using mode-specific mean and standard deviation focuses the learning signal on intra-mode variance, encouraging the model to improve response quality without conflating mode preference. For response ii, the answer advantage is defined as:\\nAianswer={ri−mean\\u200b(𝐫on)std\\u200b(𝐫on),if\\xa0\\u200bMi=on,ri−mean\\u200b(𝐫off)std\\u200b(𝐫off),if\\xa0\\u200bMi=off.A^{\\\\text{answer}}_{i}=\\\\begin{cases}\\\\frac{r_{i}-\\\\text{mean}(\\\\mathbf{r}_{\\\\text{on}})}{\\\\text{std}(\\\\mathbf{r}_{\\\\text{on}})},&\\\\text{if }M_{i}=\\\\text{on},\\\\\\\\[6.0pt]\\n\\\\frac{r_{i}-\\\\text{mean}(\\\\mathbf{r}_{\\\\text{off}})}{\\\\text{std}(\\\\mathbf{r}_{\\\\text{off}})},&\\\\text{if }M_{i}=\\\\text{off}.\\\\end{cases}\\n(3)\\nTo assign token-level reward for training with reinforcement learning, we define the final reward for each token tt in sample ii as follows:\\nAi,t={Aianswer,if token\\xa0\\u200bt∈𝒯answer,Aijudge,if token\\xa0\\u200bt∈𝒯judge.\\\\mathrm{A}_{i,t}=\\\\begin{cases}\\\\mathrm{A}^{\\\\text{answer}}_{i},&\\\\text{if token }t\\\\in\\\\mathcal{T}^{\\\\text{answer}},\\\\\\\\\\n\\\\mathrm{A}^{\\\\text{judge}}_{i},&\\\\text{if token }t\\\\in\\\\mathcal{T}^{\\\\text{judge}}.\\\\end{cases}\\n(4)\\nwhere 𝒯judge\\\\mathcal{T}^{\\\\text{judge}} and 𝒯answer\\\\mathcal{T}^{\\\\text{answer}} denote the token index sets corresponding to the judge segment and the answer segment, respectively, within each response.\\nGiven a query qq, HiPO generates a collection of candidate outputs {oi}i=1G\\\\{o_{i}\\\\}_{i=1}^{G} from the old policy πθold\\\\pi_{\\\\theta_{\\\\text{old}}}. For each output oio_{i}, let 𝒯i\\\\mathcal{T}_{i} denote the set of token positions in response ii, i.e., 𝒯i=𝒯judge∪𝒯answer\\\\mathcal{T}_{i}=\\\\mathcal{T}^{\\\\text{judge}}\\\\cup\\\\mathcal{T}^{\\\\text{answer}}. We define the per-token probability ratio as\\nρi,t=πθ(yi,t|hi,t)πθold(yi,t|hi,t)\\\\rho_{i,t}=\\\\frac{\\\\pi_{\\\\theta}\\\\!\\\\left(y_{i,t}\\\\,\\\\middle|\\\\,h_{i,t}\\\\right)}{\\\\pi_{\\\\theta_{\\\\text{old}}}\\\\!\\\\left(y_{i,t}\\\\,\\\\middle|\\\\,h_{i,t}\\\\right)},\\nwhere yi,ty_{i,t}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:38:05 [engine.py:317] Added request chatcmpl-6e79a837076a4c9ca2818f3749915cfb.\n",
            "\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from data/output/arxiv_org_5_0.txt...vLLM STDOUT: INFO:     127.0.0.1:46446 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:38:07 [logger.py:43] Received request chatcmpl-441e87261fc74f8fac309483c4b8a56d: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nmean}(\\\\mathbf{r}_{\\\\text{on}})-\\\\text{mean}(\\\\mathbf{r}))+\\\\gamma\\\\cdot(r_{i}-\\\\text{mean}(\\\\mathbf{r}))}{\\\\text{std}(\\\\mathbf{r})},&\\\\text{if }M_{i}=\\\\text{on},\\\\\\\\[6.0pt]\\n\\\\frac{(\\\\text{mean}(\\\\mathbf{r}_{\\\\text{off}})-\\\\text{mean}(\\\\mathbf{r}))+\\\\gamma\\\\cdot(r_{i}-\\\\text{mean}(\\\\mathbf{r}))}{\\\\text{std}(\\\\mathbf{r})},&\\\\text{if }M_{i}=\\\\text{off}.\\\\end{cases}\\n(2)\\nIn contrast to the judge advantage function, the advantage AianswerA^{\\\\text{answer}}_{i} is computed within the context of the selected reasoning mode. Since the mode MiM_{i} has already been determined prior to response generation, it is natural to assess the response quality relative to other responses within the same mode. This local normalization using mode-specific mean and standard deviation focuses the learning signal on intra-mode variance, encouraging the model to improve response quality without conflating mode preference. For response ii, the answer advantage is defined as:\\nAianswer={ri−mean\\u200b(𝐫on)std\\u200b(𝐫on),if\\xa0\\u200bMi=on,ri−mean\\u200b(𝐫off)std\\u200b(𝐫off),if\\xa0\\u200bMi=off.A^{\\\\text{answer}}_{i}=\\\\begin{cases}\\\\frac{r_{i}-\\\\text{mean}(\\\\mathbf{r}_{\\\\text{on}})}{\\\\text{std}(\\\\mathbf{r}_{\\\\text{on}})},&\\\\text{if }M_{i}=\\\\text{on},\\\\\\\\[6.0pt]\\n\\\\frac{r_{i}-\\\\text{mean}(\\\\mathbf{r}_{\\\\text{off}})}{\\\\text{std}(\\\\mathbf{r}_{\\\\text{off}})},&\\\\text{if }M_{i}=\\\\text{off}.\\\\end{cases}\\n(3)\\nTo assign token-level reward for training with reinforcement learning, we define the final reward for each token tt in sample ii as follows:\\nAi,t={Aianswer,if token\\xa0\\u200bt∈𝒯answer,Aijudge,if token\\xa0\\u200bt∈𝒯judge.\\\\mathrm{A}_{i,t}=\\\\begin{cases}\\\\mathrm{A}^{\\\\text{answer}}_{i},&\\\\text{if token }t\\\\in\\\\mathcal{T}^{\\\\text{answer}},\\\\\\\\\\n\\\\mathrm{A}^{\\\\text{judge}}_{i},&\\\\text{if token }t\\\\in\\\\mathcal{T}^{\\\\text{judge}}.\\\\end{cases}\\n(4)\\nwhere 𝒯judge\\\\mathcal{T}^{\\\\text{judge}} and 𝒯answer\\\\mathcal{T}^{\\\\text{answer}} denote the token index sets corresponding to the judge segment and the answer segment, respectively, within each response.\\nGiven a query qq, HiPO generates a collection of candidate outputs {oi}i=1G\\\\{o_{i}\\\\}_{i=1}^{G} from the old policy πθold\\\\pi_{\\\\theta_{\\\\text{old}}}. For each output oio_{i}, let 𝒯i\\\\mathcal{T}_{i} denote the set of token positions in response ii, i.e., 𝒯i=𝒯judge∪𝒯answer\\\\mathcal{T}_{i}=\\\\mathcal{T}^{\\\\text{judge}}\\\\cup\\\\mathcal{T}^{\\\\text{answer}}. We define the per-token probability ratio as\\nρi,t=πθ(yi,t|hi,t)πθold(yi,t|hi,t)\\\\rho_{i,t}=\\\\frac{\\\\pi_{\\\\theta}\\\\!\\\\left(y_{i,t}\\\\,\\\\middle|\\\\,h_{i,t}\\\\right)}{\\\\pi_{\\\\theta_{\\\\text{old}}}\\\\!\\\\left(y_{i,t}\\\\,\\\\middle|\\\\,h_{i,t}\\\\right)},\\nwhere yi,ty_{i,t}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:38:07 [engine.py:317] Added request chatcmpl-441e87261fc74f8fac309483c4b8a56d.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from data/output/arxiv_org_5_0.txt...vLLM STDOUT: INFO:     127.0.0.1:46448 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 0 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/arxiv_org_5_0_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/arxiv_org_5_0_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from data/output/arxiv_org_5_0.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/arxiv_org_5_0_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:51648 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:51654 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:38:13 [logger.py:43] Received request chatcmpl-0cae534b128f4ec8a6ab8e6adf69a183: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n20.9\\\\%}\\n1.00↑2.0%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 2.0\\\\%}\\n58.37↓7.3%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 7.3\\\\%}\\n16029↑18.2%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 18.2\\\\%}\\n0.88↑7.3%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 7.3\\\\%}\\n89.63↓0.6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 0.6\\\\%}\\n2052↑164.4%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 164.4\\\\%}\\n0.32↑166.7%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 166.7\\\\%}\\nTable 3: Performance of different design strategies on advantage functions.\\nFigure 4: Performance of different γ\\\\gamma values.\\nFigure 5: Performance of different rollout numbers and ω\\\\omega values.\\nEffect of different γ\\\\gamma values. Figure 4 shows that, when the value of γ\\\\gamma is set to 0.00, the reward for the judge token lacks information about the current response, resulting in lower model accuracy and higher token length. On the other hand, when γ\\\\gamma is set too high, the scales of the two terms (mean(𝐫off\\\\mathbf{r}_{\\\\text{off}}) - mean(𝐫\\\\mathbf{r})) and (ri−mean\\u200b(𝐫))(r_{i}-\\\\textit{mean}(\\\\mathbf{r})) become imbalanced, which leads to a decrease in model accuracy and an increase in token length.\\nEffect of different rollout numbers. Table 5 shows that, when the rollout number NN is set to 16, the model achieves better average performance, shorter token length, and lower think rate. We attribute this to the fact that this configuration provides sufficient data to explore diverse possibilities while avoiding excessive samples with redundant reasoning that dilute the training signal. As a result, the model focuses more on learning from higher-quality samples, leading to a more concise strategy with improved accuracy, reduced token length, and lower think rate.\\nEffect of different ω\\\\omega values. Table 5 shows that, setting ω\\\\omega to 0.01 provides a balanced trade-off between performance and efficiency. This configuration mitigates the overly conservative behavior seen at 0.0 while avoiding the overly aggressive behavior at higher settings, ultimately achieving the largest efficiency gains with minimal performance loss.\\n4.4 Further Analysis\\n(a) (a) Think-on and Think-off ratio in training. (b) Think-on ratio of different datasets.\\n(b) (a) Average token usage in RL training. (b) Token usage of different datasets.\\nAIME24\\nLiveCodeBench\\nHumanEval\\nMBPP\\nMethod\\nAcc\\nLength\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}\\nAcc\\nLength\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}\\nAcc\\nLength\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}\\nAcc\\nLength\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}\\nQwen3-1.7B\\nCold-Start (On)\\n63.3\\n24214\\n1.00\\n33.7\\n25616\\n1.00\\n77.4\\n4172\\n1.00\\n54.6\\n8587\\n1.00\\nCold-Start\\n65.0↑2.7%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 2.7\\\\%}\\n21039↓13.1%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 13.1\\\\%}\\n1.00−0.0%\\\\scriptstyle-{\\\\scriptstyle 0.0\\\\%}\\n37.4↑11.0%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 11.0\\\\%}\\n21364↓16.6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 16.6\\\\%}\\n0.98↓2.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 2.0\\\\%}\\n81.7↑5.2%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 5.2\\\\%}\\n3084↓26.1%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 26.1\\\\%}\\n0.39↓61.0<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:38:13 [engine.py:317] Added request chatcmpl-0cae534b128f4ec8a6ab8e6adf69a183.\n",
            "\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from data/output/arxiv_org_13_0.txt...vLLM STDOUT: INFO:     127.0.0.1:51670 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[32m⠧\u001b[0m Generating qa content from data/output/arxiv_org_13_0.txt...vLLM STDOUT: INFO 10-03 08:38:17 [logger.py:43] Received request chatcmpl-5fbe7d127cfd462ea496b10d3b0af19b: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\n 20.9\\\\%}\\n1.00↑2.0%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 2.0\\\\%}\\n58.37↓7.3%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 7.3\\\\%}\\n16029↑18.2%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 18.2\\\\%}\\n0.88↑7.3%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 7.3\\\\%}\\n89.63↓0.6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 0.6\\\\%}\\n2052↑164.4%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 164.4\\\\%}\\n0.32↑166.7%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 166.7\\\\%}\\nTable 3: Performance of different design strategies on advantage functions.\\nFigure 4: Performance of different γ\\\\gamma values.\\nFigure 5: Performance of different rollout numbers and ω\\\\omega values.\\nEffect of different γ\\\\gamma values. Figure 4 shows that, when the value of γ\\\\gamma is set to 0.00, the reward for the judge token lacks information about the current response, resulting in lower model accuracy and higher token length. On the other hand, when γ\\\\gamma is set too high, the scales of the two terms (mean(𝐫off\\\\mathbf{r}_{\\\\text{off}}) - mean(𝐫\\\\mathbf{r})) and (ri−mean\\u200b(𝐫))(r_{i}-\\\\textit{mean}(\\\\mathbf{r})) become imbalanced, which leads to a decrease in model accuracy and an increase in token length.\\nEffect of different rollout numbers. Table 5 shows that, when the rollout number NN is set to 16, the model achieves better average performance, shorter token length, and lower think rate. We attribute this to the fact that this configuration provides sufficient data to explore diverse possibilities while avoiding excessive samples with redundant reasoning that dilute the training signal. As a result, the model focuses more on learning from higher-quality samples, leading to a more concise strategy with improved accuracy, reduced token length, and lower think rate.\\nEffect of different ω\\\\omega values. Table 5 shows that, setting ω\\\\omega to 0.01 provides a balanced trade-off between performance and efficiency. This configuration mitigates the overly conservative behavior seen at 0.0 while avoiding the overly aggressive behavior at higher settings, ultimately achieving the largest efficiency gains with minimal performance loss.\\n4.4 Further Analysis\\n(a) (a) Think-on and Think-off ratio in training. (b) Think-on ratio of different datasets.\\n(b) (a) Average token usage in RL training. (b) Token usage of different datasets.\\nAIME24\\nLiveCodeBench\\nHumanEval\\nMBPP\\nMethod\\nAcc\\nLength\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}\\nAcc\\nLength\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}\\nAcc\\nLength\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}\\nAcc\\nLength\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}\\nQwen3-1.7B\\nCold-Start (On)\\n63.3\\n24214\\n1.00\\n33.7\\n25616\\n1.00\\n77.4\\n4172\\n1.00\\n54.6\\n8587\\n1.00\\nCold-Start\\n65.0↑2.7%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 2.7\\\\%}\\n21039↓13.1%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 13.1\\\\%}\\n1.00−0.0%\\\\scriptstyle-{\\\\scriptstyle 0.0\\\\%}\\n37.4↑11.0%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 11.0\\\\%}\\n21364↓16.6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 16.6\\\\%}\\n0.98↓2.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 2.0\\\\%}\\n81.7↑5.2%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 5.2\\\\%}\\n3084↓26.1%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 26.1\\\\%}\\n0.39↓61.0<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:38:17 [engine.py:317] Added request chatcmpl-5fbe7d127cfd462ea496b10d3b0af19b.\n",
            "\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from data/output/arxiv_org_13_0.txt...vLLM STDOUT: INFO:     127.0.0.1:51678 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 12 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/arxiv_org_13_0_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/arxiv_org_13_0_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from data/output/arxiv_org_13_0.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/arxiv_org_13_0_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:59682 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:59688 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:38:26 [logger.py:43] Received request chatcmpl-f83692319d4242a5935d47d0de6af1b3: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nuanzhe Pang, Julien Dirani, Julian Michael, and Samuel\\xa0R. Bowman.\\nGpqa: A graduate-level google-proof q&a benchmark.\\nArXiv, abs/2311.12022, 2023.\\nURL https://api.semanticscholar.org/CorpusID:265295009.\\nAppendix A Appendix\\nA.1 Use of LLMs\\nLLMs were used solely to assist in editing, formatting, and improving the clarity of the manuscript.\\nAll ideas, experiments, and analyses were conceived and executed by the authors.\\nNo LLM outputs were used as experimental data or results in this work.\\nA.2 The decline in Qwen3’s performance on the test set.\\nThis section demonstrates the decline in Qwen3’s performance on AIME2024, AIME2025, HumanEval, and LiverCodeBench.\\nWe trained Qwen3 using AM-DeepSeek-R1-0528-Distilled, AM-Thinking-v1-Distilled, and OpenThoughts3-1.2M.\\nThe Figure 7, when the number of training steps reaches 150, Qwen3’s accuracy on all benchmarks declines.\\nNote that the batch size is set as 512 and other parameters are same as the implementation details in the main paper.\\nFigure 7: The decline in Qwen3’s performance on the AIME2024, AIME2025, HumanEval, LiveCodeBench.\\nA.3 Data Source\\nOur dataset is derived from several open-source reasoning corpora covering both code and mathematics.\\nAs shown in Table\\xa05, queries come from AM-Thinking-v1-Distilled\\xa0222https://huggingface.co/datasets/a-m-team/AM-Thinking-v1-Distilled, II-Thought-RL\\xa0333https://huggingface.co/datasets/Intelligent-Internet/II-Thought-RL-v0, AceReason-Math\\xa0444https://huggingface.co/datasets/nvidia/AceReason-Math, and Skywork-OR1-RL-Data\\xa0555https://huggingface.co/datasets/Skywork/Skywork-OR1-RL-Data.\\nThis composition ensures diversity across domains and provides a reliable basis for model training and evaluation.\\nCategory\\nData Source\\n# Query\\nCode\\nAM-Thinking-v1-Distilled\\xa0[Tian et\\xa0al., 2025]\\n85k\\nII-Thought-RL\\xa0[Internet, 2025]\\n20k\\nMath\\nAceReason-Math\\xa0[Chen et\\xa0al., 2025b]\\n49k\\nAM-Thinking-v1-Distilled\\xa0[Tian et\\xa0al., 2025]\\n32k\\nII-Thought-RL\\xa0[Internet, 2025]\\n30k\\nSkywork-OR1-RL-Data\\xa0[He et\\xa0al., 2025]\\n24k\\nTable 5: Description of data sources.\\nA.4 Prompt Templates\\nIn this section, we provide the prompt templates for the response generation and judge analysis generation.\\nResponse Generation\\nPlease read the following question carefully and provide a clear answer.\\n—\\nQuery\\n—\\nJudge Analysis Generation\\nYou are tasked with analyzing the characteristics of a question to determine why it **requires** complex reasoning.\\nYour should **not** attempting to answer or infer its solution.\\nYou should analyse user’s question to determine the **core task intention**—that is, what the user wants the model to do. (e.g., write and validate code based on a problem description, etc.).\\nThen briefly outline the basic approach to accomplishing this task (e.g., write SQL code to retrieve imformation, etc.).\\nBased on the required approach, assess the **reasoning complexity**, and indicate whether it involves multiple steps or deep analysis. Do not solve the question or provide an answer. Focus solely on interpreting the task type, approach, and cognitive demand.\\nBe concise: your analysis must be no more than two lines and under 500 characters. Use clear, natural, and varied language.\\nEnd your explanation with a statement indicating that complex reasoning is required (Think-on), but express this conclusion with a natural and diverse phrase, not repeating any single pattern. The meaning must be clear, but the expression can vary.\\nPlease analyze the following question as required above:\\n—\\nModel Response\\n—\\nGenerated\\non Sun Sep 28 16:41:38 2025 by LaTeXML<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:38:26 [engine.py:317] Added request chatcmpl-f83692319d4242a5935d47d0de6af1b3.\n",
            "\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from data/output/arxiv_org_22_0.txt...vLLM STDOUT: INFO:     127.0.0.1:59702 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:38:28 [logger.py:43] Received request chatcmpl-4742c78836d54afea19ec3f02cbf283f: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nuanzhe Pang, Julien Dirani, Julian Michael, and Samuel\\xa0R. Bowman.\\nGpqa: A graduate-level google-proof q&a benchmark.\\nArXiv, abs/2311.12022, 2023.\\nURL https://api.semanticscholar.org/CorpusID:265295009.\\nAppendix A Appendix\\nA.1 Use of LLMs\\nLLMs were used solely to assist in editing, formatting, and improving the clarity of the manuscript.\\nAll ideas, experiments, and analyses were conceived and executed by the authors.\\nNo LLM outputs were used as experimental data or results in this work.\\nA.2 The decline in Qwen3’s performance on the test set.\\nThis section demonstrates the decline in Qwen3’s performance on AIME2024, AIME2025, HumanEval, and LiverCodeBench.\\nWe trained Qwen3 using AM-DeepSeek-R1-0528-Distilled, AM-Thinking-v1-Distilled, and OpenThoughts3-1.2M.\\nThe Figure 7, when the number of training steps reaches 150, Qwen3’s accuracy on all benchmarks declines.\\nNote that the batch size is set as 512 and other parameters are same as the implementation details in the main paper.\\nFigure 7: The decline in Qwen3’s performance on the AIME2024, AIME2025, HumanEval, LiveCodeBench.\\nA.3 Data Source\\nOur dataset is derived from several open-source reasoning corpora covering both code and mathematics.\\nAs shown in Table\\xa05, queries come from AM-Thinking-v1-Distilled\\xa0222https://huggingface.co/datasets/a-m-team/AM-Thinking-v1-Distilled, II-Thought-RL\\xa0333https://huggingface.co/datasets/Intelligent-Internet/II-Thought-RL-v0, AceReason-Math\\xa0444https://huggingface.co/datasets/nvidia/AceReason-Math, and Skywork-OR1-RL-Data\\xa0555https://huggingface.co/datasets/Skywork/Skywork-OR1-RL-Data.\\nThis composition ensures diversity across domains and provides a reliable basis for model training and evaluation.\\nCategory\\nData Source\\n# Query\\nCode\\nAM-Thinking-v1-Distilled\\xa0[Tian et\\xa0al., 2025]\\n85k\\nII-Thought-RL\\xa0[Internet, 2025]\\n20k\\nMath\\nAceReason-Math\\xa0[Chen et\\xa0al., 2025b]\\n49k\\nAM-Thinking-v1-Distilled\\xa0[Tian et\\xa0al., 2025]\\n32k\\nII-Thought-RL\\xa0[Internet, 2025]\\n30k\\nSkywork-OR1-RL-Data\\xa0[He et\\xa0al., 2025]\\n24k\\nTable 5: Description of data sources.\\nA.4 Prompt Templates\\nIn this section, we provide the prompt templates for the response generation and judge analysis generation.\\nResponse Generation\\nPlease read the following question carefully and provide a clear answer.\\n—\\nQuery\\n—\\nJudge Analysis Generation\\nYou are tasked with analyzing the characteristics of a question to determine why it **requires** complex reasoning.\\nYour should **not** attempting to answer or infer its solution.\\nYou should analyse user’s question to determine the **core task intention**—that is, what the user wants the model to do. (e.g., write and validate code based on a problem description, etc.).\\nThen briefly outline the basic approach to accomplishing this task (e.g., write SQL code to retrieve imformation, etc.).\\nBased on the required approach, assess the **reasoning complexity**, and indicate whether it involves multiple steps or deep analysis. Do not solve the question or provide an answer. Focus solely on interpreting the task type, approach, and cognitive demand.\\nBe concise: your analysis must be no more than two lines and under 500 characters. Use clear, natural, and varied language.\\nEnd your explanation with a statement indicating that complex reasoning is required (Think-on), but express this conclusion with a natural and diverse phrase, not repeating any single pattern. The meaning must be clear, but the expression can vary.\\nPlease analyze the following question as required above:\\n—\\nModel Response\\n—\\nGenerated\\non Sun Sep 28 16:41:38 2025 by LaTeXML<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:38:28 [engine.py:317] Added request chatcmpl-4742c78836d54afea19ec3f02cbf283f.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from data/output/arxiv_org_22_0.txt...vLLM STDOUT: INFO:     127.0.0.1:59710 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 18 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/arxiv_org_22_0_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/arxiv_org_22_0_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from data/output/arxiv_org_22_0.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/arxiv_org_22_0_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:38952 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:38960 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:38:37 [logger.py:43] Received request chatcmpl-dffd5974b4be4f2ca671cf83f34f38b1: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nin performance. However, when applying our method HiPO to train the Cold-Start model, the accuracy improves by 6.2%, while the token length and thinking rate decrease dramatically by 30% and 39%, respectively. Moreover, experimental results show that our HiPO outperforms existing methods on both efficiency and accuracy.\\n4.3 Ablation Study\\nFigure 3: Performance of different response selection strategies.\\nEffect of selecting the shortest response.\\nIn the data construction pipeline, we select the shortest response (Cold-Start (Shortest)) as the final sample.\\nTo analyze the effect of this strategy,\\nwe additionally propose two variants called (Cold-Start (Longest) and Cold-Start (Random)) by selecting the longest responses and randomly selecting the responses,\\nrespectively.\\nIn Figure\\xa03,\\nCold-Start (Shortest) shows an improvement in accuracy compared to both Cold-Start (Longest) and Cold-Start (Random), with a decrease in both the Thinking ratio and Token length.\\nTherefore,\\nwe adopt this Cold-Start (Shortest) strategy for the Cold-Start stage.\\nEffect of design strategies for Aijudge\\\\mathrm{A}^{\\\\text{judge}}_{i} and Aianswer\\\\mathrm{A}^{\\\\text{answer}}_{i}.\\nIn the reinforcement learning stage, first, we utilize the term mean\\u200b(𝐫Mi)−mean\\u200b(𝐫)\\\\text{mean}(\\\\mathbf{r}_{M_{i}})-\\\\text{mean}(\\\\mathbf{r}) to quantify the global advantage of the chosen mode over the full group average. Second, the local normalization based on the mode-specific mean and standard deviation is used for Aianswer\\\\mathrm{A}^{\\\\text{answer}}_{i}.\\nTo demonstrate the effect of these strategies, as shown in Table\\xa03, we design two variants (i.e., HiPO (w/o global adv) and HiPO (w/o local norm)). For HiPO (w/o global adv), we directly remove the global advantage for Aijudge\\\\mathrm{A}^{\\\\text{judge}}_{i}. For HiPO (w/o local norm), we just use the global normalization across the responses in a group.\\nIn Table\\xa03, we observe that HiPO achieves significant improvements in performance and efficiency when compared to these two variants.\\nAIME2024\\nLiveCodeBench\\nHumanEval\\nMethod\\nAcc\\nLength\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}\\nAcc\\nLength\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}\\nAcc\\nLength\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}\\nHiPO\\n87.50\\n15107\\n0.98\\n63.00\\n13558\\n0.82\\n90.2\\n776\\n0.12\\nHiPO (w/o global adv)\\n85.83↓1.9%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 1.9\\\\%}\\n18064↑19.6%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 19.6\\\\%}\\n1.00↑2.0%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 2.0\\\\%}\\n56.83↓9.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 9.8\\\\%}\\n14561↑7.4%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 7.4\\\\%}\\n0.86↑4.9%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 4.9\\\\%}\\n89.63↓4.9%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 4.9\\\\%}\\n1660↑114.9%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 114.9\\\\%}\\n0.27↑125.0%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 125.0\\\\%}\\nHiPO (w/o local norm)\\n85.00↓2.9%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 2.9\\\\%}\\n18268↑20.9%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 20.9\\\\%}\\n1.00↑2.0%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 2.0\\\\%}\\n58.37↓7.3%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 7.3\\\\%}\\n16029↑18.2%\\\\scriptstyle\\\\up<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:38:37 [engine.py:317] Added request chatcmpl-dffd5974b4be4f2ca671cf83f34f38b1.\n",
            "\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from data/output/arxiv_org_12_0_0.txt...vLLM STDOUT: INFO:     127.0.0.1:38966 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:38:39 [logger.py:43] Received request chatcmpl-b069b65299904c9eab7742e7c59b5cad: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\n in performance. However, when applying our method HiPO to train the Cold-Start model, the accuracy improves by 6.2%, while the token length and thinking rate decrease dramatically by 30% and 39%, respectively. Moreover, experimental results show that our HiPO outperforms existing methods on both efficiency and accuracy.\\n4.3 Ablation Study\\nFigure 3: Performance of different response selection strategies.\\nEffect of selecting the shortest response.\\nIn the data construction pipeline, we select the shortest response (Cold-Start (Shortest)) as the final sample.\\nTo analyze the effect of this strategy,\\nwe additionally propose two variants called (Cold-Start (Longest) and Cold-Start (Random)) by selecting the longest responses and randomly selecting the responses,\\nrespectively.\\nIn Figure\\xa03,\\nCold-Start (Shortest) shows an improvement in accuracy compared to both Cold-Start (Longest) and Cold-Start (Random), with a decrease in both the Thinking ratio and Token length.\\nTherefore,\\nwe adopt this Cold-Start (Shortest) strategy for the Cold-Start stage.\\nEffect of design strategies for Aijudge\\\\mathrm{A}^{\\\\text{judge}}_{i} and Aianswer\\\\mathrm{A}^{\\\\text{answer}}_{i}.\\nIn the reinforcement learning stage, first, we utilize the term mean\\u200b(𝐫Mi)−mean\\u200b(𝐫)\\\\text{mean}(\\\\mathbf{r}_{M_{i}})-\\\\text{mean}(\\\\mathbf{r}) to quantify the global advantage of the chosen mode over the full group average. Second, the local normalization based on the mode-specific mean and standard deviation is used for Aianswer\\\\mathrm{A}^{\\\\text{answer}}_{i}.\\nTo demonstrate the effect of these strategies, as shown in Table\\xa03, we design two variants (i.e., HiPO (w/o global adv) and HiPO (w/o local norm)). For HiPO (w/o global adv), we directly remove the global advantage for Aijudge\\\\mathrm{A}^{\\\\text{judge}}_{i}. For HiPO (w/o local norm), we just use the global normalization across the responses in a group.\\nIn Table\\xa03, we observe that HiPO achieves significant improvements in performance and efficiency when compared to these two variants.\\nAIME2024\\nLiveCodeBench\\nHumanEval\\nMethod\\nAcc\\nLength\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}\\nAcc\\nLength\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}\\nAcc\\nLength\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}\\nHiPO\\n87.50\\n15107\\n0.98\\n63.00\\n13558\\n0.82\\n90.2\\n776\\n0.12\\nHiPO (w/o global adv)\\n85.83↓1.9%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 1.9\\\\%}\\n18064↑19.6%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 19.6\\\\%}\\n1.00↑2.0%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 2.0\\\\%}\\n56.83↓9.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 9.8\\\\%}\\n14561↑7.4%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 7.4\\\\%}\\n0.86↑4.9%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 4.9\\\\%}\\n89.63↓4.9%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 4.9\\\\%}\\n1660↑114.9%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 114.9\\\\%}\\n0.27↑125.0%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 125.0\\\\%}\\nHiPO (w/o local norm)\\n85.00↓2.9%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 2.9\\\\%}\\n18268↑20.9%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 20.9\\\\%}\\n1.00↑2.0%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 2.0\\\\%}\\n58.37↓7.3%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 7.3\\\\%}\\n16029↑18.2%\\\\scriptstyle\\\\up<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:38:39 [engine.py:317] Added request chatcmpl-b069b65299904c9eab7742e7c59b5cad.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from data/output/arxiv_org_12_0_0.txt...vLLM STDOUT: INFO:     127.0.0.1:38972 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 13 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/arxiv_org_12_0_0_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/arxiv_org_12_0_0_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from data/output/arxiv_org_12_0_0.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/arxiv_org_12_0_0_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:46016 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:46030 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:38:49 [logger.py:43] Received request chatcmpl-b11fb14f63e1488f855ab6c103eb82a1: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nHiPO: Hybrid Policy Optimization for Dynamic Reasoning in LLMs\\n1 Introduction\\n2 Related Works\\n3 Method\\n3.1 Hybrid Data Construction Pipeline\\n3.1.1 Data Source\\n3.1.2 Data Collection\\n3.1.3 Data format\\n3.2 Hybrid RL Reward System\\n3.2.1 Basic Reward Formulation\\n3.2.2 Bias Adjustment Mechanism\\n3.2.3 Supervision RL with HiPO\\n3.3 Training Paradigm\\n4 Experiments\\n4.1 Experimental setup\\n4.2 Main Results\\n4.3 Ablation Study\\n4.4 Further Analysis\\nThink-on vs. Think-off Dynamics During Training and Inference\\nToken Count Dynamics During Training and Inference\\nGeneralization on More Models\\n5 Conclusion\\nA Appendix\\nA.1 Use of LLMs\\nA.2 The decline in Qwen3’s performance on the test set.\\nA.3 Data Source\\nA.4 Prompt Templates\\nHiPO: Hybrid Policy Optimization for Dynamic Reasoning in LLMs\\nKen Deng∗,\\nZizheng Zhan∗,\\nWen Xiang∗,\\nWenqiang Zhu∗,\\nTianhao Peng,\\nXinping Lei,\\nWeihao Li,\\nJingxuan Xu,\\nKun Wu,\\nYifan Yao,\\nHaoyang Huang,\\nHuaixi Tang,\\nKepeng Lei,\\nZhiyi Lai,\\nSongwei Yu,\\nZongxian Feng,\\nZuchen Gao,\\nWeihao Xie,\\nChenchen Zhang,\\nYanan Wu,\\nYuanxing Zhang,\\nLecheng Huang,\\nYuqun Zhang,\\nJie Liu,\\nZhaoxiang Zhang,\\nHaotian Zhang,\\nBin Chen,\\nJiaheng Liu†\\nKuaishou Technology, Nanjing University\\ndengken@kuaishou.com, liujiaheng@nju.edu.cn\\n††footnotetext: *\\xa0Equal Contribution. \\xa0\\xa0†\\xa0Corresponding Author.\\nAbstract\\nLarge Language Models (LLMs) increasingly rely on chain-of-thought (CoT) reasoning to improve accuracy on complex tasks. However, always generating lengthy reasoning traces is inefficient, leading to excessive token usage and higher inference costs. This paper introduces the Hybrid Policy Optimization (i.e., HiPO), a framework for adaptive reasoning control that enables LLMs to selectively decide when to engage in detailed reasoning (Think-on) and when to respond directly (Think-off).\\nSpecifically, HiPO combines a hybrid data pipeline—providing paired Think-on and Think-off responses—with a hybrid reinforcement learning reward system that balances accuracy and efficiency while avoiding over-reliance on detailed reasoning.\\nExperiments across mathematics and coding benchmarks demonstrate that HiPO can substantially reduce token length while maintaining or improving accuracy.\\nFinally, we hope HiPO\\xa0111https://huggingface.co/Kwaipilot/HiPO-8B can be a principled approach for efficient adaptive reasoning, advancing the deployment of reasoning-oriented LLMs in real-world, resource-sensitive settings.\\n1 Introduction\\nLarge Language Models (LLMs) have achieved unprecedented success across diverse cognitive tasks, from code generation and mathematical reasoning to scientific problem-solving. A key driver of this progress is the integration of Chain-of-Thought (CoT)\\xa0(Yao et\\xa0al., 2023; Wei et\\xa0al., 2023) reasoning—a paradigm where models decompose complex queries into sequential, interpretable steps to derive accurate outputs.\\nThese approaches enhance accuracy on challenging problems but also introduce a persistent drawback: overthinking\\xa0(Kumar et\\xa0al., 2025; Sui et\\xa0al., 2025; Nayab et\\xa0al., 2025). Even for trivial queries, models often generate unnecessarily long reasoning chains, leading to inflated token usage, higher latency, and reduced efficiency in interactive applications. This inefficiency creates a fundamental tension between reasoning quality and computational cost, raising the need for mechanisms that can adaptively regulate reasoning depth.\\nRecently, recent work has explored adaptive reasoning control to mitigate overthinking,\\nand can be divided into two categories: (i) training-based adaptive reasoning, where reinforcement learning (RL)\\xa0(Aggarwal and Welleck, 2025; Arora and Zanette, 2025; Hou et\\xa0al., 2025; Luo et\\xa0al., 2025; Shen et\\xa0al., 2025;<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:38:49 [engine.py:317] Added request chatcmpl-b11fb14f63e1488f855ab6c103eb82a1.\n",
            "\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from data/output/arxiv_org_0_0.txt...vLLM STDOUT: INFO:     127.0.0.1:46040 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:38:50 [logger.py:43] Received request chatcmpl-b4583d796c9747cea1e4aee778a88c6b: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nHiPO: Hybrid Policy Optimization for Dynamic Reasoning in LLMs\\n1 Introduction\\n2 Related Works\\n3 Method\\n3.1 Hybrid Data Construction Pipeline\\n3.1.1 Data Source\\n3.1.2 Data Collection\\n3.1.3 Data format\\n3.2 Hybrid RL Reward System\\n3.2.1 Basic Reward Formulation\\n3.2.2 Bias Adjustment Mechanism\\n3.2.3 Supervision RL with HiPO\\n3.3 Training Paradigm\\n4 Experiments\\n4.1 Experimental setup\\n4.2 Main Results\\n4.3 Ablation Study\\n4.4 Further Analysis\\nThink-on vs. Think-off Dynamics During Training and Inference\\nToken Count Dynamics During Training and Inference\\nGeneralization on More Models\\n5 Conclusion\\nA Appendix\\nA.1 Use of LLMs\\nA.2 The decline in Qwen3’s performance on the test set.\\nA.3 Data Source\\nA.4 Prompt Templates\\nHiPO: Hybrid Policy Optimization for Dynamic Reasoning in LLMs\\nKen Deng∗,\\nZizheng Zhan∗,\\nWen Xiang∗,\\nWenqiang Zhu∗,\\nTianhao Peng,\\nXinping Lei,\\nWeihao Li,\\nJingxuan Xu,\\nKun Wu,\\nYifan Yao,\\nHaoyang Huang,\\nHuaixi Tang,\\nKepeng Lei,\\nZhiyi Lai,\\nSongwei Yu,\\nZongxian Feng,\\nZuchen Gao,\\nWeihao Xie,\\nChenchen Zhang,\\nYanan Wu,\\nYuanxing Zhang,\\nLecheng Huang,\\nYuqun Zhang,\\nJie Liu,\\nZhaoxiang Zhang,\\nHaotian Zhang,\\nBin Chen,\\nJiaheng Liu†\\nKuaishou Technology, Nanjing University\\ndengken@kuaishou.com, liujiaheng@nju.edu.cn\\n††footnotetext: *\\xa0Equal Contribution. \\xa0\\xa0†\\xa0Corresponding Author.\\nAbstract\\nLarge Language Models (LLMs) increasingly rely on chain-of-thought (CoT) reasoning to improve accuracy on complex tasks. However, always generating lengthy reasoning traces is inefficient, leading to excessive token usage and higher inference costs. This paper introduces the Hybrid Policy Optimization (i.e., HiPO), a framework for adaptive reasoning control that enables LLMs to selectively decide when to engage in detailed reasoning (Think-on) and when to respond directly (Think-off).\\nSpecifically, HiPO combines a hybrid data pipeline—providing paired Think-on and Think-off responses—with a hybrid reinforcement learning reward system that balances accuracy and efficiency while avoiding over-reliance on detailed reasoning.\\nExperiments across mathematics and coding benchmarks demonstrate that HiPO can substantially reduce token length while maintaining or improving accuracy.\\nFinally, we hope HiPO\\xa0111https://huggingface.co/Kwaipilot/HiPO-8B can be a principled approach for efficient adaptive reasoning, advancing the deployment of reasoning-oriented LLMs in real-world, resource-sensitive settings.\\n1 Introduction\\nLarge Language Models (LLMs) have achieved unprecedented success across diverse cognitive tasks, from code generation and mathematical reasoning to scientific problem-solving. A key driver of this progress is the integration of Chain-of-Thought (CoT)\\xa0(Yao et\\xa0al., 2023; Wei et\\xa0al., 2023) reasoning—a paradigm where models decompose complex queries into sequential, interpretable steps to derive accurate outputs.\\nThese approaches enhance accuracy on challenging problems but also introduce a persistent drawback: overthinking\\xa0(Kumar et\\xa0al., 2025; Sui et\\xa0al., 2025; Nayab et\\xa0al., 2025). Even for trivial queries, models often generate unnecessarily long reasoning chains, leading to inflated token usage, higher latency, and reduced efficiency in interactive applications. This inefficiency creates a fundamental tension between reasoning quality and computational cost, raising the need for mechanisms that can adaptively regulate reasoning depth.\\nRecently, recent work has explored adaptive reasoning control to mitigate overthinking,\\nand can be divided into two categories: (i) training-based adaptive reasoning, where reinforcement learning (RL)\\xa0(Aggarwal and Welleck, 2025; Arora and Zanette, 2025; Hou et\\xa0al., 2025; Luo et\\xa0al., 2025; Shen et\\xa0al., 2025;<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:38:50 [engine.py:317] Added request chatcmpl-b4583d796c9747cea1e4aee778a88c6b.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from data/output/arxiv_org_0_0.txt...vLLM STDOUT: INFO:     127.0.0.1:46056 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 9 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/arxiv_org_0_0_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/arxiv_org_0_0_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from data/output/arxiv_org_0_0.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/arxiv_org_0_0_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:55724 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:55734 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:39:00 [logger.py:43] Received request chatcmpl-4eea738d38134c668e1cb35507088145: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n=}\\\\frac{1}{N_{\\\\text{on}}}\\\\sum_{i:M_{i}=\\\\text{on}}r_{i} denote the average reward of responses generated under the Think-on mode, and let mean\\u200b(𝐫off)\\\\text{mean}(\\\\mathbf{r}_{\\\\text{off}}) denote the corresponding average reward for the Think-off mode.\\nBased on this, we define a bias term for the Think-off mode as a fraction of the Think-on average reward:\\nbiasoff\\u200b=\\u200bω⋅mean\\u200b(𝐫on)\\\\mathrm{bias}_{\\\\text{off}}\\\\text{=}\\\\omega\\\\cdot\\\\text{mean}(\\\\mathbf{r}_{\\\\text{on}}), where ω\\\\omega controls the ratio.\\nThe adjustment is applied only when the performance of the Think-off mode does not exceed that of the Think-on mode, but the difference between the two remains within the bias threshold. Formally, the adjustment mechanism is as follows:\\nmean\\u200b(𝐫off)={mean\\u200b(𝐫off)+biasoff,0≤mean\\u200b(𝐫on)−mean\\u200b(𝐫off)≤biasoff,mean\\u200b(𝐫off),otherwise.\\\\text{mean}(\\\\mathbf{r}_{\\\\text{off}})=\\\\begin{cases}\\\\text{mean}(\\\\mathbf{r}_{\\\\text{off}})+\\\\mathrm{bias}_{\\\\text{off}},&0\\\\leq\\\\text{mean}(\\\\mathbf{r}_{\\\\text{on}})-\\\\text{mean}(\\\\mathbf{r}_{\\\\text{off}})\\\\leq\\\\mathrm{bias}_{\\\\text{off}},\\\\\\\\[6.0pt]\\n\\\\text{mean}(\\\\mathbf{r}_{\\\\text{off}}),&\\\\text{otherwise}.\\\\end{cases}\\n(1)\\nThis mechanism prevents the model from gaining an unfair advantage by overfitting to the more verbose but more accurate Think-on mode. Moreover, it ensures that the adjusted accuracies remain faithful to the true relative performance between reasoning modes, thereby improving training stability and preserving the intended balance between depth and efficiency.\\n3.2.3 Supervision RL with HiPO\\nThe final advantage function is formulated as a hybrid signal that integrates both judge analysis and model response. Each response ii receives two distinct scalar advantage, including judge advantage based on the quality of the mode justification, and answer advantage based on correctness and format.\\nThe judge advantage Aijudge\\\\mathrm{A}^{\\\\text{judge}}_{i} captures the broader decision-level utility of selecting a particular mode. The first term, mean\\u200b(𝐫Mi)−mean\\u200b(𝐫)\\\\text{mean}(\\\\mathbf{r}_{M_{i}})-\\\\text{mean}(\\\\mathbf{r}), quantifies the global advantage of the chosen mode over the full group average, guiding the model toward choosing modes that lead to higher expected rewards. The second term, γ⋅(ri−mean\\u200b(𝐫))\\\\gamma\\\\cdot(r_{i}-\\\\text{mean}(\\\\mathbf{r})), ensures that the justification content is also responsible for the quality of the response under that mode, thereby aligning the explanation with actual performance. The use of a global normalization factor std\\u200b(𝐫)\\\\text{std}(\\\\mathbf{r}) stabilizes the reward signal across groups.\\nThe judge advantage function for response ii is then given by:\\nAijudge={(mean\\u200b(𝐫on)−mean\\u200b(𝐫))+γ⋅(ri−mean\\u200b(𝐫))std\\u200b(𝐫),if\\xa0\\u200bMi=on,(mean\\u200b(𝐫off)−mean\\u200b(𝐫))+γ⋅(ri−mean\\u200b(𝐫))std\\u200b(𝐫),if\\xa0\\u200bMi=off.\\\\mathrm{A}^{\\\\text{judge}}_{i}=\\\\begin{cases}\\\\frac{(\\\\text{mean}(\\\\mathbf{r}_{\\\\text{on}})-\\\\text{mean}(\\\\mathbf{r}))+\\\\gamma\\\\cdot(r_{i}-\\\\text{mean}(\\\\mathbf{r}))}{\\\\text{std}(\\\\mathbf{r})},&\\\\text{if }<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:39:00 [engine.py:317] Added request chatcmpl-4eea738d38134c668e1cb35507088145.\n",
            "\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from data/output/arxiv_org_4_0.txt...vLLM STDOUT: INFO:     127.0.0.1:55738 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:39:02 [logger.py:43] Received request chatcmpl-1c23090acc1f423ba6edd08a8f3823df: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\n=}\\\\frac{1}{N_{\\\\text{on}}}\\\\sum_{i:M_{i}=\\\\text{on}}r_{i} denote the average reward of responses generated under the Think-on mode, and let mean\\u200b(𝐫off)\\\\text{mean}(\\\\mathbf{r}_{\\\\text{off}}) denote the corresponding average reward for the Think-off mode.\\nBased on this, we define a bias term for the Think-off mode as a fraction of the Think-on average reward:\\nbiasoff\\u200b=\\u200bω⋅mean\\u200b(𝐫on)\\\\mathrm{bias}_{\\\\text{off}}\\\\text{=}\\\\omega\\\\cdot\\\\text{mean}(\\\\mathbf{r}_{\\\\text{on}}), where ω\\\\omega controls the ratio.\\nThe adjustment is applied only when the performance of the Think-off mode does not exceed that of the Think-on mode, but the difference between the two remains within the bias threshold. Formally, the adjustment mechanism is as follows:\\nmean\\u200b(𝐫off)={mean\\u200b(𝐫off)+biasoff,0≤mean\\u200b(𝐫on)−mean\\u200b(𝐫off)≤biasoff,mean\\u200b(𝐫off),otherwise.\\\\text{mean}(\\\\mathbf{r}_{\\\\text{off}})=\\\\begin{cases}\\\\text{mean}(\\\\mathbf{r}_{\\\\text{off}})+\\\\mathrm{bias}_{\\\\text{off}},&0\\\\leq\\\\text{mean}(\\\\mathbf{r}_{\\\\text{on}})-\\\\text{mean}(\\\\mathbf{r}_{\\\\text{off}})\\\\leq\\\\mathrm{bias}_{\\\\text{off}},\\\\\\\\[6.0pt]\\n\\\\text{mean}(\\\\mathbf{r}_{\\\\text{off}}),&\\\\text{otherwise}.\\\\end{cases}\\n(1)\\nThis mechanism prevents the model from gaining an unfair advantage by overfitting to the more verbose but more accurate Think-on mode. Moreover, it ensures that the adjusted accuracies remain faithful to the true relative performance between reasoning modes, thereby improving training stability and preserving the intended balance between depth and efficiency.\\n3.2.3 Supervision RL with HiPO\\nThe final advantage function is formulated as a hybrid signal that integrates both judge analysis and model response. Each response ii receives two distinct scalar advantage, including judge advantage based on the quality of the mode justification, and answer advantage based on correctness and format.\\nThe judge advantage Aijudge\\\\mathrm{A}^{\\\\text{judge}}_{i} captures the broader decision-level utility of selecting a particular mode. The first term, mean\\u200b(𝐫Mi)−mean\\u200b(𝐫)\\\\text{mean}(\\\\mathbf{r}_{M_{i}})-\\\\text{mean}(\\\\mathbf{r}), quantifies the global advantage of the chosen mode over the full group average, guiding the model toward choosing modes that lead to higher expected rewards. The second term, γ⋅(ri−mean\\u200b(𝐫))\\\\gamma\\\\cdot(r_{i}-\\\\text{mean}(\\\\mathbf{r})), ensures that the justification content is also responsible for the quality of the response under that mode, thereby aligning the explanation with actual performance. The use of a global normalization factor std\\u200b(𝐫)\\\\text{std}(\\\\mathbf{r}) stabilizes the reward signal across groups.\\nThe judge advantage function for response ii is then given by:\\nAijudge={(mean\\u200b(𝐫on)−mean\\u200b(𝐫))+γ⋅(ri−mean\\u200b(𝐫))std\\u200b(𝐫),if\\xa0\\u200bMi=on,(mean\\u200b(𝐫off)−mean\\u200b(𝐫))+γ⋅(ri−mean\\u200b(𝐫))std\\u200b(𝐫),if\\xa0\\u200bMi=off.\\\\mathrm{A}^{\\\\text{judge}}_{i}=\\\\begin{cases}\\\\frac{(\\\\text{mean}(\\\\mathbf{r}_{\\\\text{on}})-\\\\text{mean}(\\\\mathbf{r}))+\\\\gamma\\\\cdot(r_{i}-\\\\text{mean}(\\\\mathbf{r}))}{\\\\text{std}(\\\\mathbf{r})},&\\\\text{if }<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:39:02 [engine.py:317] Added request chatcmpl-1c23090acc1f423ba6edd08a8f3823df.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from data/output/arxiv_org_4_0.txt...vLLM STDOUT: INFO:     127.0.0.1:55742 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 6 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/arxiv_org_4_0_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/arxiv_org_4_0_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from data/output/arxiv_org_4_0.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/arxiv_org_4_0_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:43372 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:43374 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:39:11 [logger.py:43] Received request chatcmpl-08fefeffbdc44880a39201220ff868e5: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nuanzhe Pang, Julien Dirani, Julian Michael, and Samuel\\xa0R. Bowman.\\nGpqa: A graduate-level google-proof q&a benchmark.\\nArXiv, abs/2311.12022, 2023.\\nURL https://api.semanticscholar.org/CorpusID:265295009.\\nAppendix A Appendix\\nA.1 Use of LLMs\\nLLMs were used solely to assist in editing, formatting, and improving the clarity of the manuscript.\\nAll ideas, experiments, and analyses were conceived and executed by the authors.\\nNo LLM outputs were used as experimental data or results in this work.\\nA.2 The decline in Qwen3’s performance on the test set.\\nThis section demonstrates the decline in Qwen3’s performance on AIME2024, AIME2025, HumanEval, and LiverCodeBench.\\nWe trained Qwen3 using AM-DeepSeek-R1-0528-Distilled, AM-Thinking-v1-Distilled, and OpenThoughts3-1.2M.\\nThe Figure 7, when the number of training steps reaches 150, Qwen3’s accuracy on all benchmarks declines.\\nNote that the batch size is set as 512 and other parameters are same as the implementation details in the main paper.\\nFigure 7: The decline in Qwen3’s performance on the AIME2024, AIME2025, HumanEval, LiveCodeBench.\\nA.3 Data Source\\nOur dataset is derived from several open-source reasoning corpora covering both code and mathematics.\\nAs shown in Table\\xa05, queries come from AM-Thinking-v1-Distilled\\xa0222https://huggingface.co/datasets/a-m-team/AM-Thinking-v1-Distilled, II-Thought-RL\\xa0333https://huggingface.co/datasets/Intelligent-Internet/II-Thought-RL-v0, AceReason-Math\\xa0444https://huggingface.co/datasets/nvidia/AceReason-Math, and Skywork-OR1-RL-Data\\xa0555https://huggingface.co/datasets/Skywork/Skywork-OR1-RL-Data.\\nThis composition ensures diversity across domains and provides a reliable basis for model training and evaluation.\\nCategory\\nData Source\\n# Query\\nCode\\nAM-Thinking-v1-Distilled\\xa0[Tian et\\xa0al., 2025]\\n85k\\nII-Thought-RL\\xa0[Internet, 2025]\\n20k\\nMath\\nAceReason-Math\\xa0[Chen et\\xa0al., 2025b]\\n49k\\nAM-Thinking-v1-Distilled\\xa0[Tian et\\xa0al., 2025]\\n32k\\nII-Thought-RL\\xa0[Internet, 2025]\\n30k\\nSkywork-OR1-RL-Data\\xa0[He et\\xa0al., 2025]\\n24k\\nTable 5: Description of data sources.\\nA.4 Prompt Templates\\nIn this section, we provide the prompt templates for the response generation and judge analysis generation.\\nResponse Generation\\nPlease read the following question carefully and provide a clear answer.\\n—\\nQuery\\n—\\nJudge Analysis Generation\\nYou are tasked with analyzing the characteristics of a question to determine why it **requires** complex reasoning.\\nYour should **not** attempting to answer or infer its solution.\\nYou should analyse user’s question to determine the **core task intention**—that is, what the user wants the model to do. (e.g., write and validate code based on a problem description, etc.).\\nThen briefly outline the basic approach to accomplishing this task (e.g., write SQL code to retrieve imformation, etc.).\\nBased on the required approach, assess the **reasoning complexity**, and indicate whether it involves multiple steps or deep analysis. Do not solve the question or provide an answer. Focus solely on interpreting the task type, approach, and cognitive demand.\\nBe concise: your analysis must be no more than two lines and under 500 characters. Use clear, natural, and varied language.\\nEnd your explanation with a statement indicating that complex reasoning is required (Think-on), but express this conclusion with a natural and diverse phrase, not repeating any single pattern. The meaning must be clear, but the expression can vary.\\nPlease analyze the following question as required above:\\n—\\nModel Response\\n—\\nGenerated\\non Sun Sep 28 16:41:38 2025 by LaTeXML<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:39:11 [engine.py:317] Added request chatcmpl-08fefeffbdc44880a39201220ff868e5.\n",
            "\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from data/output/arxiv_org_22_0_0.txt...vLLM STDOUT: INFO:     127.0.0.1:43380 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:39:13 [logger.py:43] Received request chatcmpl-df77aa1c8f9f40d0b1254be04762196c: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nuanzhe Pang, Julien Dirani, Julian Michael, and Samuel\\xa0R. Bowman.\\nGpqa: A graduate-level google-proof q&a benchmark.\\nArXiv, abs/2311.12022, 2023.\\nURL https://api.semanticscholar.org/CorpusID:265295009.\\nAppendix A Appendix\\nA.1 Use of LLMs\\nLLMs were used solely to assist in editing, formatting, and improving the clarity of the manuscript.\\nAll ideas, experiments, and analyses were conceived and executed by the authors.\\nNo LLM outputs were used as experimental data or results in this work.\\nA.2 The decline in Qwen3’s performance on the test set.\\nThis section demonstrates the decline in Qwen3’s performance on AIME2024, AIME2025, HumanEval, and LiverCodeBench.\\nWe trained Qwen3 using AM-DeepSeek-R1-0528-Distilled, AM-Thinking-v1-Distilled, and OpenThoughts3-1.2M.\\nThe Figure 7, when the number of training steps reaches 150, Qwen3’s accuracy on all benchmarks declines.\\nNote that the batch size is set as 512 and other parameters are same as the implementation details in the main paper.\\nFigure 7: The decline in Qwen3’s performance on the AIME2024, AIME2025, HumanEval, LiveCodeBench.\\nA.3 Data Source\\nOur dataset is derived from several open-source reasoning corpora covering both code and mathematics.\\nAs shown in Table\\xa05, queries come from AM-Thinking-v1-Distilled\\xa0222https://huggingface.co/datasets/a-m-team/AM-Thinking-v1-Distilled, II-Thought-RL\\xa0333https://huggingface.co/datasets/Intelligent-Internet/II-Thought-RL-v0, AceReason-Math\\xa0444https://huggingface.co/datasets/nvidia/AceReason-Math, and Skywork-OR1-RL-Data\\xa0555https://huggingface.co/datasets/Skywork/Skywork-OR1-RL-Data.\\nThis composition ensures diversity across domains and provides a reliable basis for model training and evaluation.\\nCategory\\nData Source\\n# Query\\nCode\\nAM-Thinking-v1-Distilled\\xa0[Tian et\\xa0al., 2025]\\n85k\\nII-Thought-RL\\xa0[Internet, 2025]\\n20k\\nMath\\nAceReason-Math\\xa0[Chen et\\xa0al., 2025b]\\n49k\\nAM-Thinking-v1-Distilled\\xa0[Tian et\\xa0al., 2025]\\n32k\\nII-Thought-RL\\xa0[Internet, 2025]\\n30k\\nSkywork-OR1-RL-Data\\xa0[He et\\xa0al., 2025]\\n24k\\nTable 5: Description of data sources.\\nA.4 Prompt Templates\\nIn this section, we provide the prompt templates for the response generation and judge analysis generation.\\nResponse Generation\\nPlease read the following question carefully and provide a clear answer.\\n—\\nQuery\\n—\\nJudge Analysis Generation\\nYou are tasked with analyzing the characteristics of a question to determine why it **requires** complex reasoning.\\nYour should **not** attempting to answer or infer its solution.\\nYou should analyse user’s question to determine the **core task intention**—that is, what the user wants the model to do. (e.g., write and validate code based on a problem description, etc.).\\nThen briefly outline the basic approach to accomplishing this task (e.g., write SQL code to retrieve imformation, etc.).\\nBased on the required approach, assess the **reasoning complexity**, and indicate whether it involves multiple steps or deep analysis. Do not solve the question or provide an answer. Focus solely on interpreting the task type, approach, and cognitive demand.\\nBe concise: your analysis must be no more than two lines and under 500 characters. Use clear, natural, and varied language.\\nEnd your explanation with a statement indicating that complex reasoning is required (Think-on), but express this conclusion with a natural and diverse phrase, not repeating any single pattern. The meaning must be clear, but the expression can vary.\\nPlease analyze the following question as required above:\\n—\\nModel Response\\n—\\nGenerated\\non Sun Sep 28 16:41:38 2025 by LaTeXML<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:39:13 [engine.py:317] Added request chatcmpl-df77aa1c8f9f40d0b1254be04762196c.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from data/output/arxiv_org_22_0_0.txt...vLLM STDOUT: INFO:     127.0.0.1:43386 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 11 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/arxiv_org_22_0_0_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/arxiv_org_22_0_0_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from data/output/arxiv_org_22_0_0.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/arxiv_org_22_0_0_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:39826 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:39828 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:39:22 [logger.py:43] Received request chatcmpl-4aa2aed32d9546ce9e94b4c7baf5d63a: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nao et\\xa0al., 2023; Wei et\\xa0al., 2023) and R1-style \\xa0(DeepSeek-AI et\\xa0al., 2025) systems—have improved complex problem solving via explicit step-by-step reasoning and self-reflection but also suffer from “overthinking” \\xa0(Kumar et\\xa0al., 2025; Sui et\\xa0al., 2025; Nayab et\\xa0al., 2025), where simple queries trigger redundant chains that inflate compute, latency, and token usage, hindering interactive deployment. To address this, existing work focuses on: (i) Training-based adaptive reasoning: RL to conditionally trigger CoT, length penalties and conciseness rewards\\xa0(Aggarwal and Welleck, 2025; Arora and Zanette, 2025; Hou et\\xa0al., 2025; Luo et\\xa0al., 2025; Shen et\\xa0al., 2025; Team et\\xa0al., 2025; Lou et\\xa0al., 2025; Zhan et\\xa0al., 2025), and SFT\\xa0(Munkhdalai et\\xa0al., 2024; Ma et\\xa0al., 2025; Chen et\\xa0al., 2025a; Kang et\\xa0al., 2025) to prefer shorter yet correct reasoning; (ii) External control : prompt or instruction designs that limit steps or defer CoT\\xa0(Xu et\\xa0al., 2025; Renze and Guven, 2024; Chen et\\xa0al., 2024; Munkhbat et\\xa0al., 2025); (iii) Post-hoc Efficiency Optimization: pruning and restructuring chains after generation\\xa0(Aytes et\\xa0al., 2025; Xia et\\xa0al., 2025; Liu et\\xa0al., 2024b; Sun et\\xa0al., 2024; Yang et\\xa0al., 2025).\\nDespite progress, these methods still face coarse supervision, limited adaptation to hard cases due to monotonic shortening, and a lack of principled trade-offs between quality, token cost, and latency.\\n3 Method\\nOur HiPO framework consists of two important components: (i) a hybrid data construction pipeline that generates training data with both Think-on and Think-off responses; (ii) a hybrid reinforcement learning reward system that combines mode-specific accuracy and global average performance, along with a bias-adjustment mechanism to prevent over-reliance on the Think-on mode.\\n3.1 Hybrid Data Construction Pipeline\\nThis process begins with a novel data labeling system leveraging state-of-the-art LLMs to assess each query’s difficulty and domain characteristics. Queries are then classified into Think-on and Think-off categories based on their intrinsic complexity and the availability of verifiable answers.\\n3.1.1 Data Source\\nFigure 2:\\nStatistics of Data Sources.\\nWe construct a challenging corpus for code and mathematics by integrating diverse public and proprietary sources, as illustrated in Fig.\\xa02, including AM-Thinking-v1-Distilled\\xa0(Tian et\\xa0al., 2025), II-Thought-RL\\xa0(Internet, 2025), AceReason-Math\\xa0(Chen et\\xa0al., 2025b), and Skywork-OR1-RL-Data\\xa0(He et\\xa0al., 2025).\\n3.1.2 Data Collection\\nTo effectively enhance the performance of HiPO, we design a structured data construction pipeline aimed at exploring and guiding the model’s preference between the Think-on and Think-off reasoning modes. Our training dataset is meticulously curated to be logically rich, cross-domain, and sufficiently challenging.\\nWe adopt a multi-stage data generation process as shown in Figure\\xa01.\\nFor each query, the pipeline samples NN responses under the Think-on mode and NN responses under the Think-off mode using a dedicated reasoning model.\\nAll responses are then verified for correctness, and the reasoning mode with the higher pass rate is selected as the preferred mode for that query. Let ponp_{\\\\text{on}} and poffp_{\\\\text{off}} denote the pass rates of the Think-on and Think-off modes, respectively. If the difference in pass rates satisfies |pon−poff|<δ|p_{\\\\text{on}}-p_{\\\\text{off}}|<\\\\delta, where δ\\\\delta is a predefined threshold, the Think-off mode is selected. This tie-breaking strategy encourages the model to<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:39:22 [engine.py:317] Added request chatcmpl-4aa2aed32d9546ce9e94b4c7baf5d63a.\n",
            "\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from data/output/arxiv_org_2_0.txt...vLLM STDOUT: INFO:     127.0.0.1:39836 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:39:24 [logger.py:43] Received request chatcmpl-630867d700494303ba2b21fef33b63a0: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nao et\\xa0al., 2023; Wei et\\xa0al., 2023) and R1-style \\xa0(DeepSeek-AI et\\xa0al., 2025) systems—have improved complex problem solving via explicit step-by-step reasoning and self-reflection but also suffer from “overthinking” \\xa0(Kumar et\\xa0al., 2025; Sui et\\xa0al., 2025; Nayab et\\xa0al., 2025), where simple queries trigger redundant chains that inflate compute, latency, and token usage, hindering interactive deployment. To address this, existing work focuses on: (i) Training-based adaptive reasoning: RL to conditionally trigger CoT, length penalties and conciseness rewards\\xa0(Aggarwal and Welleck, 2025; Arora and Zanette, 2025; Hou et\\xa0al., 2025; Luo et\\xa0al., 2025; Shen et\\xa0al., 2025; Team et\\xa0al., 2025; Lou et\\xa0al., 2025; Zhan et\\xa0al., 2025), and SFT\\xa0(Munkhdalai et\\xa0al., 2024; Ma et\\xa0al., 2025; Chen et\\xa0al., 2025a; Kang et\\xa0al., 2025) to prefer shorter yet correct reasoning; (ii) External control : prompt or instruction designs that limit steps or defer CoT\\xa0(Xu et\\xa0al., 2025; Renze and Guven, 2024; Chen et\\xa0al., 2024; Munkhbat et\\xa0al., 2025); (iii) Post-hoc Efficiency Optimization: pruning and restructuring chains after generation\\xa0(Aytes et\\xa0al., 2025; Xia et\\xa0al., 2025; Liu et\\xa0al., 2024b; Sun et\\xa0al., 2024; Yang et\\xa0al., 2025).\\nDespite progress, these methods still face coarse supervision, limited adaptation to hard cases due to monotonic shortening, and a lack of principled trade-offs between quality, token cost, and latency.\\n3 Method\\nOur HiPO framework consists of two important components: (i) a hybrid data construction pipeline that generates training data with both Think-on and Think-off responses; (ii) a hybrid reinforcement learning reward system that combines mode-specific accuracy and global average performance, along with a bias-adjustment mechanism to prevent over-reliance on the Think-on mode.\\n3.1 Hybrid Data Construction Pipeline\\nThis process begins with a novel data labeling system leveraging state-of-the-art LLMs to assess each query’s difficulty and domain characteristics. Queries are then classified into Think-on and Think-off categories based on their intrinsic complexity and the availability of verifiable answers.\\n3.1.1 Data Source\\nFigure 2:\\nStatistics of Data Sources.\\nWe construct a challenging corpus for code and mathematics by integrating diverse public and proprietary sources, as illustrated in Fig.\\xa02, including AM-Thinking-v1-Distilled\\xa0(Tian et\\xa0al., 2025), II-Thought-RL\\xa0(Internet, 2025), AceReason-Math\\xa0(Chen et\\xa0al., 2025b), and Skywork-OR1-RL-Data\\xa0(He et\\xa0al., 2025).\\n3.1.2 Data Collection\\nTo effectively enhance the performance of HiPO, we design a structured data construction pipeline aimed at exploring and guiding the model’s preference between the Think-on and Think-off reasoning modes. Our training dataset is meticulously curated to be logically rich, cross-domain, and sufficiently challenging.\\nWe adopt a multi-stage data generation process as shown in Figure\\xa01.\\nFor each query, the pipeline samples NN responses under the Think-on mode and NN responses under the Think-off mode using a dedicated reasoning model.\\nAll responses are then verified for correctness, and the reasoning mode with the higher pass rate is selected as the preferred mode for that query. Let ponp_{\\\\text{on}} and poffp_{\\\\text{off}} denote the pass rates of the Think-on and Think-off modes, respectively. If the difference in pass rates satisfies |pon−poff|<δ|p_{\\\\text{on}}-p_{\\\\text{off}}|<\\\\delta, where δ\\\\delta is a predefined threshold, the Think-off mode is selected. This tie-breaking strategy encourages the model to<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:39:24 [engine.py:317] Added request chatcmpl-630867d700494303ba2b21fef33b63a0.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from data/output/arxiv_org_2_0.txt...vLLM STDOUT: INFO:     127.0.0.1:44506 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 11 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/arxiv_org_2_0_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/arxiv_org_2_0_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from data/output/arxiv_org_2_0.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/arxiv_org_2_0_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:54294 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:54296 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:39:33 [logger.py:43] Received request chatcmpl-412f08d81e194f16bf9840ca31eaee47: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n28.9%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 28.9\\\\%}\\n0.82↓18.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 18.0\\\\%}\\n92.7↑5.6%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 5.6\\\\%}\\n824↓80.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 80.8\\\\%}\\n0.18↓82.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 82.0\\\\%}\\n84.4↑10.8%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 10.8\\\\%}\\n2070↓56.4%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 56.4\\\\%}\\n0.24↓76.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 76.0\\\\%}\\nTable 4: Performance of HiPO on more models.\\nWe analyze two key dimensions: (i) reasoning-mode activation (<think_on> vs. <think_off>) and (ii) token efficiency across RL training steps and benchmark tasks.\\nSpecifically, during the training and evaluation processes, we track how the model’s decision-making evolves by monitoring the frequency of reasoning-mode activations and the corresponding output length.\\nThink-on vs. Think-off Dynamics During Training and Inference\\nWe logged the frequency of <think_on> and <think_off> activations at each step. As shown in Figure\\xa0LABEL:fig:Training_Ratio, HiPO not only improves final accuracy but also sharpens the model’s gating behavior, allowing it to skip unnecessary reasoning. Specifically, the gap between <think_on> and <think_off> activations decreases from 89.5% at the beginning of training to 53.1% by the end. In Figure LABEL:fig:Testing_Ratio shows the proportion of Think-on activations across different datasets during inference. Reasoning-intensive tasks, including AIME2024, and LiveCodeBench, consistently demonstrate high Think-on activation rates (>70%) throughout training. Conversely, tasks that require less explicit reasoning, such as HumanEval — exhibit a clear downward trend in Think-on activation as training progresses.\\nToken Count Dynamics During Training and Inference\\nDuring RL training, the average token count shows a consistent downward trend in Figure\\xa0LABEL:fig:Training_Token, which indicates that the model gradually learns to produce more concise responses and highlight the HiPO reward design in encouraging efficient token usage\\nBesides, Figure LABEL:fig:Testing_Token shows the corresponding dynamics in average token counts per generated response during inference, and we also observe consistent token reduction in training.\\nGeneralization on More Models\\nIn Table\\xa04, we report the performance of HiPO on Qwen3-1.7B and Qwen3-32B, which shows consistent improvements on both accuracy and efficiency.\\n5 Conclusion\\nIn this work, we introduced HiPO, a hybrid framework for adaptive reasoning in LLMs. By combining a hybrid data pipeline with a hybrid reinforcement learning reward system, HiPO enables models to dynamically balance Think-on and Think-off reasoning, mitigating the issue of overthinking while preserving accuracy. Experiments demonstrate that HiPO achieves competitive or superior accuracy with significantly improved token efficiency and reduced reasoning redundancy.\\nReferences\\nYao et\\xa0al. [2023]\\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas\\xa0L. Griffiths, Yuan Cao, and Karthik Narasimhan.\\nTree of thoughts: Deliberate problem solving with large language models.\\narXiv preprint arXiv: 2305.10601, 2023.\\nWei et\\xa0al. [2023]\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed\\xa0Chi, Quoc Le, and Denny Zhou.\\nChain-of-thought prompting elicits reasoning in large language models, 2023.\\nURL https://arxiv.org/abs/2201.11903.\\nKumar et\\xa0al. [2025]\\nAbhinav Kumar, Jaechul Roh, Ali Naseh, Marzena Karpinska, Mohit Iyyer, Amir Houmansadr, and Eugene Bagdasarian.\\nOverthink: Slowdown attacks on reasoning llms, 2025.\\nURL https://arxiv.org/abs/2502.02542.\\nSui et\\xa0al<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:39:33 [engine.py:317] Added request chatcmpl-412f08d81e194f16bf9840ca31eaee47.\n",
            "\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from data/output/arxiv_org_15_0.txt...vLLM STDOUT: INFO:     127.0.0.1:54310 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:39:35 [logger.py:43] Received request chatcmpl-255d4537f45c400ca16b635ee72d81fb: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\n28.9%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 28.9\\\\%}\\n0.82↓18.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 18.0\\\\%}\\n92.7↑5.6%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 5.6\\\\%}\\n824↓80.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 80.8\\\\%}\\n0.18↓82.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 82.0\\\\%}\\n84.4↑10.8%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 10.8\\\\%}\\n2070↓56.4%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 56.4\\\\%}\\n0.24↓76.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 76.0\\\\%}\\nTable 4: Performance of HiPO on more models.\\nWe analyze two key dimensions: (i) reasoning-mode activation (<think_on> vs. <think_off>) and (ii) token efficiency across RL training steps and benchmark tasks.\\nSpecifically, during the training and evaluation processes, we track how the model’s decision-making evolves by monitoring the frequency of reasoning-mode activations and the corresponding output length.\\nThink-on vs. Think-off Dynamics During Training and Inference\\nWe logged the frequency of <think_on> and <think_off> activations at each step. As shown in Figure\\xa0LABEL:fig:Training_Ratio, HiPO not only improves final accuracy but also sharpens the model’s gating behavior, allowing it to skip unnecessary reasoning. Specifically, the gap between <think_on> and <think_off> activations decreases from 89.5% at the beginning of training to 53.1% by the end. In Figure LABEL:fig:Testing_Ratio shows the proportion of Think-on activations across different datasets during inference. Reasoning-intensive tasks, including AIME2024, and LiveCodeBench, consistently demonstrate high Think-on activation rates (>70%) throughout training. Conversely, tasks that require less explicit reasoning, such as HumanEval — exhibit a clear downward trend in Think-on activation as training progresses.\\nToken Count Dynamics During Training and Inference\\nDuring RL training, the average token count shows a consistent downward trend in Figure\\xa0LABEL:fig:Training_Token, which indicates that the model gradually learns to produce more concise responses and highlight the HiPO reward design in encouraging efficient token usage\\nBesides, Figure LABEL:fig:Testing_Token shows the corresponding dynamics in average token counts per generated response during inference, and we also observe consistent token reduction in training.\\nGeneralization on More Models\\nIn Table\\xa04, we report the performance of HiPO on Qwen3-1.7B and Qwen3-32B, which shows consistent improvements on both accuracy and efficiency.\\n5 Conclusion\\nIn this work, we introduced HiPO, a hybrid framework for adaptive reasoning in LLMs. By combining a hybrid data pipeline with a hybrid reinforcement learning reward system, HiPO enables models to dynamically balance Think-on and Think-off reasoning, mitigating the issue of overthinking while preserving accuracy. Experiments demonstrate that HiPO achieves competitive or superior accuracy with significantly improved token efficiency and reduced reasoning redundancy.\\nReferences\\nYao et\\xa0al. [2023]\\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas\\xa0L. Griffiths, Yuan Cao, and Karthik Narasimhan.\\nTree of thoughts: Deliberate problem solving with large language models.\\narXiv preprint arXiv: 2305.10601, 2023.\\nWei et\\xa0al. [2023]\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed\\xa0Chi, Quoc Le, and Denny Zhou.\\nChain-of-thought prompting elicits reasoning in large language models, 2023.\\nURL https://arxiv.org/abs/2201.11903.\\nKumar et\\xa0al. [2025]\\nAbhinav Kumar, Jaechul Roh, Ali Naseh, Marzena Karpinska, Mohit Iyyer, Amir Houmansadr, and Eugene Bagdasarian.\\nOverthink: Slowdown attacks on reasoning llms, 2025.\\nURL https://arxiv.org/abs/2502.02542.\\nSui et\\xa0al<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:39:35 [engine.py:317] Added request chatcmpl-255d4537f45c400ca16b635ee72d81fb.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from data/output/arxiv_org_15_0.txt...vLLM STDOUT: INFO:     127.0.0.1:54312 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 15 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/arxiv_org_15_0_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/arxiv_org_15_0_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from data/output/arxiv_org_15_0.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/arxiv_org_15_0_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:36292 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:36300 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:39:44 [logger.py:43] Received request chatcmpl-631d7553c7b4493b8e6045ac75bcea96: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n2024.\\ndoi: 10.1109/fllm63129.2024.10852493.\\nURL http://dx.doi.org/10.1109/FLLM63129.2024.10852493.\\nChen et\\xa0al. [2024]\\nQiguang Chen, Libo Qin, Jiaqi Wang, Jinxuan Zhou, and Wanxiang Che.\\nUnlocking the capabilities of thought: A reasoning boundary framework to quantify and optimize chain-of-thought, 2024.\\nURL https://arxiv.org/abs/2410.05695.\\nMunkhbat et\\xa0al. [2025]\\nTergel Munkhbat, Namgyu Ho, Seo\\xa0Hyun Kim, Yongjin Yang, Yujin Kim, and Se-Young Yun.\\nSelf-training elicits concise reasoning in large language models, 2025.\\nURL https://arxiv.org/abs/2502.20122.\\nLiu et\\xa0al. [2024a]\\nAixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et\\xa0al.\\nDeepseek-v3 technical report.\\narXiv preprint arXiv:2412.19437, 2024a.\\nShao et\\xa0al. [2024]\\nZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y.\\xa0K. Li, Y.\\xa0Wu, and Daya Guo.\\nDeepseekmath: Pushing the limits of mathematical reasoning in open language models.\\narXiv preprint arXiv: 2402.03300, 2024.\\nURL https://arxiv.org/abs/2402.03300v3.\\nZheng et\\xa0al. [2025]\\nChujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An\\xa0Yang, Jingren Zhou, and Junyang Lin.\\nGroup sequence policy optimization, 2025.\\nURL https://arxiv.org/abs/2507.18071.\\nYue et\\xa0al. [2025]\\nYu\\xa0Yue, Yufeng Yuan, Qiying Yu, Xiaochen Zuo, Ruofei Zhu, Wenyuan Xu, Jiaze Chen, Chengyi Wang, TianTian Fan, Zhengyin Du, Xiangpeng Wei, Xiangyu Yu, Gaohong Liu, Juncai Liu, Lingjun Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Ru\\xa0Zhang, Xin Liu, Mingxuan Wang, Yonghui Wu, and Lin Yan.\\nVapo: Efficient and reliable reinforcement learning for advanced reasoning tasks, 2025.\\nURL https://arxiv.org/abs/2504.05118.\\nSchulman et\\xa0al. [2017]\\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.\\nProximal policy optimization algorithms, 2017.\\nURL https://arxiv.org/abs/1707.06347.\\nRafailov et\\xa0al. [2024]\\nRafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher\\xa0D. Manning, and Chelsea Finn.\\nDirect preference optimization: Your language model is secretly a reward model, 2024.\\nURL https://arxiv.org/abs/2305.18290.\\nDeepSeek-AI et\\xa0al. [2025]\\nDeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu\\xa0Wu, Z.\\xa0F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:39:44 [engine.py:317] Added request chatcmpl-631d7553c7b4493b8e6045ac75bcea96.\n",
            "\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from data/output/arxiv_org_18_0_0.txt...vLLM STDOUT: INFO:     127.0.0.1:36316 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[32m⠹\u001b[0m Generating qa content from data/output/arxiv_org_18_0_0.txt...vLLM STDOUT: INFO 10-03 08:39:47 [logger.py:43] Received request chatcmpl-487f9bb460b64440b36fbf1a7ee9ac6e: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\n2024.\\ndoi: 10.1109/fllm63129.2024.10852493.\\nURL http://dx.doi.org/10.1109/FLLM63129.2024.10852493.\\nChen et\\xa0al. [2024]\\nQiguang Chen, Libo Qin, Jiaqi Wang, Jinxuan Zhou, and Wanxiang Che.\\nUnlocking the capabilities of thought: A reasoning boundary framework to quantify and optimize chain-of-thought, 2024.\\nURL https://arxiv.org/abs/2410.05695.\\nMunkhbat et\\xa0al. [2025]\\nTergel Munkhbat, Namgyu Ho, Seo\\xa0Hyun Kim, Yongjin Yang, Yujin Kim, and Se-Young Yun.\\nSelf-training elicits concise reasoning in large language models, 2025.\\nURL https://arxiv.org/abs/2502.20122.\\nLiu et\\xa0al. [2024a]\\nAixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et\\xa0al.\\nDeepseek-v3 technical report.\\narXiv preprint arXiv:2412.19437, 2024a.\\nShao et\\xa0al. [2024]\\nZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y.\\xa0K. Li, Y.\\xa0Wu, and Daya Guo.\\nDeepseekmath: Pushing the limits of mathematical reasoning in open language models.\\narXiv preprint arXiv: 2402.03300, 2024.\\nURL https://arxiv.org/abs/2402.03300v3.\\nZheng et\\xa0al. [2025]\\nChujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An\\xa0Yang, Jingren Zhou, and Junyang Lin.\\nGroup sequence policy optimization, 2025.\\nURL https://arxiv.org/abs/2507.18071.\\nYue et\\xa0al. [2025]\\nYu\\xa0Yue, Yufeng Yuan, Qiying Yu, Xiaochen Zuo, Ruofei Zhu, Wenyuan Xu, Jiaze Chen, Chengyi Wang, TianTian Fan, Zhengyin Du, Xiangpeng Wei, Xiangyu Yu, Gaohong Liu, Juncai Liu, Lingjun Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Ru\\xa0Zhang, Xin Liu, Mingxuan Wang, Yonghui Wu, and Lin Yan.\\nVapo: Efficient and reliable reinforcement learning for advanced reasoning tasks, 2025.\\nURL https://arxiv.org/abs/2504.05118.\\nSchulman et\\xa0al. [2017]\\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.\\nProximal policy optimization algorithms, 2017.\\nURL https://arxiv.org/abs/1707.06347.\\nRafailov et\\xa0al. [2024]\\nRafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher\\xa0D. Manning, and Chelsea Finn.\\nDirect preference optimization: Your language model is secretly a reward model, 2024.\\nURL https://arxiv.org/abs/2305.18290.\\nDeepSeek-AI et\\xa0al. [2025]\\nDeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu\\xa0Wu, Z.\\xa0F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:39:47 [engine.py:317] Added request chatcmpl-487f9bb460b64440b36fbf1a7ee9ac6e.\n",
            "\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from data/output/arxiv_org_18_0_0.txt...vLLM STDOUT: INFO:     127.0.0.1:36324 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 10 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/arxiv_org_18_0_0_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/arxiv_org_18_0_0_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from data/output/arxiv_org_18_0_0.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/arxiv_org_18_0_0_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:39736 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:39752 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:39:56 [logger.py:43] Received request chatcmpl-ce425a5d4dfd45348c445199688222c7: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nmean}(\\\\mathbf{r}_{\\\\text{on}})-\\\\text{mean}(\\\\mathbf{r}))+\\\\gamma\\\\cdot(r_{i}-\\\\text{mean}(\\\\mathbf{r}))}{\\\\text{std}(\\\\mathbf{r})},&\\\\text{if }M_{i}=\\\\text{on},\\\\\\\\[6.0pt]\\n\\\\frac{(\\\\text{mean}(\\\\mathbf{r}_{\\\\text{off}})-\\\\text{mean}(\\\\mathbf{r}))+\\\\gamma\\\\cdot(r_{i}-\\\\text{mean}(\\\\mathbf{r}))}{\\\\text{std}(\\\\mathbf{r})},&\\\\text{if }M_{i}=\\\\text{off}.\\\\end{cases}\\n(2)\\nIn contrast to the judge advantage function, the advantage AianswerA^{\\\\text{answer}}_{i} is computed within the context of the selected reasoning mode. Since the mode MiM_{i} has already been determined prior to response generation, it is natural to assess the response quality relative to other responses within the same mode. This local normalization using mode-specific mean and standard deviation focuses the learning signal on intra-mode variance, encouraging the model to improve response quality without conflating mode preference. For response ii, the answer advantage is defined as:\\nAianswer={ri−mean\\u200b(𝐫on)std\\u200b(𝐫on),if\\xa0\\u200bMi=on,ri−mean\\u200b(𝐫off)std\\u200b(𝐫off),if\\xa0\\u200bMi=off.A^{\\\\text{answer}}_{i}=\\\\begin{cases}\\\\frac{r_{i}-\\\\text{mean}(\\\\mathbf{r}_{\\\\text{on}})}{\\\\text{std}(\\\\mathbf{r}_{\\\\text{on}})},&\\\\text{if }M_{i}=\\\\text{on},\\\\\\\\[6.0pt]\\n\\\\frac{r_{i}-\\\\text{mean}(\\\\mathbf{r}_{\\\\text{off}})}{\\\\text{std}(\\\\mathbf{r}_{\\\\text{off}})},&\\\\text{if }M_{i}=\\\\text{off}.\\\\end{cases}\\n(3)\\nTo assign token-level reward for training with reinforcement learning, we define the final reward for each token tt in sample ii as follows:\\nAi,t={Aianswer,if token\\xa0\\u200bt∈𝒯answer,Aijudge,if token\\xa0\\u200bt∈𝒯judge.\\\\mathrm{A}_{i,t}=\\\\begin{cases}\\\\mathrm{A}^{\\\\text{answer}}_{i},&\\\\text{if token }t\\\\in\\\\mathcal{T}^{\\\\text{answer}},\\\\\\\\\\n\\\\mathrm{A}^{\\\\text{judge}}_{i},&\\\\text{if token }t\\\\in\\\\mathcal{T}^{\\\\text{judge}}.\\\\end{cases}\\n(4)\\nwhere 𝒯judge\\\\mathcal{T}^{\\\\text{judge}} and 𝒯answer\\\\mathcal{T}^{\\\\text{answer}} denote the token index sets corresponding to the judge segment and the answer segment, respectively, within each response.\\nGiven a query qq, HiPO generates a collection of candidate outputs {oi}i=1G\\\\{o_{i}\\\\}_{i=1}^{G} from the old policy πθold\\\\pi_{\\\\theta_{\\\\text{old}}}. For each output oio_{i}, let 𝒯i\\\\mathcal{T}_{i} denote the set of token positions in response ii, i.e., 𝒯i=𝒯judge∪𝒯answer\\\\mathcal{T}_{i}=\\\\mathcal{T}^{\\\\text{judge}}\\\\cup\\\\mathcal{T}^{\\\\text{answer}}. We define the per-token probability ratio as\\nρi,t=πθ(yi,t|hi,t)πθold(yi,t|hi,t)\\\\rho_{i,t}=\\\\frac{\\\\pi_{\\\\theta}\\\\!\\\\left(y_{i,t}\\\\,\\\\middle|\\\\,h_{i,t}\\\\right)}{\\\\pi_{\\\\theta_{\\\\text{old}}}\\\\!\\\\left(y_{i,t}\\\\,\\\\middle|\\\\,h_{i,t}\\\\right)},\\nwhere yi,ty_{i,t}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:39:56 [engine.py:317] Added request chatcmpl-ce425a5d4dfd45348c445199688222c7.\n",
            "\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from data/output/arxiv_org_5_0_0.txt...vLLM STDOUT: INFO:     127.0.0.1:39764 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:39:58 [logger.py:43] Received request chatcmpl-8b9944fd835845eb940f0408f0ccb2bb: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nmean}(\\\\mathbf{r}_{\\\\text{on}})-\\\\text{mean}(\\\\mathbf{r}))+\\\\gamma\\\\cdot(r_{i}-\\\\text{mean}(\\\\mathbf{r}))}{\\\\text{std}(\\\\mathbf{r})},&\\\\text{if }M_{i}=\\\\text{on},\\\\\\\\[6.0pt]\\n\\\\frac{(\\\\text{mean}(\\\\mathbf{r}_{\\\\text{off}})-\\\\text{mean}(\\\\mathbf{r}))+\\\\gamma\\\\cdot(r_{i}-\\\\text{mean}(\\\\mathbf{r}))}{\\\\text{std}(\\\\mathbf{r})},&\\\\text{if }M_{i}=\\\\text{off}.\\\\end{cases}\\n(2)\\nIn contrast to the judge advantage function, the advantage AianswerA^{\\\\text{answer}}_{i} is computed within the context of the selected reasoning mode. Since the mode MiM_{i} has already been determined prior to response generation, it is natural to assess the response quality relative to other responses within the same mode. This local normalization using mode-specific mean and standard deviation focuses the learning signal on intra-mode variance, encouraging the model to improve response quality without conflating mode preference. For response ii, the answer advantage is defined as:\\nAianswer={ri−mean\\u200b(𝐫on)std\\u200b(𝐫on),if\\xa0\\u200bMi=on,ri−mean\\u200b(𝐫off)std\\u200b(𝐫off),if\\xa0\\u200bMi=off.A^{\\\\text{answer}}_{i}=\\\\begin{cases}\\\\frac{r_{i}-\\\\text{mean}(\\\\mathbf{r}_{\\\\text{on}})}{\\\\text{std}(\\\\mathbf{r}_{\\\\text{on}})},&\\\\text{if }M_{i}=\\\\text{on},\\\\\\\\[6.0pt]\\n\\\\frac{r_{i}-\\\\text{mean}(\\\\mathbf{r}_{\\\\text{off}})}{\\\\text{std}(\\\\mathbf{r}_{\\\\text{off}})},&\\\\text{if }M_{i}=\\\\text{off}.\\\\end{cases}\\n(3)\\nTo assign token-level reward for training with reinforcement learning, we define the final reward for each token tt in sample ii as follows:\\nAi,t={Aianswer,if token\\xa0\\u200bt∈𝒯answer,Aijudge,if token\\xa0\\u200bt∈𝒯judge.\\\\mathrm{A}_{i,t}=\\\\begin{cases}\\\\mathrm{A}^{\\\\text{answer}}_{i},&\\\\text{if token }t\\\\in\\\\mathcal{T}^{\\\\text{answer}},\\\\\\\\\\n\\\\mathrm{A}^{\\\\text{judge}}_{i},&\\\\text{if token }t\\\\in\\\\mathcal{T}^{\\\\text{judge}}.\\\\end{cases}\\n(4)\\nwhere 𝒯judge\\\\mathcal{T}^{\\\\text{judge}} and 𝒯answer\\\\mathcal{T}^{\\\\text{answer}} denote the token index sets corresponding to the judge segment and the answer segment, respectively, within each response.\\nGiven a query qq, HiPO generates a collection of candidate outputs {oi}i=1G\\\\{o_{i}\\\\}_{i=1}^{G} from the old policy πθold\\\\pi_{\\\\theta_{\\\\text{old}}}. For each output oio_{i}, let 𝒯i\\\\mathcal{T}_{i} denote the set of token positions in response ii, i.e., 𝒯i=𝒯judge∪𝒯answer\\\\mathcal{T}_{i}=\\\\mathcal{T}^{\\\\text{judge}}\\\\cup\\\\mathcal{T}^{\\\\text{answer}}. We define the per-token probability ratio as\\nρi,t=πθ(yi,t|hi,t)πθold(yi,t|hi,t)\\\\rho_{i,t}=\\\\frac{\\\\pi_{\\\\theta}\\\\!\\\\left(y_{i,t}\\\\,\\\\middle|\\\\,h_{i,t}\\\\right)}{\\\\pi_{\\\\theta_{\\\\text{old}}}\\\\!\\\\left(y_{i,t}\\\\,\\\\middle|\\\\,h_{i,t}\\\\right)},\\nwhere yi,ty_{i,t}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:39:58 [engine.py:317] Added request chatcmpl-8b9944fd835845eb940f0408f0ccb2bb.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from data/output/arxiv_org_5_0_0.txt...vLLM STDOUT: INFO:     127.0.0.1:39768 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 4 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/arxiv_org_5_0_0_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/arxiv_org_5_0_0_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from data/output/arxiv_org_5_0_0.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/arxiv_org_5_0_0_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:42504 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:42520 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:40:07 [logger.py:43] Received request chatcmpl-8e337a215fe245b5b3f296892dad2e9d: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n) training-based adaptive reasoning, where reinforcement learning (RL)\\xa0(Aggarwal and Welleck, 2025; Arora and Zanette, 2025; Hou et\\xa0al., 2025; Luo et\\xa0al., 2025; Shen et\\xa0al., 2025; Team et\\xa0al., 2025; Lou et\\xa0al., 2025) or supervised fine-tuning (SFT)\\xa0(Munkhdalai et\\xa0al., 2024; Ma et\\xa0al., 2025; Chen et\\xa0al., 2025a; Kang et\\xa0al., 2025) encourages concise reasoning through length penalties or conciseness rewards; (ii) external control, which constrains reasoning with handcrafted prompts or dynamic instructions\\xa0(Xu et\\xa0al., 2025; Renze and Guven, 2024; Chen et\\xa0al., 2024; Munkhbat et\\xa0al., 2025). While effective to some extent, these methods suffer from important limitations: coarse supervision signals, monotonic incentives that discourage deeper reasoning on difficult problems, and a lack of principled trade-offs between accuracy, latency, and token efficiency.\\nTo address these challenges, we introduce HiPO (Hybrid Policy Optimization), a unified framework for adaptive reasoning in LLMs. HiPO is designed to enable models to decide when to “think” (i.e., Think-on)and when to skip reasoning (i.e., Think-off), thereby striking a balance between correctness and efficiency. Specifically, our approach builds on two key innovations:\\n(1) Hybrid Data Construction Pipeline. As shown in Figure\\xa01, we first collect the training data containing both Think-on and Think-off responses. Each query is automatically categorized based on its difficulty and response correctness. Then, a high-performance model (i.e., DeepSeek-V3\\xa0(Liu et\\xa0al., 2024a)) is used to produce the explicit explanations to justify its reasoning-mode decisions. Finally, for each query, the final response based on the thinking mode and the corresponding explanation construct the hybrid output.\\n(2) Hybrid Reinforcement Learning Reward System. We propose a hybrid reward design that balances Think-on and Think-off decisions. Specifically, a bias adjustment mechanism prevents the model from over-relying on verbose reasoning, while mode-aware advantage functions align reasoning-mode selection with actual performance gains. This ensures stable training and principled control over reasoning depth.\\nIn summary, our contributions are threefold:\\n•\\nWe propose HiPO for adaptive LLM reasoning, which mainly includes the hybrid data construction and hybrid reinforcement learning.\\n•\\nIn the hybrid data construction pipeline, we produce logically rich Think-on and concise Think-off responses with the justification for the thinking mode. Then, for hybrid reinforcement learning, we introduce both the judge analysis and the response reward signal to enable principled control of reasoning depth.\\n•\\nExperimental results on multiple datasets demonstrate that HiPO can consistently reduce redundant reasoning while improving or maintaining accuracy.\\nFigure 1: Framework of the hybrid data construction pipeline.\\n2 Related Works\\nRL for LLM Reasoning.\\nRecent advances in reinforcement learning (RL) have significantly enhanced LLMs’ complex reasoning capabilities, moving beyond supervised fine-tuning (SFT) limitations. State-of-the-art RL algorithms demonstrate superior performance in mathematical reasoning and multi-step problem solving: GRPO \\xa0(Shao et\\xa0al., 2024) stabilizes training through intra-group relative reward comparisons; GSPO \\xa0(Zheng et\\xa0al., 2025) defines sequence-level importance ratios and applies sequence-level clipping/rewarding/updates to improve efficiency and stabilize MoE training; VAPO \\xa0(Yue et\\xa0al., 2025) ensures reward consistency via value-aware optimization; PPO \\xa0(Schulman et\\xa0al., 2017) constrains policy updates through clipping mechanisms; and DPO \\xa0(Rafailov et\\xa0al., 2024) learns directly from human preferences without explicit reward modeling.\\nAdaptive Reasoning.\\nReasoning-oriented large language models—exemplified by Chain-of-Thought (CoT) \\xa0(Yao et\\xa0al., 2023; Wei et\\xa0al., 2023) and R1-style \\xa0(DeepSeek-AI et\\xa0al., 2025) systems—have improved complex problem solving via explicit step-by-step reasoning and self-reflection but also suffer from “overthinking”<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:40:07 [engine.py:317] Added request chatcmpl-8e337a215fe245b5b3f296892dad2e9d.\n",
            "\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from data/output/arxiv_org_1_0_0.txt...vLLM STDOUT: INFO:     127.0.0.1:42536 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:40:09 [logger.py:43] Received request chatcmpl-aa4fd45db8a54a8ab3e9ff9de94feddd: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\n) training-based adaptive reasoning, where reinforcement learning (RL)\\xa0(Aggarwal and Welleck, 2025; Arora and Zanette, 2025; Hou et\\xa0al., 2025; Luo et\\xa0al., 2025; Shen et\\xa0al., 2025; Team et\\xa0al., 2025; Lou et\\xa0al., 2025) or supervised fine-tuning (SFT)\\xa0(Munkhdalai et\\xa0al., 2024; Ma et\\xa0al., 2025; Chen et\\xa0al., 2025a; Kang et\\xa0al., 2025) encourages concise reasoning through length penalties or conciseness rewards; (ii) external control, which constrains reasoning with handcrafted prompts or dynamic instructions\\xa0(Xu et\\xa0al., 2025; Renze and Guven, 2024; Chen et\\xa0al., 2024; Munkhbat et\\xa0al., 2025). While effective to some extent, these methods suffer from important limitations: coarse supervision signals, monotonic incentives that discourage deeper reasoning on difficult problems, and a lack of principled trade-offs between accuracy, latency, and token efficiency.\\nTo address these challenges, we introduce HiPO (Hybrid Policy Optimization), a unified framework for adaptive reasoning in LLMs. HiPO is designed to enable models to decide when to “think” (i.e., Think-on)and when to skip reasoning (i.e., Think-off), thereby striking a balance between correctness and efficiency. Specifically, our approach builds on two key innovations:\\n(1) Hybrid Data Construction Pipeline. As shown in Figure\\xa01, we first collect the training data containing both Think-on and Think-off responses. Each query is automatically categorized based on its difficulty and response correctness. Then, a high-performance model (i.e., DeepSeek-V3\\xa0(Liu et\\xa0al., 2024a)) is used to produce the explicit explanations to justify its reasoning-mode decisions. Finally, for each query, the final response based on the thinking mode and the corresponding explanation construct the hybrid output.\\n(2) Hybrid Reinforcement Learning Reward System. We propose a hybrid reward design that balances Think-on and Think-off decisions. Specifically, a bias adjustment mechanism prevents the model from over-relying on verbose reasoning, while mode-aware advantage functions align reasoning-mode selection with actual performance gains. This ensures stable training and principled control over reasoning depth.\\nIn summary, our contributions are threefold:\\n•\\nWe propose HiPO for adaptive LLM reasoning, which mainly includes the hybrid data construction and hybrid reinforcement learning.\\n•\\nIn the hybrid data construction pipeline, we produce logically rich Think-on and concise Think-off responses with the justification for the thinking mode. Then, for hybrid reinforcement learning, we introduce both the judge analysis and the response reward signal to enable principled control of reasoning depth.\\n•\\nExperimental results on multiple datasets demonstrate that HiPO can consistently reduce redundant reasoning while improving or maintaining accuracy.\\nFigure 1: Framework of the hybrid data construction pipeline.\\n2 Related Works\\nRL for LLM Reasoning.\\nRecent advances in reinforcement learning (RL) have significantly enhanced LLMs’ complex reasoning capabilities, moving beyond supervised fine-tuning (SFT) limitations. State-of-the-art RL algorithms demonstrate superior performance in mathematical reasoning and multi-step problem solving: GRPO \\xa0(Shao et\\xa0al., 2024) stabilizes training through intra-group relative reward comparisons; GSPO \\xa0(Zheng et\\xa0al., 2025) defines sequence-level importance ratios and applies sequence-level clipping/rewarding/updates to improve efficiency and stabilize MoE training; VAPO \\xa0(Yue et\\xa0al., 2025) ensures reward consistency via value-aware optimization; PPO \\xa0(Schulman et\\xa0al., 2017) constrains policy updates through clipping mechanisms; and DPO \\xa0(Rafailov et\\xa0al., 2024) learns directly from human preferences without explicit reward modeling.\\nAdaptive Reasoning.\\nReasoning-oriented large language models—exemplified by Chain-of-Thought (CoT) \\xa0(Yao et\\xa0al., 2023; Wei et\\xa0al., 2023) and R1-style \\xa0(DeepSeek-AI et\\xa0al., 2025) systems—have improved complex problem solving via explicit step-by-step reasoning and self-reflection but also suffer from “overthinking”<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:40:09 [engine.py:317] Added request chatcmpl-aa4fd45db8a54a8ab3e9ff9de94feddd.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from data/output/arxiv_org_1_0_0.txt...vLLM STDOUT: INFO:     127.0.0.1:42540 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 8 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/arxiv_org_1_0_0_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/arxiv_org_1_0_0_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from data/output/arxiv_org_1_0_0.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/arxiv_org_1_0_0_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:35526 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:35530 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:40:18 [logger.py:43] Received request chatcmpl-51afe90abd9f433281bd3410a5f12c2c: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nXiao Bi, Xiaokang Zhang, Xingkai Yu, Yu\\xa0Wu, Z.\\xa0F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H.\\xa0Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J.\\xa0L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong\\nZhang, Ruizhe Pan, Runji Wang, R.\\xa0J. Chen, R.\\xa0L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S.\\xa0S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T.\\xa0Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W.\\xa0L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X.\\xa0Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y.\\xa0K. Li, Y.\\xa0Q. Wang, Y.\\xa0X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi\\xa0Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y.\\xa0X. Zhu,\\nYanhong Xu, Yanping Huang, Yaohui Li, Yi\\xa0Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z.\\xa0Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang.\\nDeepseek-r1: Incentivizing reasoning capability in llms via reinforcement<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:40:18 [engine.py:317] Added request chatcmpl-51afe90abd9f433281bd3410a5f12c2c.\n",
            "\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from data/output/arxiv_org_19_0.txt...vLLM STDOUT: INFO:     127.0.0.1:35546 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:40:21 [logger.py:43] Received request chatcmpl-14a7dd9668e44e4ea6dd5e1ba9065abe: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\n Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu\\xa0Wu, Z.\\xa0F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H.\\xa0Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J.\\xa0L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong\\nZhang, Ruizhe Pan, Runji Wang, R.\\xa0J. Chen, R.\\xa0L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S.\\xa0S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T.\\xa0Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W.\\xa0L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X.\\xa0Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y.\\xa0K. Li, Y.\\xa0Q. Wang, Y.\\xa0X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi\\xa0Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y.\\xa0X. Zhu,\\nYanhong Xu, Yanping Huang, Yaohui Li, Yi\\xa0Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z.\\xa0Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang.\\nDeepseek-r1: Incentivizing reasoning capability in llms via reinforcement<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:40:21 [engine.py:317] Added request chatcmpl-14a7dd9668e44e4ea6dd5e1ba9065abe.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from data/output/arxiv_org_19_0.txt...vLLM STDOUT: INFO:     127.0.0.1:35554 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 14 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/arxiv_org_19_0_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/arxiv_org_19_0_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from data/output/arxiv_org_19_0.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/arxiv_org_19_0_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:33954 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:33960 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:40:30 [logger.py:43] Received request chatcmpl-1ce5f2a6110d4ef5bb29102da7a86d10: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n28.9%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 28.9\\\\%}\\n0.82↓18.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 18.0\\\\%}\\n92.7↑5.6%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 5.6\\\\%}\\n824↓80.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 80.8\\\\%}\\n0.18↓82.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 82.0\\\\%}\\n84.4↑10.8%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 10.8\\\\%}\\n2070↓56.4%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 56.4\\\\%}\\n0.24↓76.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 76.0\\\\%}\\nTable 4: Performance of HiPO on more models.\\nWe analyze two key dimensions: (i) reasoning-mode activation (<think_on> vs. <think_off>) and (ii) token efficiency across RL training steps and benchmark tasks.\\nSpecifically, during the training and evaluation processes, we track how the model’s decision-making evolves by monitoring the frequency of reasoning-mode activations and the corresponding output length.\\nThink-on vs. Think-off Dynamics During Training and Inference\\nWe logged the frequency of <think_on> and <think_off> activations at each step. As shown in Figure\\xa0LABEL:fig:Training_Ratio, HiPO not only improves final accuracy but also sharpens the model’s gating behavior, allowing it to skip unnecessary reasoning. Specifically, the gap between <think_on> and <think_off> activations decreases from 89.5% at the beginning of training to 53.1% by the end. In Figure LABEL:fig:Testing_Ratio shows the proportion of Think-on activations across different datasets during inference. Reasoning-intensive tasks, including AIME2024, and LiveCodeBench, consistently demonstrate high Think-on activation rates (>70%) throughout training. Conversely, tasks that require less explicit reasoning, such as HumanEval — exhibit a clear downward trend in Think-on activation as training progresses.\\nToken Count Dynamics During Training and Inference\\nDuring RL training, the average token count shows a consistent downward trend in Figure\\xa0LABEL:fig:Training_Token, which indicates that the model gradually learns to produce more concise responses and highlight the HiPO reward design in encouraging efficient token usage\\nBesides, Figure LABEL:fig:Testing_Token shows the corresponding dynamics in average token counts per generated response during inference, and we also observe consistent token reduction in training.\\nGeneralization on More Models\\nIn Table\\xa04, we report the performance of HiPO on Qwen3-1.7B and Qwen3-32B, which shows consistent improvements on both accuracy and efficiency.\\n5 Conclusion\\nIn this work, we introduced HiPO, a hybrid framework for adaptive reasoning in LLMs. By combining a hybrid data pipeline with a hybrid reinforcement learning reward system, HiPO enables models to dynamically balance Think-on and Think-off reasoning, mitigating the issue of overthinking while preserving accuracy. Experiments demonstrate that HiPO achieves competitive or superior accuracy with significantly improved token efficiency and reduced reasoning redundancy.\\nReferences\\nYao et\\xa0al. [2023]\\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas\\xa0L. Griffiths, Yuan Cao, and Karthik Narasimhan.\\nTree of thoughts: Deliberate problem solving with large language models.\\narXiv preprint arXiv: 2305.10601, 2023.\\nWei et\\xa0al. [2023]\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed\\xa0Chi, Quoc Le, and Denny Zhou.\\nChain-of-thought prompting elicits reasoning in large language models, 2023.\\nURL https://arxiv.org/abs/2201.11903.\\nKumar et\\xa0al. [2025]\\nAbhinav Kumar, Jaechul Roh, Ali Naseh, Marzena Karpinska, Mohit Iyyer, Amir Houmansadr, and Eugene Bagdasarian.\\nOverthink: Slowdown attacks on reasoning llms, 2025.\\nURL https://arxiv.org/abs/2502.02542.\\nSui et\\xa0al<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:40:30 [engine.py:317] Added request chatcmpl-1ce5f2a6110d4ef5bb29102da7a86d10.\n",
            "\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from data/output/arxiv_org_15_0_0.txt...vLLM STDOUT: INFO:     127.0.0.1:33966 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:40:32 [logger.py:43] Received request chatcmpl-e1a0a2dc642c4043ab842ee58da112b4: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\n28.9%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 28.9\\\\%}\\n0.82↓18.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 18.0\\\\%}\\n92.7↑5.6%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 5.6\\\\%}\\n824↓80.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 80.8\\\\%}\\n0.18↓82.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 82.0\\\\%}\\n84.4↑10.8%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 10.8\\\\%}\\n2070↓56.4%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 56.4\\\\%}\\n0.24↓76.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 76.0\\\\%}\\nTable 4: Performance of HiPO on more models.\\nWe analyze two key dimensions: (i) reasoning-mode activation (<think_on> vs. <think_off>) and (ii) token efficiency across RL training steps and benchmark tasks.\\nSpecifically, during the training and evaluation processes, we track how the model’s decision-making evolves by monitoring the frequency of reasoning-mode activations and the corresponding output length.\\nThink-on vs. Think-off Dynamics During Training and Inference\\nWe logged the frequency of <think_on> and <think_off> activations at each step. As shown in Figure\\xa0LABEL:fig:Training_Ratio, HiPO not only improves final accuracy but also sharpens the model’s gating behavior, allowing it to skip unnecessary reasoning. Specifically, the gap between <think_on> and <think_off> activations decreases from 89.5% at the beginning of training to 53.1% by the end. In Figure LABEL:fig:Testing_Ratio shows the proportion of Think-on activations across different datasets during inference. Reasoning-intensive tasks, including AIME2024, and LiveCodeBench, consistently demonstrate high Think-on activation rates (>70%) throughout training. Conversely, tasks that require less explicit reasoning, such as HumanEval — exhibit a clear downward trend in Think-on activation as training progresses.\\nToken Count Dynamics During Training and Inference\\nDuring RL training, the average token count shows a consistent downward trend in Figure\\xa0LABEL:fig:Training_Token, which indicates that the model gradually learns to produce more concise responses and highlight the HiPO reward design in encouraging efficient token usage\\nBesides, Figure LABEL:fig:Testing_Token shows the corresponding dynamics in average token counts per generated response during inference, and we also observe consistent token reduction in training.\\nGeneralization on More Models\\nIn Table\\xa04, we report the performance of HiPO on Qwen3-1.7B and Qwen3-32B, which shows consistent improvements on both accuracy and efficiency.\\n5 Conclusion\\nIn this work, we introduced HiPO, a hybrid framework for adaptive reasoning in LLMs. By combining a hybrid data pipeline with a hybrid reinforcement learning reward system, HiPO enables models to dynamically balance Think-on and Think-off reasoning, mitigating the issue of overthinking while preserving accuracy. Experiments demonstrate that HiPO achieves competitive or superior accuracy with significantly improved token efficiency and reduced reasoning redundancy.\\nReferences\\nYao et\\xa0al. [2023]\\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas\\xa0L. Griffiths, Yuan Cao, and Karthik Narasimhan.\\nTree of thoughts: Deliberate problem solving with large language models.\\narXiv preprint arXiv: 2305.10601, 2023.\\nWei et\\xa0al. [2023]\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed\\xa0Chi, Quoc Le, and Denny Zhou.\\nChain-of-thought prompting elicits reasoning in large language models, 2023.\\nURL https://arxiv.org/abs/2201.11903.\\nKumar et\\xa0al. [2025]\\nAbhinav Kumar, Jaechul Roh, Ali Naseh, Marzena Karpinska, Mohit Iyyer, Amir Houmansadr, and Eugene Bagdasarian.\\nOverthink: Slowdown attacks on reasoning llms, 2025.\\nURL https://arxiv.org/abs/2502.02542.\\nSui et\\xa0al<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:40:32 [engine.py:317] Added request chatcmpl-e1a0a2dc642c4043ab842ee58da112b4.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from data/output/arxiv_org_15_0_0.txt...vLLM STDOUT: INFO:     127.0.0.1:33972 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 15 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/arxiv_org_15_0_0_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/arxiv_org_15_0_0_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from data/output/arxiv_org_15_0_0.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/arxiv_org_15_0_0_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:45304 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:45316 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:40:41 [logger.py:43] Received request chatcmpl-37b2b49f9c95484f8b306aabce0a2b13: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n2024.\\ndoi: 10.1109/fllm63129.2024.10852493.\\nURL http://dx.doi.org/10.1109/FLLM63129.2024.10852493.\\nChen et\\xa0al. [2024]\\nQiguang Chen, Libo Qin, Jiaqi Wang, Jinxuan Zhou, and Wanxiang Che.\\nUnlocking the capabilities of thought: A reasoning boundary framework to quantify and optimize chain-of-thought, 2024.\\nURL https://arxiv.org/abs/2410.05695.\\nMunkhbat et\\xa0al. [2025]\\nTergel Munkhbat, Namgyu Ho, Seo\\xa0Hyun Kim, Yongjin Yang, Yujin Kim, and Se-Young Yun.\\nSelf-training elicits concise reasoning in large language models, 2025.\\nURL https://arxiv.org/abs/2502.20122.\\nLiu et\\xa0al. [2024a]\\nAixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et\\xa0al.\\nDeepseek-v3 technical report.\\narXiv preprint arXiv:2412.19437, 2024a.\\nShao et\\xa0al. [2024]\\nZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y.\\xa0K. Li, Y.\\xa0Wu, and Daya Guo.\\nDeepseekmath: Pushing the limits of mathematical reasoning in open language models.\\narXiv preprint arXiv: 2402.03300, 2024.\\nURL https://arxiv.org/abs/2402.03300v3.\\nZheng et\\xa0al. [2025]\\nChujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An\\xa0Yang, Jingren Zhou, and Junyang Lin.\\nGroup sequence policy optimization, 2025.\\nURL https://arxiv.org/abs/2507.18071.\\nYue et\\xa0al. [2025]\\nYu\\xa0Yue, Yufeng Yuan, Qiying Yu, Xiaochen Zuo, Ruofei Zhu, Wenyuan Xu, Jiaze Chen, Chengyi Wang, TianTian Fan, Zhengyin Du, Xiangpeng Wei, Xiangyu Yu, Gaohong Liu, Juncai Liu, Lingjun Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Ru\\xa0Zhang, Xin Liu, Mingxuan Wang, Yonghui Wu, and Lin Yan.\\nVapo: Efficient and reliable reinforcement learning for advanced reasoning tasks, 2025.\\nURL https://arxiv.org/abs/2504.05118.\\nSchulman et\\xa0al. [2017]\\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.\\nProximal policy optimization algorithms, 2017.\\nURL https://arxiv.org/abs/1707.06347.\\nRafailov et\\xa0al. [2024]\\nRafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher\\xa0D. Manning, and Chelsea Finn.\\nDirect preference optimization: Your language model is secretly a reward model, 2024.\\nURL https://arxiv.org/abs/2305.18290.\\nDeepSeek-AI et\\xa0al. [2025]\\nDeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu\\xa0Wu, Z.\\xa0F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:40:41 [engine.py:317] Added request chatcmpl-37b2b49f9c95484f8b306aabce0a2b13.\n",
            "\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from data/output/arxiv_org_18_0.txt...vLLM STDOUT: INFO:     127.0.0.1:45326 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:40:43 [logger.py:43] Received request chatcmpl-d5627d120b654c29ae289266db63771c: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\n2024.\\ndoi: 10.1109/fllm63129.2024.10852493.\\nURL http://dx.doi.org/10.1109/FLLM63129.2024.10852493.\\nChen et\\xa0al. [2024]\\nQiguang Chen, Libo Qin, Jiaqi Wang, Jinxuan Zhou, and Wanxiang Che.\\nUnlocking the capabilities of thought: A reasoning boundary framework to quantify and optimize chain-of-thought, 2024.\\nURL https://arxiv.org/abs/2410.05695.\\nMunkhbat et\\xa0al. [2025]\\nTergel Munkhbat, Namgyu Ho, Seo\\xa0Hyun Kim, Yongjin Yang, Yujin Kim, and Se-Young Yun.\\nSelf-training elicits concise reasoning in large language models, 2025.\\nURL https://arxiv.org/abs/2502.20122.\\nLiu et\\xa0al. [2024a]\\nAixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et\\xa0al.\\nDeepseek-v3 technical report.\\narXiv preprint arXiv:2412.19437, 2024a.\\nShao et\\xa0al. [2024]\\nZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y.\\xa0K. Li, Y.\\xa0Wu, and Daya Guo.\\nDeepseekmath: Pushing the limits of mathematical reasoning in open language models.\\narXiv preprint arXiv: 2402.03300, 2024.\\nURL https://arxiv.org/abs/2402.03300v3.\\nZheng et\\xa0al. [2025]\\nChujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An\\xa0Yang, Jingren Zhou, and Junyang Lin.\\nGroup sequence policy optimization, 2025.\\nURL https://arxiv.org/abs/2507.18071.\\nYue et\\xa0al. [2025]\\nYu\\xa0Yue, Yufeng Yuan, Qiying Yu, Xiaochen Zuo, Ruofei Zhu, Wenyuan Xu, Jiaze Chen, Chengyi Wang, TianTian Fan, Zhengyin Du, Xiangpeng Wei, Xiangyu Yu, Gaohong Liu, Juncai Liu, Lingjun Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Ru\\xa0Zhang, Xin Liu, Mingxuan Wang, Yonghui Wu, and Lin Yan.\\nVapo: Efficient and reliable reinforcement learning for advanced reasoning tasks, 2025.\\nURL https://arxiv.org/abs/2504.05118.\\nSchulman et\\xa0al. [2017]\\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.\\nProximal policy optimization algorithms, 2017.\\nURL https://arxiv.org/abs/1707.06347.\\nRafailov et\\xa0al. [2024]\\nRafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher\\xa0D. Manning, and Chelsea Finn.\\nDirect preference optimization: Your language model is secretly a reward model, 2024.\\nURL https://arxiv.org/abs/2305.18290.\\nDeepSeek-AI et\\xa0al. [2025]\\nDeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu\\xa0Wu, Z.\\xa0F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:40:43 [engine.py:317] Added request chatcmpl-d5627d120b654c29ae289266db63771c.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from data/output/arxiv_org_18_0.txt...vLLM STDOUT: INFO:     127.0.0.1:45330 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 9 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/arxiv_org_18_0_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/arxiv_org_18_0_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from data/output/arxiv_org_18_0.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/arxiv_org_18_0_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:45346 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:45358 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:40:52 [logger.py:43] Received request chatcmpl-a30d341b01434b8da795bb878c719b28: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nHiPO: Hybrid Policy Optimization for Dynamic Reasoning in LLMs\\n1 Introduction\\n2 Related Works\\n3 Method\\n3.1 Hybrid Data Construction Pipeline\\n3.1.1 Data Source\\n3.1.2 Data Collection\\n3.1.3 Data format\\n3.2 Hybrid RL Reward System\\n3.2.1 Basic Reward Formulation\\n3.2.2 Bias Adjustment Mechanism\\n3.2.3 Supervision RL with HiPO\\n3.3 Training Paradigm\\n4 Experiments\\n4.1 Experimental setup\\n4.2 Main Results\\n4.3 Ablation Study\\n4.4 Further Analysis\\nThink-on vs. Think-off Dynamics During Training and Inference\\nToken Count Dynamics During Training and Inference\\nGeneralization on More Models\\n5 Conclusion\\nA Appendix\\nA.1 Use of LLMs\\nA.2 The decline in Qwen3’s performance on the test set.\\nA.3 Data Source\\nA.4 Prompt Templates\\nHiPO: Hybrid Policy Optimization for Dynamic Reasoning in LLMs\\nKen Deng∗,\\nZizheng Zhan∗,\\nWen Xiang∗,\\nWenqiang Zhu∗,\\nTianhao Peng,\\nXinping Lei,\\nWeihao Li,\\nJingxuan Xu,\\nKun Wu,\\nYifan Yao,\\nHaoyang Huang,\\nHuaixi Tang,\\nKepeng Lei,\\nZhiyi Lai,\\nSongwei Yu,\\nZongxian Feng,\\nZuchen Gao,\\nWeihao Xie,\\nChenchen Zhang,\\nYanan Wu,\\nYuanxing Zhang,\\nLecheng Huang,\\nYuqun Zhang,\\nJie Liu,\\nZhaoxiang Zhang,\\nHaotian Zhang,\\nBin Chen,\\nJiaheng Liu†\\nKuaishou Technology, Nanjing University\\ndengken@kuaishou.com, liujiaheng@nju.edu.cn\\n††footnotetext: *\\xa0Equal Contribution. \\xa0\\xa0†\\xa0Corresponding Author.\\nAbstract\\nLarge Language Models (LLMs) increasingly rely on chain-of-thought (CoT) reasoning to improve accuracy on complex tasks. However, always generating lengthy reasoning traces is inefficient, leading to excessive token usage and higher inference costs. This paper introduces the Hybrid Policy Optimization (i.e., HiPO), a framework for adaptive reasoning control that enables LLMs to selectively decide when to engage in detailed reasoning (Think-on) and when to respond directly (Think-off).\\nSpecifically, HiPO combines a hybrid data pipeline—providing paired Think-on and Think-off responses—with a hybrid reinforcement learning reward system that balances accuracy and efficiency while avoiding over-reliance on detailed reasoning.\\nExperiments across mathematics and coding benchmarks demonstrate that HiPO can substantially reduce token length while maintaining or improving accuracy.\\nFinally, we hope HiPO\\xa0111https://huggingface.co/Kwaipilot/HiPO-8B can be a principled approach for efficient adaptive reasoning, advancing the deployment of reasoning-oriented LLMs in real-world, resource-sensitive settings.\\n1 Introduction\\nLarge Language Models (LLMs) have achieved unprecedented success across diverse cognitive tasks, from code generation and mathematical reasoning to scientific problem-solving. A key driver of this progress is the integration of Chain-of-Thought (CoT)\\xa0(Yao et\\xa0al., 2023; Wei et\\xa0al., 2023) reasoning—a paradigm where models decompose complex queries into sequential, interpretable steps to derive accurate outputs.\\nThese approaches enhance accuracy on challenging problems but also introduce a persistent drawback: overthinking\\xa0(Kumar et\\xa0al., 2025; Sui et\\xa0al., 2025; Nayab et\\xa0al., 2025). Even for trivial queries, models often generate unnecessarily long reasoning chains, leading to inflated token usage, higher latency, and reduced efficiency in interactive applications. This inefficiency creates a fundamental tension between reasoning quality and computational cost, raising the need for mechanisms that can adaptively regulate reasoning depth.\\nRecently, recent work has explored adaptive reasoning control to mitigate overthinking,\\nand can be divided into two categories: (i) training-based adaptive reasoning, where reinforcement learning (RL)\\xa0(Aggarwal and Welleck, 2025; Arora and Zanette, 2025; Hou et\\xa0al., 2025; Luo et\\xa0al., 2025; Shen et\\xa0al., 2025;<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:40:52 [engine.py:317] Added request chatcmpl-a30d341b01434b8da795bb878c719b28.\n",
            "\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from data/output/arxiv_org_0.txt...vLLM STDOUT: INFO:     127.0.0.1:45360 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:40:54 [logger.py:43] Received request chatcmpl-9e2841ec628d4005b23d7326717951bb: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nHiPO: Hybrid Policy Optimization for Dynamic Reasoning in LLMs\\n1 Introduction\\n2 Related Works\\n3 Method\\n3.1 Hybrid Data Construction Pipeline\\n3.1.1 Data Source\\n3.1.2 Data Collection\\n3.1.3 Data format\\n3.2 Hybrid RL Reward System\\n3.2.1 Basic Reward Formulation\\n3.2.2 Bias Adjustment Mechanism\\n3.2.3 Supervision RL with HiPO\\n3.3 Training Paradigm\\n4 Experiments\\n4.1 Experimental setup\\n4.2 Main Results\\n4.3 Ablation Study\\n4.4 Further Analysis\\nThink-on vs. Think-off Dynamics During Training and Inference\\nToken Count Dynamics During Training and Inference\\nGeneralization on More Models\\n5 Conclusion\\nA Appendix\\nA.1 Use of LLMs\\nA.2 The decline in Qwen3’s performance on the test set.\\nA.3 Data Source\\nA.4 Prompt Templates\\nHiPO: Hybrid Policy Optimization for Dynamic Reasoning in LLMs\\nKen Deng∗,\\nZizheng Zhan∗,\\nWen Xiang∗,\\nWenqiang Zhu∗,\\nTianhao Peng,\\nXinping Lei,\\nWeihao Li,\\nJingxuan Xu,\\nKun Wu,\\nYifan Yao,\\nHaoyang Huang,\\nHuaixi Tang,\\nKepeng Lei,\\nZhiyi Lai,\\nSongwei Yu,\\nZongxian Feng,\\nZuchen Gao,\\nWeihao Xie,\\nChenchen Zhang,\\nYanan Wu,\\nYuanxing Zhang,\\nLecheng Huang,\\nYuqun Zhang,\\nJie Liu,\\nZhaoxiang Zhang,\\nHaotian Zhang,\\nBin Chen,\\nJiaheng Liu†\\nKuaishou Technology, Nanjing University\\ndengken@kuaishou.com, liujiaheng@nju.edu.cn\\n††footnotetext: *\\xa0Equal Contribution. \\xa0\\xa0†\\xa0Corresponding Author.\\nAbstract\\nLarge Language Models (LLMs) increasingly rely on chain-of-thought (CoT) reasoning to improve accuracy on complex tasks. However, always generating lengthy reasoning traces is inefficient, leading to excessive token usage and higher inference costs. This paper introduces the Hybrid Policy Optimization (i.e., HiPO), a framework for adaptive reasoning control that enables LLMs to selectively decide when to engage in detailed reasoning (Think-on) and when to respond directly (Think-off).\\nSpecifically, HiPO combines a hybrid data pipeline—providing paired Think-on and Think-off responses—with a hybrid reinforcement learning reward system that balances accuracy and efficiency while avoiding over-reliance on detailed reasoning.\\nExperiments across mathematics and coding benchmarks demonstrate that HiPO can substantially reduce token length while maintaining or improving accuracy.\\nFinally, we hope HiPO\\xa0111https://huggingface.co/Kwaipilot/HiPO-8B can be a principled approach for efficient adaptive reasoning, advancing the deployment of reasoning-oriented LLMs in real-world, resource-sensitive settings.\\n1 Introduction\\nLarge Language Models (LLMs) have achieved unprecedented success across diverse cognitive tasks, from code generation and mathematical reasoning to scientific problem-solving. A key driver of this progress is the integration of Chain-of-Thought (CoT)\\xa0(Yao et\\xa0al., 2023; Wei et\\xa0al., 2023) reasoning—a paradigm where models decompose complex queries into sequential, interpretable steps to derive accurate outputs.\\nThese approaches enhance accuracy on challenging problems but also introduce a persistent drawback: overthinking\\xa0(Kumar et\\xa0al., 2025; Sui et\\xa0al., 2025; Nayab et\\xa0al., 2025). Even for trivial queries, models often generate unnecessarily long reasoning chains, leading to inflated token usage, higher latency, and reduced efficiency in interactive applications. This inefficiency creates a fundamental tension between reasoning quality and computational cost, raising the need for mechanisms that can adaptively regulate reasoning depth.\\nRecently, recent work has explored adaptive reasoning control to mitigate overthinking,\\nand can be divided into two categories: (i) training-based adaptive reasoning, where reinforcement learning (RL)\\xa0(Aggarwal and Welleck, 2025; Arora and Zanette, 2025; Hou et\\xa0al., 2025; Luo et\\xa0al., 2025; Shen et\\xa0al., 2025;<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[32m⠋\u001b[0m Generating qa content from data/output/arxiv_org_0.txt...vLLM STDOUT: INFO 10-03 08:40:54 [engine.py:317] Added request chatcmpl-9e2841ec628d4005b23d7326717951bb.\n",
            "\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from data/output/arxiv_org_0.txt...vLLM STDOUT: INFO:     127.0.0.1:49382 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 11 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/arxiv_org_0_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/arxiv_org_0_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from data/output/arxiv_org_0.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/arxiv_org_0_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:39028 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:39044 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:41:03 [logger.py:43] Received request chatcmpl-287dae0568be4053ab1c25e8bf289daa: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n) training-based adaptive reasoning, where reinforcement learning (RL)\\xa0(Aggarwal and Welleck, 2025; Arora and Zanette, 2025; Hou et\\xa0al., 2025; Luo et\\xa0al., 2025; Shen et\\xa0al., 2025; Team et\\xa0al., 2025; Lou et\\xa0al., 2025) or supervised fine-tuning (SFT)\\xa0(Munkhdalai et\\xa0al., 2024; Ma et\\xa0al., 2025; Chen et\\xa0al., 2025a; Kang et\\xa0al., 2025) encourages concise reasoning through length penalties or conciseness rewards; (ii) external control, which constrains reasoning with handcrafted prompts or dynamic instructions\\xa0(Xu et\\xa0al., 2025; Renze and Guven, 2024; Chen et\\xa0al., 2024; Munkhbat et\\xa0al., 2025). While effective to some extent, these methods suffer from important limitations: coarse supervision signals, monotonic incentives that discourage deeper reasoning on difficult problems, and a lack of principled trade-offs between accuracy, latency, and token efficiency.\\nTo address these challenges, we introduce HiPO (Hybrid Policy Optimization), a unified framework for adaptive reasoning in LLMs. HiPO is designed to enable models to decide when to “think” (i.e., Think-on)and when to skip reasoning (i.e., Think-off), thereby striking a balance between correctness and efficiency. Specifically, our approach builds on two key innovations:\\n(1) Hybrid Data Construction Pipeline. As shown in Figure\\xa01, we first collect the training data containing both Think-on and Think-off responses. Each query is automatically categorized based on its difficulty and response correctness. Then, a high-performance model (i.e., DeepSeek-V3\\xa0(Liu et\\xa0al., 2024a)) is used to produce the explicit explanations to justify its reasoning-mode decisions. Finally, for each query, the final response based on the thinking mode and the corresponding explanation construct the hybrid output.\\n(2) Hybrid Reinforcement Learning Reward System. We propose a hybrid reward design that balances Think-on and Think-off decisions. Specifically, a bias adjustment mechanism prevents the model from over-relying on verbose reasoning, while mode-aware advantage functions align reasoning-mode selection with actual performance gains. This ensures stable training and principled control over reasoning depth.\\nIn summary, our contributions are threefold:\\n•\\nWe propose HiPO for adaptive LLM reasoning, which mainly includes the hybrid data construction and hybrid reinforcement learning.\\n•\\nIn the hybrid data construction pipeline, we produce logically rich Think-on and concise Think-off responses with the justification for the thinking mode. Then, for hybrid reinforcement learning, we introduce both the judge analysis and the response reward signal to enable principled control of reasoning depth.\\n•\\nExperimental results on multiple datasets demonstrate that HiPO can consistently reduce redundant reasoning while improving or maintaining accuracy.\\nFigure 1: Framework of the hybrid data construction pipeline.\\n2 Related Works\\nRL for LLM Reasoning.\\nRecent advances in reinforcement learning (RL) have significantly enhanced LLMs’ complex reasoning capabilities, moving beyond supervised fine-tuning (SFT) limitations. State-of-the-art RL algorithms demonstrate superior performance in mathematical reasoning and multi-step problem solving: GRPO \\xa0(Shao et\\xa0al., 2024) stabilizes training through intra-group relative reward comparisons; GSPO \\xa0(Zheng et\\xa0al., 2025) defines sequence-level importance ratios and applies sequence-level clipping/rewarding/updates to improve efficiency and stabilize MoE training; VAPO \\xa0(Yue et\\xa0al., 2025) ensures reward consistency via value-aware optimization; PPO \\xa0(Schulman et\\xa0al., 2017) constrains policy updates through clipping mechanisms; and DPO \\xa0(Rafailov et\\xa0al., 2024) learns directly from human preferences without explicit reward modeling.\\nAdaptive Reasoning.\\nReasoning-oriented large language models—exemplified by Chain-of-Thought (CoT) \\xa0(Yao et\\xa0al., 2023; Wei et\\xa0al., 2023) and R1-style \\xa0(DeepSeek-AI et\\xa0al., 2025) systems—have improved complex problem solving via explicit step-by-step reasoning and self-reflection but also suffer from “overthinking”<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:41:03 [engine.py:317] Added request chatcmpl-287dae0568be4053ab1c25e8bf289daa.\n",
            "\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from data/output/arxiv_org_1.txt...vLLM STDOUT: INFO:     127.0.0.1:39048 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:41:05 [logger.py:43] Received request chatcmpl-8d4ae99efd46450e859d977d80a58613: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\n) training-based adaptive reasoning, where reinforcement learning (RL)\\xa0(Aggarwal and Welleck, 2025; Arora and Zanette, 2025; Hou et\\xa0al., 2025; Luo et\\xa0al., 2025; Shen et\\xa0al., 2025; Team et\\xa0al., 2025; Lou et\\xa0al., 2025) or supervised fine-tuning (SFT)\\xa0(Munkhdalai et\\xa0al., 2024; Ma et\\xa0al., 2025; Chen et\\xa0al., 2025a; Kang et\\xa0al., 2025) encourages concise reasoning through length penalties or conciseness rewards; (ii) external control, which constrains reasoning with handcrafted prompts or dynamic instructions\\xa0(Xu et\\xa0al., 2025; Renze and Guven, 2024; Chen et\\xa0al., 2024; Munkhbat et\\xa0al., 2025). While effective to some extent, these methods suffer from important limitations: coarse supervision signals, monotonic incentives that discourage deeper reasoning on difficult problems, and a lack of principled trade-offs between accuracy, latency, and token efficiency.\\nTo address these challenges, we introduce HiPO (Hybrid Policy Optimization), a unified framework for adaptive reasoning in LLMs. HiPO is designed to enable models to decide when to “think” (i.e., Think-on)and when to skip reasoning (i.e., Think-off), thereby striking a balance between correctness and efficiency. Specifically, our approach builds on two key innovations:\\n(1) Hybrid Data Construction Pipeline. As shown in Figure\\xa01, we first collect the training data containing both Think-on and Think-off responses. Each query is automatically categorized based on its difficulty and response correctness. Then, a high-performance model (i.e., DeepSeek-V3\\xa0(Liu et\\xa0al., 2024a)) is used to produce the explicit explanations to justify its reasoning-mode decisions. Finally, for each query, the final response based on the thinking mode and the corresponding explanation construct the hybrid output.\\n(2) Hybrid Reinforcement Learning Reward System. We propose a hybrid reward design that balances Think-on and Think-off decisions. Specifically, a bias adjustment mechanism prevents the model from over-relying on verbose reasoning, while mode-aware advantage functions align reasoning-mode selection with actual performance gains. This ensures stable training and principled control over reasoning depth.\\nIn summary, our contributions are threefold:\\n•\\nWe propose HiPO for adaptive LLM reasoning, which mainly includes the hybrid data construction and hybrid reinforcement learning.\\n•\\nIn the hybrid data construction pipeline, we produce logically rich Think-on and concise Think-off responses with the justification for the thinking mode. Then, for hybrid reinforcement learning, we introduce both the judge analysis and the response reward signal to enable principled control of reasoning depth.\\n•\\nExperimental results on multiple datasets demonstrate that HiPO can consistently reduce redundant reasoning while improving or maintaining accuracy.\\nFigure 1: Framework of the hybrid data construction pipeline.\\n2 Related Works\\nRL for LLM Reasoning.\\nRecent advances in reinforcement learning (RL) have significantly enhanced LLMs’ complex reasoning capabilities, moving beyond supervised fine-tuning (SFT) limitations. State-of-the-art RL algorithms demonstrate superior performance in mathematical reasoning and multi-step problem solving: GRPO \\xa0(Shao et\\xa0al., 2024) stabilizes training through intra-group relative reward comparisons; GSPO \\xa0(Zheng et\\xa0al., 2025) defines sequence-level importance ratios and applies sequence-level clipping/rewarding/updates to improve efficiency and stabilize MoE training; VAPO \\xa0(Yue et\\xa0al., 2025) ensures reward consistency via value-aware optimization; PPO \\xa0(Schulman et\\xa0al., 2017) constrains policy updates through clipping mechanisms; and DPO \\xa0(Rafailov et\\xa0al., 2024) learns directly from human preferences without explicit reward modeling.\\nAdaptive Reasoning.\\nReasoning-oriented large language models—exemplified by Chain-of-Thought (CoT) \\xa0(Yao et\\xa0al., 2023; Wei et\\xa0al., 2023) and R1-style \\xa0(DeepSeek-AI et\\xa0al., 2025) systems—have improved complex problem solving via explicit step-by-step reasoning and self-reflection but also suffer from “overthinking”<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:41:05 [engine.py:317] Added request chatcmpl-8d4ae99efd46450e859d977d80a58613.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from data/output/arxiv_org_1.txt...vLLM STDOUT: INFO:     127.0.0.1:39060 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 11 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/arxiv_org_1_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/arxiv_org_1_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from data/output/arxiv_org_1.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/arxiv_org_1_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:40888 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:40896 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:41:14 [logger.py:43] Received request chatcmpl-575418c7831f4a3bb5a449ec5be3e39c: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nao et\\xa0al., 2023; Wei et\\xa0al., 2023) and R1-style \\xa0(DeepSeek-AI et\\xa0al., 2025) systems—have improved complex problem solving via explicit step-by-step reasoning and self-reflection but also suffer from “overthinking” \\xa0(Kumar et\\xa0al., 2025; Sui et\\xa0al., 2025; Nayab et\\xa0al., 2025), where simple queries trigger redundant chains that inflate compute, latency, and token usage, hindering interactive deployment. To address this, existing work focuses on: (i) Training-based adaptive reasoning: RL to conditionally trigger CoT, length penalties and conciseness rewards\\xa0(Aggarwal and Welleck, 2025; Arora and Zanette, 2025; Hou et\\xa0al., 2025; Luo et\\xa0al., 2025; Shen et\\xa0al., 2025; Team et\\xa0al., 2025; Lou et\\xa0al., 2025; Zhan et\\xa0al., 2025), and SFT\\xa0(Munkhdalai et\\xa0al., 2024; Ma et\\xa0al., 2025; Chen et\\xa0al., 2025a; Kang et\\xa0al., 2025) to prefer shorter yet correct reasoning; (ii) External control : prompt or instruction designs that limit steps or defer CoT\\xa0(Xu et\\xa0al., 2025; Renze and Guven, 2024; Chen et\\xa0al., 2024; Munkhbat et\\xa0al., 2025); (iii) Post-hoc Efficiency Optimization: pruning and restructuring chains after generation\\xa0(Aytes et\\xa0al., 2025; Xia et\\xa0al., 2025; Liu et\\xa0al., 2024b; Sun et\\xa0al., 2024; Yang et\\xa0al., 2025).\\nDespite progress, these methods still face coarse supervision, limited adaptation to hard cases due to monotonic shortening, and a lack of principled trade-offs between quality, token cost, and latency.\\n3 Method\\nOur HiPO framework consists of two important components: (i) a hybrid data construction pipeline that generates training data with both Think-on and Think-off responses; (ii) a hybrid reinforcement learning reward system that combines mode-specific accuracy and global average performance, along with a bias-adjustment mechanism to prevent over-reliance on the Think-on mode.\\n3.1 Hybrid Data Construction Pipeline\\nThis process begins with a novel data labeling system leveraging state-of-the-art LLMs to assess each query’s difficulty and domain characteristics. Queries are then classified into Think-on and Think-off categories based on their intrinsic complexity and the availability of verifiable answers.\\n3.1.1 Data Source\\nFigure 2:\\nStatistics of Data Sources.\\nWe construct a challenging corpus for code and mathematics by integrating diverse public and proprietary sources, as illustrated in Fig.\\xa02, including AM-Thinking-v1-Distilled\\xa0(Tian et\\xa0al., 2025), II-Thought-RL\\xa0(Internet, 2025), AceReason-Math\\xa0(Chen et\\xa0al., 2025b), and Skywork-OR1-RL-Data\\xa0(He et\\xa0al., 2025).\\n3.1.2 Data Collection\\nTo effectively enhance the performance of HiPO, we design a structured data construction pipeline aimed at exploring and guiding the model’s preference between the Think-on and Think-off reasoning modes. Our training dataset is meticulously curated to be logically rich, cross-domain, and sufficiently challenging.\\nWe adopt a multi-stage data generation process as shown in Figure\\xa01.\\nFor each query, the pipeline samples NN responses under the Think-on mode and NN responses under the Think-off mode using a dedicated reasoning model.\\nAll responses are then verified for correctness, and the reasoning mode with the higher pass rate is selected as the preferred mode for that query. Let ponp_{\\\\text{on}} and poffp_{\\\\text{off}} denote the pass rates of the Think-on and Think-off modes, respectively. If the difference in pass rates satisfies |pon−poff|<δ|p_{\\\\text{on}}-p_{\\\\text{off}}|<\\\\delta, where δ\\\\delta is a predefined threshold, the Think-off mode is selected. This tie-breaking strategy encourages the model to<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:41:14 [engine.py:317] Added request chatcmpl-575418c7831f4a3bb5a449ec5be3e39c.\n",
            "\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from data/output/arxiv_org_2.txt...vLLM STDOUT: INFO:     127.0.0.1:40908 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:41:16 [logger.py:43] Received request chatcmpl-a52e28796c2642f3b9cf6eb9984acec8: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nao et\\xa0al., 2023; Wei et\\xa0al., 2023) and R1-style \\xa0(DeepSeek-AI et\\xa0al., 2025) systems—have improved complex problem solving via explicit step-by-step reasoning and self-reflection but also suffer from “overthinking” \\xa0(Kumar et\\xa0al., 2025; Sui et\\xa0al., 2025; Nayab et\\xa0al., 2025), where simple queries trigger redundant chains that inflate compute, latency, and token usage, hindering interactive deployment. To address this, existing work focuses on: (i) Training-based adaptive reasoning: RL to conditionally trigger CoT, length penalties and conciseness rewards\\xa0(Aggarwal and Welleck, 2025; Arora and Zanette, 2025; Hou et\\xa0al., 2025; Luo et\\xa0al., 2025; Shen et\\xa0al., 2025; Team et\\xa0al., 2025; Lou et\\xa0al., 2025; Zhan et\\xa0al., 2025), and SFT\\xa0(Munkhdalai et\\xa0al., 2024; Ma et\\xa0al., 2025; Chen et\\xa0al., 2025a; Kang et\\xa0al., 2025) to prefer shorter yet correct reasoning; (ii) External control : prompt or instruction designs that limit steps or defer CoT\\xa0(Xu et\\xa0al., 2025; Renze and Guven, 2024; Chen et\\xa0al., 2024; Munkhbat et\\xa0al., 2025); (iii) Post-hoc Efficiency Optimization: pruning and restructuring chains after generation\\xa0(Aytes et\\xa0al., 2025; Xia et\\xa0al., 2025; Liu et\\xa0al., 2024b; Sun et\\xa0al., 2024; Yang et\\xa0al., 2025).\\nDespite progress, these methods still face coarse supervision, limited adaptation to hard cases due to monotonic shortening, and a lack of principled trade-offs between quality, token cost, and latency.\\n3 Method\\nOur HiPO framework consists of two important components: (i) a hybrid data construction pipeline that generates training data with both Think-on and Think-off responses; (ii) a hybrid reinforcement learning reward system that combines mode-specific accuracy and global average performance, along with a bias-adjustment mechanism to prevent over-reliance on the Think-on mode.\\n3.1 Hybrid Data Construction Pipeline\\nThis process begins with a novel data labeling system leveraging state-of-the-art LLMs to assess each query’s difficulty and domain characteristics. Queries are then classified into Think-on and Think-off categories based on their intrinsic complexity and the availability of verifiable answers.\\n3.1.1 Data Source\\nFigure 2:\\nStatistics of Data Sources.\\nWe construct a challenging corpus for code and mathematics by integrating diverse public and proprietary sources, as illustrated in Fig.\\xa02, including AM-Thinking-v1-Distilled\\xa0(Tian et\\xa0al., 2025), II-Thought-RL\\xa0(Internet, 2025), AceReason-Math\\xa0(Chen et\\xa0al., 2025b), and Skywork-OR1-RL-Data\\xa0(He et\\xa0al., 2025).\\n3.1.2 Data Collection\\nTo effectively enhance the performance of HiPO, we design a structured data construction pipeline aimed at exploring and guiding the model’s preference between the Think-on and Think-off reasoning modes. Our training dataset is meticulously curated to be logically rich, cross-domain, and sufficiently challenging.\\nWe adopt a multi-stage data generation process as shown in Figure\\xa01.\\nFor each query, the pipeline samples NN responses under the Think-on mode and NN responses under the Think-off mode using a dedicated reasoning model.\\nAll responses are then verified for correctness, and the reasoning mode with the higher pass rate is selected as the preferred mode for that query. Let ponp_{\\\\text{on}} and poffp_{\\\\text{off}} denote the pass rates of the Think-on and Think-off modes, respectively. If the difference in pass rates satisfies |pon−poff|<δ|p_{\\\\text{on}}-p_{\\\\text{off}}|<\\\\delta, where δ\\\\delta is a predefined threshold, the Think-off mode is selected. This tie-breaking strategy encourages the model to<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:41:16 [engine.py:317] Added request chatcmpl-a52e28796c2642f3b9cf6eb9984acec8.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from data/output/arxiv_org_2.txt...vLLM STDOUT: INFO:     127.0.0.1:40914 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 11 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/arxiv_org_2_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/arxiv_org_2_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from data/output/arxiv_org_2.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/arxiv_org_2_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:58742 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:58758 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:41:25 [logger.py:43] Received request chatcmpl-8c6cd5fe30a54cbabe371366144ecaef: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nmodes, respectively. If the difference in pass rates satisfies |pon−poff|<δ|p_{\\\\text{on}}-p_{\\\\text{off}}|<\\\\delta, where δ\\\\delta is a predefined threshold, the Think-off mode is selected. This tie-breaking strategy encourages the model to prefer more concise responses when deeper reasoning does not lead to a significant improvement in correctness.\\nFor the winning mode,\\nthe shortest correct response is retained as the final sample.\\nTo expose the model to diverse reasoning scenarios and encourage adaptive behavior, we randomly assign a mode to 1% of the queries, forcing the model to encounter diverse reasoning scenarios. This forces the model to engage with both reasoning styles in varying contexts, which is essential for learning when to switch modes dynamically during inference.\\nAdditionally, we incorporate an auxiliary explanation signal to enhance the model’s mode alignment capabilities. For each query-response pair, we prompt DeepSeek-V3\\xa0(Liu et\\xa0al., 2024a) to generate a justification explaining why the selected mode is appropriate. This explanation provides a valuable training signal for aligning mode decisions with the underlying reasoning complexity.\\n3.1.3 Data format\\nThe training samples follow a unified structure encompassing justification and answer generation. As shown in Table\\xa01, this design guides the model to decide when reasoning is needed and to generate answers consistent with it. The special tokens are detailed in Table\\xa01, ensuring a clear separation between reasoning and final response for better alignment.\\nTable 1: Formatting templates (left) and special tokens with their descriptions (right).\\nThink-on Mode\\nThink-off Mode\\n<judge>\\n<judge>\\n{judge_analysis}\\n{judge_analysis}\\n</judge>\\n</judge>\\n<think_on>\\n<think_off>\\n<think>\\n<answer>\\n{thinking_content}\\n{response}\\n</think>\\n</answer>\\n<answer>\\n{response}\\n</answer>\\nSpecial Token\\nDescription\\n<judge>\\nAnalyzes input query to determine whether reasoning is required.\\n<think_on/off>\\nSpecifies whether reasoning should be activated (\"on\") or skipped (\"off\").\\n<think>\\nMarks the beginning of reasoning in Think-on mode.\\n<answer>\\nMarks the beginning of the model’s answer.\\n3.2 Hybrid RL Reward System\\nThis section details the reinforcement learning process used to teach the model how to effectively balance Think-on and Think-off reasoning modes. The approach is built on a hybrid RL reward system that guides the model’s optimization.\\n3.2.1 Basic Reward Formulation\\nConsider a group of NN sampled responses, for each response i∈{1,…,N}i\\\\in\\\\{1,\\\\dots,N\\\\}, we denote its answer correctness by ACCi∈{0,1}\\\\mathrm{ACC}_{i}\\\\in\\\\{0,1\\\\}, its format correctness by FORMATi∈{0,1}\\\\mathrm{FORMAT}_{i}\\\\in\\\\{0,1\\\\}, its basic reward by ri\\u200b=\\u200bACCi+0.2⋅FORMATi∈ℝr_{i}\\\\text{=}\\\\mathrm{ACC}_{i}+0.2\\\\cdot\\\\mathrm{FORMAT}_{i}\\\\in\\\\mathbb{R}, and its reasoning mode by Mi∈{on,off}M_{i}\\\\in\\\\{\\\\text{on},\\\\text{off}\\\\}, where Mi\\u200b=onM_{i}\\\\text{=}\\\\text{on} indicates the Think-on mode and Mi\\u200b=offM_{i}\\\\text{=}\\\\text{off} indicates the Think-off mode.\\n3.2.2 Bias Adjustment Mechanism\\nA potential risk of the hybrid reward design is that the model may overfit to the more accurate Think-on mode, favoring deep reasoning even when it is unnecessary. This tendency can reduce response efficiency and hinder the intended flexibility in reasoning behavior.\\nTo mitigate this issue, we introduce a bias adjustment mechanism that dynamically regularizes the contribution of mode-specific accuracies.\\nLet mean\\u200b(𝐫on)\\u200b=\\u200b1Non\\u200b∑i:Mi=onri\\\\text{mean}(\\\\mathbf{r}_{\\\\text{on}})\\\\text{=}\\\\frac{1}{N_{\\\\text{on}}}\\\\sum_{i:M_{i}=\\\\text{on}}r_{i} denote the average reward of responses generated under the Think-on mode, and let mean\\u200b(𝐫off)\\\\text{mean}(\\\\mathbf{r}_{\\\\<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:41:25 [engine.py:317] Added request chatcmpl-8c6cd5fe30a54cbabe371366144ecaef.\n",
            "\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from data/output/arxiv_org_3.txt...vLLM STDOUT: INFO:     127.0.0.1:58760 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:41:28 [logger.py:43] Received request chatcmpl-8e1ce4e0d37d45449368abe2fac6d7bc: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\n modes, respectively. If the difference in pass rates satisfies |pon−poff|<δ|p_{\\\\text{on}}-p_{\\\\text{off}}|<\\\\delta, where δ\\\\delta is a predefined threshold, the Think-off mode is selected. This tie-breaking strategy encourages the model to prefer more concise responses when deeper reasoning does not lead to a significant improvement in correctness.\\nFor the winning mode,\\nthe shortest correct response is retained as the final sample.\\nTo expose the model to diverse reasoning scenarios and encourage adaptive behavior, we randomly assign a mode to 1% of the queries, forcing the model to encounter diverse reasoning scenarios. This forces the model to engage with both reasoning styles in varying contexts, which is essential for learning when to switch modes dynamically during inference.\\nAdditionally, we incorporate an auxiliary explanation signal to enhance the model’s mode alignment capabilities. For each query-response pair, we prompt DeepSeek-V3\\xa0(Liu et\\xa0al., 2024a) to generate a justification explaining why the selected mode is appropriate. This explanation provides a valuable training signal for aligning mode decisions with the underlying reasoning complexity.\\n3.1.3 Data format\\nThe training samples follow a unified structure encompassing justification and answer generation. As shown in Table\\xa01, this design guides the model to decide when reasoning is needed and to generate answers consistent with it. The special tokens are detailed in Table\\xa01, ensuring a clear separation between reasoning and final response for better alignment.\\nTable 1: Formatting templates (left) and special tokens with their descriptions (right).\\nThink-on Mode\\nThink-off Mode\\n<judge>\\n<judge>\\n{judge_analysis}\\n{judge_analysis}\\n</judge>\\n</judge>\\n<think_on>\\n<think_off>\\n<think>\\n<answer>\\n{thinking_content}\\n{response}\\n</think>\\n</answer>\\n<answer>\\n{response}\\n</answer>\\nSpecial Token\\nDescription\\n<judge>\\nAnalyzes input query to determine whether reasoning is required.\\n<think_on/off>\\nSpecifies whether reasoning should be activated (\"on\") or skipped (\"off\").\\n<think>\\nMarks the beginning of reasoning in Think-on mode.\\n<answer>\\nMarks the beginning of the model’s answer.\\n3.2 Hybrid RL Reward System\\nThis section details the reinforcement learning process used to teach the model how to effectively balance Think-on and Think-off reasoning modes. The approach is built on a hybrid RL reward system that guides the model’s optimization.\\n3.2.1 Basic Reward Formulation\\nConsider a group of NN sampled responses, for each response i∈{1,…,N}i\\\\in\\\\{1,\\\\dots,N\\\\}, we denote its answer correctness by ACCi∈{0,1}\\\\mathrm{ACC}_{i}\\\\in\\\\{0,1\\\\}, its format correctness by FORMATi∈{0,1}\\\\mathrm{FORMAT}_{i}\\\\in\\\\{0,1\\\\}, its basic reward by ri\\u200b=\\u200bACCi+0.2⋅FORMATi∈ℝr_{i}\\\\text{=}\\\\mathrm{ACC}_{i}+0.2\\\\cdot\\\\mathrm{FORMAT}_{i}\\\\in\\\\mathbb{R}, and its reasoning mode by Mi∈{on,off}M_{i}\\\\in\\\\{\\\\text{on},\\\\text{off}\\\\}, where Mi\\u200b=onM_{i}\\\\text{=}\\\\text{on} indicates the Think-on mode and Mi\\u200b=offM_{i}\\\\text{=}\\\\text{off} indicates the Think-off mode.\\n3.2.2 Bias Adjustment Mechanism\\nA potential risk of the hybrid reward design is that the model may overfit to the more accurate Think-on mode, favoring deep reasoning even when it is unnecessary. This tendency can reduce response efficiency and hinder the intended flexibility in reasoning behavior.\\nTo mitigate this issue, we introduce a bias adjustment mechanism that dynamically regularizes the contribution of mode-specific accuracies.\\nLet mean\\u200b(𝐫on)\\u200b=\\u200b1Non\\u200b∑i:Mi=onri\\\\text{mean}(\\\\mathbf{r}_{\\\\text{on}})\\\\text{=}\\\\frac{1}{N_{\\\\text{on}}}\\\\sum_{i:M_{i}=\\\\text{on}}r_{i} denote the average reward of responses generated under the Think-on mode, and let mean\\u200b(𝐫off)\\\\text{mean}(\\\\mathbf{r}_{\\\\<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:41:28 [engine.py:317] Added request chatcmpl-8e1ce4e0d37d45449368abe2fac6d7bc.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from data/output/arxiv_org_3.txt...vLLM STDOUT: INFO:     127.0.0.1:58774 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 11 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/arxiv_org_3_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/arxiv_org_3_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from data/output/arxiv_org_3.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/arxiv_org_3_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:56838 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:56852 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:41:37 [logger.py:43] Received request chatcmpl-804f70a40c374491aee61e7661c900ac: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n=}\\\\frac{1}{N_{\\\\text{on}}}\\\\sum_{i:M_{i}=\\\\text{on}}r_{i} denote the average reward of responses generated under the Think-on mode, and let mean\\u200b(𝐫off)\\\\text{mean}(\\\\mathbf{r}_{\\\\text{off}}) denote the corresponding average reward for the Think-off mode.\\nBased on this, we define a bias term for the Think-off mode as a fraction of the Think-on average reward:\\nbiasoff\\u200b=\\u200bω⋅mean\\u200b(𝐫on)\\\\mathrm{bias}_{\\\\text{off}}\\\\text{=}\\\\omega\\\\cdot\\\\text{mean}(\\\\mathbf{r}_{\\\\text{on}}), where ω\\\\omega controls the ratio.\\nThe adjustment is applied only when the performance of the Think-off mode does not exceed that of the Think-on mode, but the difference between the two remains within the bias threshold. Formally, the adjustment mechanism is as follows:\\nmean\\u200b(𝐫off)={mean\\u200b(𝐫off)+biasoff,0≤mean\\u200b(𝐫on)−mean\\u200b(𝐫off)≤biasoff,mean\\u200b(𝐫off),otherwise.\\\\text{mean}(\\\\mathbf{r}_{\\\\text{off}})=\\\\begin{cases}\\\\text{mean}(\\\\mathbf{r}_{\\\\text{off}})+\\\\mathrm{bias}_{\\\\text{off}},&0\\\\leq\\\\text{mean}(\\\\mathbf{r}_{\\\\text{on}})-\\\\text{mean}(\\\\mathbf{r}_{\\\\text{off}})\\\\leq\\\\mathrm{bias}_{\\\\text{off}},\\\\\\\\[6.0pt]\\n\\\\text{mean}(\\\\mathbf{r}_{\\\\text{off}}),&\\\\text{otherwise}.\\\\end{cases}\\n(1)\\nThis mechanism prevents the model from gaining an unfair advantage by overfitting to the more verbose but more accurate Think-on mode. Moreover, it ensures that the adjusted accuracies remain faithful to the true relative performance between reasoning modes, thereby improving training stability and preserving the intended balance between depth and efficiency.\\n3.2.3 Supervision RL with HiPO\\nThe final advantage function is formulated as a hybrid signal that integrates both judge analysis and model response. Each response ii receives two distinct scalar advantage, including judge advantage based on the quality of the mode justification, and answer advantage based on correctness and format.\\nThe judge advantage Aijudge\\\\mathrm{A}^{\\\\text{judge}}_{i} captures the broader decision-level utility of selecting a particular mode. The first term, mean\\u200b(𝐫Mi)−mean\\u200b(𝐫)\\\\text{mean}(\\\\mathbf{r}_{M_{i}})-\\\\text{mean}(\\\\mathbf{r}), quantifies the global advantage of the chosen mode over the full group average, guiding the model toward choosing modes that lead to higher expected rewards. The second term, γ⋅(ri−mean\\u200b(𝐫))\\\\gamma\\\\cdot(r_{i}-\\\\text{mean}(\\\\mathbf{r})), ensures that the justification content is also responsible for the quality of the response under that mode, thereby aligning the explanation with actual performance. The use of a global normalization factor std\\u200b(𝐫)\\\\text{std}(\\\\mathbf{r}) stabilizes the reward signal across groups.\\nThe judge advantage function for response ii is then given by:\\nAijudge={(mean\\u200b(𝐫on)−mean\\u200b(𝐫))+γ⋅(ri−mean\\u200b(𝐫))std\\u200b(𝐫),if\\xa0\\u200bMi=on,(mean\\u200b(𝐫off)−mean\\u200b(𝐫))+γ⋅(ri−mean\\u200b(𝐫))std\\u200b(𝐫),if\\xa0\\u200bMi=off.\\\\mathrm{A}^{\\\\text{judge}}_{i}=\\\\begin{cases}\\\\frac{(\\\\text{mean}(\\\\mathbf{r}_{\\\\text{on}})-\\\\text{mean}(\\\\mathbf{r}))+\\\\gamma\\\\cdot(r_{i}-\\\\text{mean}(\\\\mathbf{r}))}{\\\\text{std}(\\\\mathbf{r})},&\\\\text{if }<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:41:37 [engine.py:317] Added request chatcmpl-804f70a40c374491aee61e7661c900ac.\n",
            "\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from data/output/arxiv_org_4.txt...vLLM STDOUT: INFO:     127.0.0.1:56868 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:41:39 [logger.py:43] Received request chatcmpl-1376377155694131a27b52f0bef532a0: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\n=}\\\\frac{1}{N_{\\\\text{on}}}\\\\sum_{i:M_{i}=\\\\text{on}}r_{i} denote the average reward of responses generated under the Think-on mode, and let mean\\u200b(𝐫off)\\\\text{mean}(\\\\mathbf{r}_{\\\\text{off}}) denote the corresponding average reward for the Think-off mode.\\nBased on this, we define a bias term for the Think-off mode as a fraction of the Think-on average reward:\\nbiasoff\\u200b=\\u200bω⋅mean\\u200b(𝐫on)\\\\mathrm{bias}_{\\\\text{off}}\\\\text{=}\\\\omega\\\\cdot\\\\text{mean}(\\\\mathbf{r}_{\\\\text{on}}), where ω\\\\omega controls the ratio.\\nThe adjustment is applied only when the performance of the Think-off mode does not exceed that of the Think-on mode, but the difference between the two remains within the bias threshold. Formally, the adjustment mechanism is as follows:\\nmean\\u200b(𝐫off)={mean\\u200b(𝐫off)+biasoff,0≤mean\\u200b(𝐫on)−mean\\u200b(𝐫off)≤biasoff,mean\\u200b(𝐫off),otherwise.\\\\text{mean}(\\\\mathbf{r}_{\\\\text{off}})=\\\\begin{cases}\\\\text{mean}(\\\\mathbf{r}_{\\\\text{off}})+\\\\mathrm{bias}_{\\\\text{off}},&0\\\\leq\\\\text{mean}(\\\\mathbf{r}_{\\\\text{on}})-\\\\text{mean}(\\\\mathbf{r}_{\\\\text{off}})\\\\leq\\\\mathrm{bias}_{\\\\text{off}},\\\\\\\\[6.0pt]\\n\\\\text{mean}(\\\\mathbf{r}_{\\\\text{off}}),&\\\\text{otherwise}.\\\\end{cases}\\n(1)\\nThis mechanism prevents the model from gaining an unfair advantage by overfitting to the more verbose but more accurate Think-on mode. Moreover, it ensures that the adjusted accuracies remain faithful to the true relative performance between reasoning modes, thereby improving training stability and preserving the intended balance between depth and efficiency.\\n3.2.3 Supervision RL with HiPO\\nThe final advantage function is formulated as a hybrid signal that integrates both judge analysis and model response. Each response ii receives two distinct scalar advantage, including judge advantage based on the quality of the mode justification, and answer advantage based on correctness and format.\\nThe judge advantage Aijudge\\\\mathrm{A}^{\\\\text{judge}}_{i} captures the broader decision-level utility of selecting a particular mode. The first term, mean\\u200b(𝐫Mi)−mean\\u200b(𝐫)\\\\text{mean}(\\\\mathbf{r}_{M_{i}})-\\\\text{mean}(\\\\mathbf{r}), quantifies the global advantage of the chosen mode over the full group average, guiding the model toward choosing modes that lead to higher expected rewards. The second term, γ⋅(ri−mean\\u200b(𝐫))\\\\gamma\\\\cdot(r_{i}-\\\\text{mean}(\\\\mathbf{r})), ensures that the justification content is also responsible for the quality of the response under that mode, thereby aligning the explanation with actual performance. The use of a global normalization factor std\\u200b(𝐫)\\\\text{std}(\\\\mathbf{r}) stabilizes the reward signal across groups.\\nThe judge advantage function for response ii is then given by:\\nAijudge={(mean\\u200b(𝐫on)−mean\\u200b(𝐫))+γ⋅(ri−mean\\u200b(𝐫))std\\u200b(𝐫),if\\xa0\\u200bMi=on,(mean\\u200b(𝐫off)−mean\\u200b(𝐫))+γ⋅(ri−mean\\u200b(𝐫))std\\u200b(𝐫),if\\xa0\\u200bMi=off.\\\\mathrm{A}^{\\\\text{judge}}_{i}=\\\\begin{cases}\\\\frac{(\\\\text{mean}(\\\\mathbf{r}_{\\\\text{on}})-\\\\text{mean}(\\\\mathbf{r}))+\\\\gamma\\\\cdot(r_{i}-\\\\text{mean}(\\\\mathbf{r}))}{\\\\text{std}(\\\\mathbf{r})},&\\\\text{if }<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:41:39 [engine.py:317] Added request chatcmpl-1376377155694131a27b52f0bef532a0.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from data/output/arxiv_org_4.txt...vLLM STDOUT: INFO:     127.0.0.1:56880 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 9 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/arxiv_org_4_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/arxiv_org_4_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from data/output/arxiv_org_4.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/arxiv_org_4_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:34968 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:34974 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:41:48 [logger.py:43] Received request chatcmpl-7b65df50db944aa9af533fb4efdd1333: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nmean}(\\\\mathbf{r}_{\\\\text{on}})-\\\\text{mean}(\\\\mathbf{r}))+\\\\gamma\\\\cdot(r_{i}-\\\\text{mean}(\\\\mathbf{r}))}{\\\\text{std}(\\\\mathbf{r})},&\\\\text{if }M_{i}=\\\\text{on},\\\\\\\\[6.0pt]\\n\\\\frac{(\\\\text{mean}(\\\\mathbf{r}_{\\\\text{off}})-\\\\text{mean}(\\\\mathbf{r}))+\\\\gamma\\\\cdot(r_{i}-\\\\text{mean}(\\\\mathbf{r}))}{\\\\text{std}(\\\\mathbf{r})},&\\\\text{if }M_{i}=\\\\text{off}.\\\\end{cases}\\n(2)\\nIn contrast to the judge advantage function, the advantage AianswerA^{\\\\text{answer}}_{i} is computed within the context of the selected reasoning mode. Since the mode MiM_{i} has already been determined prior to response generation, it is natural to assess the response quality relative to other responses within the same mode. This local normalization using mode-specific mean and standard deviation focuses the learning signal on intra-mode variance, encouraging the model to improve response quality without conflating mode preference. For response ii, the answer advantage is defined as:\\nAianswer={ri−mean\\u200b(𝐫on)std\\u200b(𝐫on),if\\xa0\\u200bMi=on,ri−mean\\u200b(𝐫off)std\\u200b(𝐫off),if\\xa0\\u200bMi=off.A^{\\\\text{answer}}_{i}=\\\\begin{cases}\\\\frac{r_{i}-\\\\text{mean}(\\\\mathbf{r}_{\\\\text{on}})}{\\\\text{std}(\\\\mathbf{r}_{\\\\text{on}})},&\\\\text{if }M_{i}=\\\\text{on},\\\\\\\\[6.0pt]\\n\\\\frac{r_{i}-\\\\text{mean}(\\\\mathbf{r}_{\\\\text{off}})}{\\\\text{std}(\\\\mathbf{r}_{\\\\text{off}})},&\\\\text{if }M_{i}=\\\\text{off}.\\\\end{cases}\\n(3)\\nTo assign token-level reward for training with reinforcement learning, we define the final reward for each token tt in sample ii as follows:\\nAi,t={Aianswer,if token\\xa0\\u200bt∈𝒯answer,Aijudge,if token\\xa0\\u200bt∈𝒯judge.\\\\mathrm{A}_{i,t}=\\\\begin{cases}\\\\mathrm{A}^{\\\\text{answer}}_{i},&\\\\text{if token }t\\\\in\\\\mathcal{T}^{\\\\text{answer}},\\\\\\\\\\n\\\\mathrm{A}^{\\\\text{judge}}_{i},&\\\\text{if token }t\\\\in\\\\mathcal{T}^{\\\\text{judge}}.\\\\end{cases}\\n(4)\\nwhere 𝒯judge\\\\mathcal{T}^{\\\\text{judge}} and 𝒯answer\\\\mathcal{T}^{\\\\text{answer}} denote the token index sets corresponding to the judge segment and the answer segment, respectively, within each response.\\nGiven a query qq, HiPO generates a collection of candidate outputs {oi}i=1G\\\\{o_{i}\\\\}_{i=1}^{G} from the old policy πθold\\\\pi_{\\\\theta_{\\\\text{old}}}. For each output oio_{i}, let 𝒯i\\\\mathcal{T}_{i} denote the set of token positions in response ii, i.e., 𝒯i=𝒯judge∪𝒯answer\\\\mathcal{T}_{i}=\\\\mathcal{T}^{\\\\text{judge}}\\\\cup\\\\mathcal{T}^{\\\\text{answer}}. We define the per-token probability ratio as\\nρi,t=πθ(yi,t|hi,t)πθold(yi,t|hi,t)\\\\rho_{i,t}=\\\\frac{\\\\pi_{\\\\theta}\\\\!\\\\left(y_{i,t}\\\\,\\\\middle|\\\\,h_{i,t}\\\\right)}{\\\\pi_{\\\\theta_{\\\\text{old}}}\\\\!\\\\left(y_{i,t}\\\\,\\\\middle|\\\\,h_{i,t}\\\\right)},\\nwhere yi,ty_{i,t}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:41:48 [engine.py:317] Added request chatcmpl-7b65df50db944aa9af533fb4efdd1333.\n",
            "\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from data/output/arxiv_org_5.txt...vLLM STDOUT: INFO:     127.0.0.1:34978 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:41:51 [logger.py:43] Received request chatcmpl-5e699bb7b23c49b9aeb26be3e579ff8f: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nmean}(\\\\mathbf{r}_{\\\\text{on}})-\\\\text{mean}(\\\\mathbf{r}))+\\\\gamma\\\\cdot(r_{i}-\\\\text{mean}(\\\\mathbf{r}))}{\\\\text{std}(\\\\mathbf{r})},&\\\\text{if }M_{i}=\\\\text{on},\\\\\\\\[6.0pt]\\n\\\\frac{(\\\\text{mean}(\\\\mathbf{r}_{\\\\text{off}})-\\\\text{mean}(\\\\mathbf{r}))+\\\\gamma\\\\cdot(r_{i}-\\\\text{mean}(\\\\mathbf{r}))}{\\\\text{std}(\\\\mathbf{r})},&\\\\text{if }M_{i}=\\\\text{off}.\\\\end{cases}\\n(2)\\nIn contrast to the judge advantage function, the advantage AianswerA^{\\\\text{answer}}_{i} is computed within the context of the selected reasoning mode. Since the mode MiM_{i} has already been determined prior to response generation, it is natural to assess the response quality relative to other responses within the same mode. This local normalization using mode-specific mean and standard deviation focuses the learning signal on intra-mode variance, encouraging the model to improve response quality without conflating mode preference. For response ii, the answer advantage is defined as:\\nAianswer={ri−mean\\u200b(𝐫on)std\\u200b(𝐫on),if\\xa0\\u200bMi=on,ri−mean\\u200b(𝐫off)std\\u200b(𝐫off),if\\xa0\\u200bMi=off.A^{\\\\text{answer}}_{i}=\\\\begin{cases}\\\\frac{r_{i}-\\\\text{mean}(\\\\mathbf{r}_{\\\\text{on}})}{\\\\text{std}(\\\\mathbf{r}_{\\\\text{on}})},&\\\\text{if }M_{i}=\\\\text{on},\\\\\\\\[6.0pt]\\n\\\\frac{r_{i}-\\\\text{mean}(\\\\mathbf{r}_{\\\\text{off}})}{\\\\text{std}(\\\\mathbf{r}_{\\\\text{off}})},&\\\\text{if }M_{i}=\\\\text{off}.\\\\end{cases}\\n(3)\\nTo assign token-level reward for training with reinforcement learning, we define the final reward for each token tt in sample ii as follows:\\nAi,t={Aianswer,if token\\xa0\\u200bt∈𝒯answer,Aijudge,if token\\xa0\\u200bt∈𝒯judge.\\\\mathrm{A}_{i,t}=\\\\begin{cases}\\\\mathrm{A}^{\\\\text{answer}}_{i},&\\\\text{if token }t\\\\in\\\\mathcal{T}^{\\\\text{answer}},\\\\\\\\\\n\\\\mathrm{A}^{\\\\text{judge}}_{i},&\\\\text{if token }t\\\\in\\\\mathcal{T}^{\\\\text{judge}}.\\\\end{cases}\\n(4)\\nwhere 𝒯judge\\\\mathcal{T}^{\\\\text{judge}} and 𝒯answer\\\\mathcal{T}^{\\\\text{answer}} denote the token index sets corresponding to the judge segment and the answer segment, respectively, within each response.\\nGiven a query qq, HiPO generates a collection of candidate outputs {oi}i=1G\\\\{o_{i}\\\\}_{i=1}^{G} from the old policy πθold\\\\pi_{\\\\theta_{\\\\text{old}}}. For each output oio_{i}, let 𝒯i\\\\mathcal{T}_{i} denote the set of token positions in response ii, i.e., 𝒯i=𝒯judge∪𝒯answer\\\\mathcal{T}_{i}=\\\\mathcal{T}^{\\\\text{judge}}\\\\cup\\\\mathcal{T}^{\\\\text{answer}}. We define the per-token probability ratio as\\nρi,t=πθ(yi,t|hi,t)πθold(yi,t|hi,t)\\\\rho_{i,t}=\\\\frac{\\\\pi_{\\\\theta}\\\\!\\\\left(y_{i,t}\\\\,\\\\middle|\\\\,h_{i,t}\\\\right)}{\\\\pi_{\\\\theta_{\\\\text{old}}}\\\\!\\\\left(y_{i,t}\\\\,\\\\middle|\\\\,h_{i,t}\\\\right)},\\nwhere yi,ty_{i,t}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:41:51 [engine.py:317] Added request chatcmpl-5e699bb7b23c49b9aeb26be3e579ff8f.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from data/output/arxiv_org_5.txt...vLLM STDOUT: INFO:     127.0.0.1:34980 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 5 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/arxiv_org_5_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/arxiv_org_5_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from data/output/arxiv_org_5.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/arxiv_org_5_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:37298 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:37308 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:42:00 [logger.py:43] Received request chatcmpl-e0c600390e124b8280b9094558e545a4: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n=\\\\frac{\\\\pi_{\\\\theta}\\\\!\\\\left(y_{i,t}\\\\,\\\\middle|\\\\,h_{i,t}\\\\right)}{\\\\pi_{\\\\theta_{\\\\text{old}}}\\\\!\\\\left(y_{i,t}\\\\,\\\\middle|\\\\,h_{i,t}\\\\right)},\\nwhere yi,ty_{i,t} is the tt-th generated token in oio_{i} and hi,th_{i,t} is its conditioning context.\\nThe policy πθ\\\\pi_{\\\\theta} is optimized by maximizing the following token-level objective:\\n𝒥\\u200b(θ)\\\\displaystyle\\\\mathcal{J}(\\\\theta)\\n=𝔼[q∼P(Q),{oi}i=1G∼πθold(⋅|q)]\\\\displaystyle=\\\\mathbb{E}\\\\Big[q\\\\sim P(Q),\\\\{o_{i}\\\\}_{i=1}^{G}\\\\sim\\\\pi_{\\\\theta_{\\\\text{old}}}(\\\\cdot\\\\,|\\\\,q)\\\\Big]\\n(5)\\n⋅1G∑i=1G1|𝒯i|∑t∈𝒯i(min(ρi,tAi,t,clip(ρi,t,\\u20091−ϵ,\\u20091+ϵ)Ai,t)\\\\displaystyle\\\\quad\\\\cdot\\\\frac{1}{G}\\\\sum_{i=1}^{G}\\\\frac{1}{\\\\lvert\\\\mathcal{T}_{i}\\\\rvert}\\\\sum_{t\\\\in\\\\mathcal{T}_{i}}\\\\Big(\\\\min\\\\big(\\\\rho_{i,t}\\\\,A_{i,t},\\\\;\\\\text{clip}(\\\\rho_{i,t},1-\\\\epsilon,1+\\\\epsilon)\\\\,A_{i,t}\\\\big)\\n−β𝔻K\\u200bL(πθ(⋅|hi,t)∥πref(⋅|hi,t))).\\\\displaystyle\\\\qquad-\\\\beta\\\\,\\\\mathbb{D}_{KL}\\\\!\\\\big(\\\\pi_{\\\\theta}(\\\\cdot\\\\,|\\\\,h_{i,t})\\\\,\\\\big\\\\|\\\\,\\\\pi_{\\\\text{ref}}(\\\\cdot\\\\,|\\\\,h_{i,t})\\\\big)\\\\Big).\\nHere Ai,tA_{i,t} is the token-level advantage defined in Eq. (4) via segment-wise assignment, and 𝔻K\\u200bL\\\\mathbb{D}_{KL} is the token-level KL between the current policy and the reference policy at context hi,th_{i,t}.\\n3.3 Training Paradigm\\nOur HiPO framework adopts a two-stage training paradigm, consisting of a cold-start stage and a RL stage.\\nIn the code-start stage, the model is initialized with high-quality, hybrid training data that contains both Think-on and Think-off responses.\\nThis stage enables the model to acquire fundamental reasoning and answering capabilities, while establishing an initial balance between analytical reasoning and concise responses.\\nIn the RL stage, the model is further optimized using our hybrid reward system, which integrates mode-specific accuracy and global average performance.\\nTogether, these two stages ensure that HiPO achieves both strong factual accuracy and robust reasoning ability across diverse domains.\\n4 Experiments\\n4.1 Experimental setup\\nImplementation details.\\nSince the Qwen3 model can freely switch between inference modes, we chose it for our experiment. However, when the training data is insufficient, training the Qwen3 model can easily lead to a decline in performance on the test set (details can be found in the appendix A.2).\\nTo address this, we conducted Cold-Start tuning to stabilize its performance with relatively large datasets.\\nFor the Cold-Start stage, we use the “AM-Thinking-v1-Distilled”, “AceReason-Math”, “AM-Thinking”, “II-Thought-RL(math)” dataset for training. The parameters are set as: maximum learning rate is 8e-5, minimum learning rate is 8e-6\\nand batch size is 512. For the RL stage, we use the “II-Thought-RL(code)”, “Skywork-OR1-RL-Data” dataset for training. The parameters are set as: batch size = 16, maximum response length = 32k, NN = 16, ω\\\\omega = 0.01, and γ=0.3\\\\gamma=0.3.\\nBaselines.\\nTo demonstrate the effect of mitigating overthinking, we<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:42:00 [engine.py:317] Added request chatcmpl-e0c600390e124b8280b9094558e545a4.\n",
            "\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from data/output/arxiv_org_6.txt...vLLM STDOUT: INFO:     127.0.0.1:37314 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:42:02 [logger.py:43] Received request chatcmpl-cafb73c0deb34b7fb0b7a6f1b48a37d5: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\n=\\\\frac{\\\\pi_{\\\\theta}\\\\!\\\\left(y_{i,t}\\\\,\\\\middle|\\\\,h_{i,t}\\\\right)}{\\\\pi_{\\\\theta_{\\\\text{old}}}\\\\!\\\\left(y_{i,t}\\\\,\\\\middle|\\\\,h_{i,t}\\\\right)},\\nwhere yi,ty_{i,t} is the tt-th generated token in oio_{i} and hi,th_{i,t} is its conditioning context.\\nThe policy πθ\\\\pi_{\\\\theta} is optimized by maximizing the following token-level objective:\\n𝒥\\u200b(θ)\\\\displaystyle\\\\mathcal{J}(\\\\theta)\\n=𝔼[q∼P(Q),{oi}i=1G∼πθold(⋅|q)]\\\\displaystyle=\\\\mathbb{E}\\\\Big[q\\\\sim P(Q),\\\\{o_{i}\\\\}_{i=1}^{G}\\\\sim\\\\pi_{\\\\theta_{\\\\text{old}}}(\\\\cdot\\\\,|\\\\,q)\\\\Big]\\n(5)\\n⋅1G∑i=1G1|𝒯i|∑t∈𝒯i(min(ρi,tAi,t,clip(ρi,t,\\u20091−ϵ,\\u20091+ϵ)Ai,t)\\\\displaystyle\\\\quad\\\\cdot\\\\frac{1}{G}\\\\sum_{i=1}^{G}\\\\frac{1}{\\\\lvert\\\\mathcal{T}_{i}\\\\rvert}\\\\sum_{t\\\\in\\\\mathcal{T}_{i}}\\\\Big(\\\\min\\\\big(\\\\rho_{i,t}\\\\,A_{i,t},\\\\;\\\\text{clip}(\\\\rho_{i,t},1-\\\\epsilon,1+\\\\epsilon)\\\\,A_{i,t}\\\\big)\\n−β𝔻K\\u200bL(πθ(⋅|hi,t)∥πref(⋅|hi,t))).\\\\displaystyle\\\\qquad-\\\\beta\\\\,\\\\mathbb{D}_{KL}\\\\!\\\\big(\\\\pi_{\\\\theta}(\\\\cdot\\\\,|\\\\,h_{i,t})\\\\,\\\\big\\\\|\\\\,\\\\pi_{\\\\text{ref}}(\\\\cdot\\\\,|\\\\,h_{i,t})\\\\big)\\\\Big).\\nHere Ai,tA_{i,t} is the token-level advantage defined in Eq. (4) via segment-wise assignment, and 𝔻K\\u200bL\\\\mathbb{D}_{KL} is the token-level KL between the current policy and the reference policy at context hi,th_{i,t}.\\n3.3 Training Paradigm\\nOur HiPO framework adopts a two-stage training paradigm, consisting of a cold-start stage and a RL stage.\\nIn the code-start stage, the model is initialized with high-quality, hybrid training data that contains both Think-on and Think-off responses.\\nThis stage enables the model to acquire fundamental reasoning and answering capabilities, while establishing an initial balance between analytical reasoning and concise responses.\\nIn the RL stage, the model is further optimized using our hybrid reward system, which integrates mode-specific accuracy and global average performance.\\nTogether, these two stages ensure that HiPO achieves both strong factual accuracy and robust reasoning ability across diverse domains.\\n4 Experiments\\n4.1 Experimental setup\\nImplementation details.\\nSince the Qwen3 model can freely switch between inference modes, we chose it for our experiment. However, when the training data is insufficient, training the Qwen3 model can easily lead to a decline in performance on the test set (details can be found in the appendix A.2).\\nTo address this, we conducted Cold-Start tuning to stabilize its performance with relatively large datasets.\\nFor the Cold-Start stage, we use the “AM-Thinking-v1-Distilled”, “AceReason-Math”, “AM-Thinking”, “II-Thought-RL(math)” dataset for training. The parameters are set as: maximum learning rate is 8e-5, minimum learning rate is 8e-6\\nand batch size is 512. For the RL stage, we use the “II-Thought-RL(code)”, “Skywork-OR1-RL-Data” dataset for training. The parameters are set as: batch size = 16, maximum response length = 32k, NN = 16, ω\\\\omega = 0.01, and γ=0.3\\\\gamma=0.3.\\nBaselines.\\nTo demonstrate the effect of mitigating overthinking, we<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:42:02 [engine.py:317] Added request chatcmpl-cafb73c0deb34b7fb0b7a6f1b48a37d5.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from data/output/arxiv_org_6.txt...vLLM STDOUT: INFO:     127.0.0.1:37324 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 7 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/arxiv_org_6_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/arxiv_org_6_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from data/output/arxiv_org_6.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/arxiv_org_6_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:55436 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:55446 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:42:11 [logger.py:43] Received request chatcmpl-163ec84296e644bea9001a1aae1fbb2d: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nfor training. The parameters are set as: batch size = 16, maximum response length = 32k, NN = 16, ω\\\\omega = 0.01, and γ=0.3\\\\gamma=0.3.\\nBaselines.\\nTo demonstrate the effect of mitigating overthinking, we designed the following baselines for comparison.\\n(1) Cold-Start: We perform Cold-Star on the model using the data construction method described in Section\\xa03.1.\\n(2) Cold-Start (On): We apply the same Cold-Star procedure as in Section\\xa03.1, but only include the data collected under the Think-on mode.\\n(3) Cold-Start (On) + GRPO: We further train the Cold-Start (On) model using the GRPO algorithm.\\n(4) Cold-Start + GRPO: We further train the Cold-Start model with the GRPO algorithm.\\n(5) HiPO: We train the model following our HiPO.\\n(6) AdaptThink: We reproduced the code provided in (Zhang et\\xa0al., 2025).\\n(7) AutoThink: We reproduced the code provided in (Tu et\\xa0al., 2025).\\nEvaluation benchmarks.\\nWe conducted tests on AIME2024, AIME2025, HumanEval\\xa0(Chen et\\xa0al., 2021), LiveCodeBench V6\\xa0(Jain et\\xa0al., 2024), MBPP\\xa0(Austin et\\xa0al., 2021),\\nMATH-500\\xa0(Lightman et\\xa0al., 2023), and GPQA-Diamond\\xa0(Rein et\\xa0al., 2023).\\n4.2 Main Results\\nAIME2024\\nAIME2025\\nLiveCodeBench\\nHumanEval\\nMethod\\nAcc↑\\\\uparrow\\nLength↓\\\\downarrow\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}↓\\\\downarrow\\nAcc↑\\\\uparrow\\nLength↓\\\\downarrow\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}↓\\\\downarrow\\nAcc↑\\\\uparrow\\nLength↓\\\\downarrow\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}↓\\\\downarrow\\nAcc↑\\\\uparrow\\nLength↓\\\\downarrow\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}↓\\\\downarrow\\nCold-Start (on)\\n80.8\\n21265\\n1.00\\n71.7\\n23791\\n1.00\\n56.2\\n19473\\n1.00\\n82.9\\n2662\\n1.00\\n+ GRPO\\n82.5↑2.1%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 2.1\\\\%}\\n21045↓1.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 1.0\\\\%}\\n1.00−0.0%\\\\scriptstyle{\\\\scriptstyle-0.0\\\\%}\\n76.7↑7%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 7\\\\%}\\n22695↓4.6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 4.6\\\\%}\\n1.00−0.0%\\\\scriptstyle{\\\\scriptstyle-0.0\\\\%}\\n57.3↑2.0%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 2.0\\\\%}\\n19067↓2.1%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 2.1\\\\%}\\n1.00−0.0%\\\\scriptstyle{\\\\scriptstyle-0.0\\\\%}\\n95.1↑14.7%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 14.7\\\\%}\\n3597↑35.1%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 35.1\\\\%}\\n1.00−0.0%\\\\scriptstyle{\\\\scriptstyle-0.0\\\\%}\\nCold-Start\\n85.8↑6.2%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 6.2\\\\%}\\n18138↓14.7%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 14.7\\\\%}\\n1.00−0.0%\\\\scriptstyle{\\\\scriptstyle-0.0\\\\%}\\n76.7↑7.0%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 7.0\\\\%}\\n20613↓13.4%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 13.4\\\\<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:42:11 [engine.py:317] Added request chatcmpl-163ec84296e644bea9001a1aae1fbb2d.\n",
            "\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from data/output/arxiv_org_7.txt...vLLM STDOUT: INFO:     127.0.0.1:55448 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:42:13 [logger.py:43] Received request chatcmpl-1878f69e71254811bec48879bf6e0024: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\n for training. The parameters are set as: batch size = 16, maximum response length = 32k, NN = 16, ω\\\\omega = 0.01, and γ=0.3\\\\gamma=0.3.\\nBaselines.\\nTo demonstrate the effect of mitigating overthinking, we designed the following baselines for comparison.\\n(1) Cold-Start: We perform Cold-Star on the model using the data construction method described in Section\\xa03.1.\\n(2) Cold-Start (On): We apply the same Cold-Star procedure as in Section\\xa03.1, but only include the data collected under the Think-on mode.\\n(3) Cold-Start (On) + GRPO: We further train the Cold-Start (On) model using the GRPO algorithm.\\n(4) Cold-Start + GRPO: We further train the Cold-Start model with the GRPO algorithm.\\n(5) HiPO: We train the model following our HiPO.\\n(6) AdaptThink: We reproduced the code provided in (Zhang et\\xa0al., 2025).\\n(7) AutoThink: We reproduced the code provided in (Tu et\\xa0al., 2025).\\nEvaluation benchmarks.\\nWe conducted tests on AIME2024, AIME2025, HumanEval\\xa0(Chen et\\xa0al., 2021), LiveCodeBench V6\\xa0(Jain et\\xa0al., 2024), MBPP\\xa0(Austin et\\xa0al., 2021),\\nMATH-500\\xa0(Lightman et\\xa0al., 2023), and GPQA-Diamond\\xa0(Rein et\\xa0al., 2023).\\n4.2 Main Results\\nAIME2024\\nAIME2025\\nLiveCodeBench\\nHumanEval\\nMethod\\nAcc↑\\\\uparrow\\nLength↓\\\\downarrow\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}↓\\\\downarrow\\nAcc↑\\\\uparrow\\nLength↓\\\\downarrow\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}↓\\\\downarrow\\nAcc↑\\\\uparrow\\nLength↓\\\\downarrow\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}↓\\\\downarrow\\nAcc↑\\\\uparrow\\nLength↓\\\\downarrow\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}↓\\\\downarrow\\nCold-Start (on)\\n80.8\\n21265\\n1.00\\n71.7\\n23791\\n1.00\\n56.2\\n19473\\n1.00\\n82.9\\n2662\\n1.00\\n+ GRPO\\n82.5↑2.1%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 2.1\\\\%}\\n21045↓1.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 1.0\\\\%}\\n1.00−0.0%\\\\scriptstyle{\\\\scriptstyle-0.0\\\\%}\\n76.7↑7%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 7\\\\%}\\n22695↓4.6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 4.6\\\\%}\\n1.00−0.0%\\\\scriptstyle{\\\\scriptstyle-0.0\\\\%}\\n57.3↑2.0%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 2.0\\\\%}\\n19067↓2.1%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 2.1\\\\%}\\n1.00−0.0%\\\\scriptstyle{\\\\scriptstyle-0.0\\\\%}\\n95.1↑14.7%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 14.7\\\\%}\\n3597↑35.1%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 35.1\\\\%}\\n1.00−0.0%\\\\scriptstyle{\\\\scriptstyle-0.0\\\\%}\\nCold-Start\\n85.8↑6.2%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 6.2\\\\%}\\n18138↓14.7%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 14.7\\\\%}\\n1.00−0.0%\\\\scriptstyle{\\\\scriptstyle-0.0\\\\%}\\n76.7↑7.0%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 7.0\\\\%}\\n20613↓13.4%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 13.4\\\\<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:42:13 [engine.py:317] Added request chatcmpl-1878f69e71254811bec48879bf6e0024.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from data/output/arxiv_org_7.txt...vLLM STDOUT: INFO:     127.0.0.1:55462 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 10 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/arxiv_org_7_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/arxiv_org_7_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from data/output/arxiv_org_7.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/arxiv_org_7_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:49746 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:49756 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:42:22 [logger.py:43] Received request chatcmpl-597e82467ec94e9a93d74c436cce6d3c: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n}\\n1.00−0.0%\\\\scriptstyle{\\\\scriptstyle-0.0\\\\%}\\n76.7↑7.0%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 7.0\\\\%}\\n20613↓13.4%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 13.4\\\\%}\\n1.00−0.0%\\\\scriptstyle{\\\\scriptstyle-0.0\\\\%}\\n60.8↑8.2%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 8.2\\\\%}\\n18158↓6.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 6.8\\\\%}\\n0.91↓9.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 9.0\\\\%}\\n88.4↑6.6%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 6.6\\\\%}\\n2272↓14.6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 14.6\\\\%}\\n0.54↓46.3%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 46.3\\\\%}\\n+ GRPO\\n86.7↑7.2%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 7.2\\\\%}\\n17083↓19.7%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 19.7\\\\%}\\n1.00−0.0%\\\\scriptstyle{\\\\scriptstyle-0.0\\\\%}\\n79.17↑10.5%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 10.5\\\\%}\\n19869↓16.5%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 16.5\\\\%}\\n1.00−0.0%\\\\scriptstyle{\\\\scriptstyle-0.0\\\\%}\\n62.1↑10.6%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 10.6\\\\%}\\n18046↓7.3%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 7.3\\\\%}\\n0.93↓7.3%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 7.3\\\\%}\\n87.8↑5.9%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 5.9\\\\%}\\n2220↓16.6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 16.6\\\\%}\\n0.59↓40.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 40.8\\\\%}\\nAdaptThink\\n83.3↑3.1%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 3.1\\\\%}\\n16598↓21.9%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 21.9\\\\%}\\n0.93↓7.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 7.0\\\\%}\\n74.2↑3.5%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 3.5\\\\%}\\n19993↓16.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 16.0\\\\%}\\n0.84↓16.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 16.0\\\\%}\\n57.1↑1.6%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 1.6\\\\%}\\n16162↓17.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 17.0\\\\%}\\n0.78↓28.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 28.0\\\\%}\\n85.4↑3.0%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 3.0\\\\%}\\n915↓65.6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 65.6\\\\%}\\n0.16↓84.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 84.0\\\\%}\\nAutoThink\\n84.3↑3.5%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 3.5\\\\%}\\n17061↓19.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 19.8\\\\%}\\n0.95↓5.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 5.0\\\\%}\\n75.0↑4.6%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 4.6\\\\%}\\n18784↓21.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 21.0\\\\%}\\n0.88↓12.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 12.0\\\\%}\\n57.5↑<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:42:22 [engine.py:317] Added request chatcmpl-597e82467ec94e9a93d74c436cce6d3c.\n",
            "\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from data/output/arxiv_org_8.txt...vLLM STDOUT: INFO:     127.0.0.1:49768 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:42:24 [logger.py:43] Received request chatcmpl-b3db920d79764bcd82577a73dbde740e: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\n}\\n1.00−0.0%\\\\scriptstyle{\\\\scriptstyle-0.0\\\\%}\\n76.7↑7.0%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 7.0\\\\%}\\n20613↓13.4%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 13.4\\\\%}\\n1.00−0.0%\\\\scriptstyle{\\\\scriptstyle-0.0\\\\%}\\n60.8↑8.2%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 8.2\\\\%}\\n18158↓6.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 6.8\\\\%}\\n0.91↓9.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 9.0\\\\%}\\n88.4↑6.6%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 6.6\\\\%}\\n2272↓14.6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 14.6\\\\%}\\n0.54↓46.3%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 46.3\\\\%}\\n+ GRPO\\n86.7↑7.2%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 7.2\\\\%}\\n17083↓19.7%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 19.7\\\\%}\\n1.00−0.0%\\\\scriptstyle{\\\\scriptstyle-0.0\\\\%}\\n79.17↑10.5%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 10.5\\\\%}\\n19869↓16.5%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 16.5\\\\%}\\n1.00−0.0%\\\\scriptstyle{\\\\scriptstyle-0.0\\\\%}\\n62.1↑10.6%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 10.6\\\\%}\\n18046↓7.3%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 7.3\\\\%}\\n0.93↓7.3%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 7.3\\\\%}\\n87.8↑5.9%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 5.9\\\\%}\\n2220↓16.6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 16.6\\\\%}\\n0.59↓40.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 40.8\\\\%}\\nAdaptThink\\n83.3↑3.1%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 3.1\\\\%}\\n16598↓21.9%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 21.9\\\\%}\\n0.93↓7.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 7.0\\\\%}\\n74.2↑3.5%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 3.5\\\\%}\\n19993↓16.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 16.0\\\\%}\\n0.84↓16.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 16.0\\\\%}\\n57.1↑1.6%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 1.6\\\\%}\\n16162↓17.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 17.0\\\\%}\\n0.78↓28.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 28.0\\\\%}\\n85.4↑3.0%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 3.0\\\\%}\\n915↓65.6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 65.6\\\\%}\\n0.16↓84.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 84.0\\\\%}\\nAutoThink\\n84.3↑3.5%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 3.5\\\\%}\\n17061↓19.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 19.8\\\\%}\\n0.95↓5.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 5.0\\\\%}\\n75.0↑4.6%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 4.6\\\\%}\\n18784↓21.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 21.0\\\\%}\\n0.88↓12.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 12.0\\\\%}\\n57.5↑<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:42:24 [engine.py:317] Added request chatcmpl-b3db920d79764bcd82577a73dbde740e.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from data/output/arxiv_org_8.txt...vLLM STDOUT: INFO:     127.0.0.1:36918 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 17 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/arxiv_org_8_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/arxiv_org_8_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from data/output/arxiv_org_8.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/arxiv_org_8_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:36856 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:36872 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:42:34 [logger.py:43] Received request chatcmpl-c1e0dec0dcfd42998bd11f79f14328da: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nscriptstyle\\\\uparrow{\\\\scriptstyle 4.6\\\\%}\\n18784↓21.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 21.0\\\\%}\\n0.88↓12.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 12.0\\\\%}\\n57.5↑2.3%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 2.3\\\\%}\\n15672↓19.5%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 19.5\\\\%}\\n0.80↓20.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 20.0\\\\%}\\n82.3↓0.7%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 0.7\\\\%}\\n1050↓60.6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 60.6\\\\%}\\n0.18↓82.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 82.0\\\\%}\\nHiPO\\n87.5↑8.3%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 8.3\\\\%}\\n15107↓29.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 29.0\\\\%}\\n0.98↓1.7%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 1.7\\\\%}\\n82.5↑15.1%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 15.1\\\\%}\\n17655↓25.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 25.8\\\\%}\\n0.95↓5.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 5.0\\\\%}\\n63.0↑12.2%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 12.2\\\\%}\\n13558↓30.4%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 30.4\\\\%}\\n0.82↓18.5%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 18.5\\\\%}\\n90.2↑8.8%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 8.8\\\\%}\\n776↓70.9%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 70.9\\\\%}\\n0.12↓88.4%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 88.4\\\\%}\\nMATH-500\\nGPQA-Diamond\\nMBPP\\nAverage\\nMethod\\nAcc↑\\\\uparrow\\nLength↓\\\\downarrow\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}↓\\\\downarrow\\nAcc↑\\\\uparrow\\nLength↓\\\\downarrow\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}↓\\\\downarrow\\nAcc↑\\\\uparrow\\nLength↓\\\\downarrow\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}↓\\\\downarrow\\nAcc↑\\\\uparrow\\nLength↓\\\\downarrow\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}↓\\\\downarrow\\nCold-Start (on)\\n92.0\\n6237\\n1.00\\n61.1\\n10832\\n1.00\\n72.0\\n4411\\n1.00\\n73.8\\n12667\\n1.00\\n+ GRPO\\n93.2↑0.0%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 0.0\\\\%}\\n6256↑1.3%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 1.3\\\\%}\\n1.00−0.0%\\\\scriptstyle{\\\\scriptstyle-0.0\\\\%}\\n57.6↓5.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 5.8\\\\%}\\n10633↓1.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 1.8\\\\%}\\n1.00−0.0%\\\\scriptstyle{\\\\scriptstyle-0.0\\\\%}\\n71.8↓0.3%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 0.3\\\\%}\\n5103↑15.7%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 15.7\\\\%}\\n1.00−0.0%\\\\scriptstyle-{\\\\scriptstyle 0.0\\\\%}\\n76.3↑3.3%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 3.3\\\\%}\\n12628↓0.3%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 0.3\\\\%}\\n1.00−0.0%\\\\script<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:42:34 [engine.py:317] Added request chatcmpl-c1e0dec0dcfd42998bd11f79f14328da.\n",
            "\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from data/output/arxiv_org_9.txt...vLLM STDOUT: INFO:     127.0.0.1:36886 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:42:37 [logger.py:43] Received request chatcmpl-7ba231c19b63448b918883ee4742519e: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nscriptstyle\\\\uparrow{\\\\scriptstyle 4.6\\\\%}\\n18784↓21.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 21.0\\\\%}\\n0.88↓12.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 12.0\\\\%}\\n57.5↑2.3%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 2.3\\\\%}\\n15672↓19.5%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 19.5\\\\%}\\n0.80↓20.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 20.0\\\\%}\\n82.3↓0.7%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 0.7\\\\%}\\n1050↓60.6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 60.6\\\\%}\\n0.18↓82.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 82.0\\\\%}\\nHiPO\\n87.5↑8.3%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 8.3\\\\%}\\n15107↓29.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 29.0\\\\%}\\n0.98↓1.7%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 1.7\\\\%}\\n82.5↑15.1%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 15.1\\\\%}\\n17655↓25.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 25.8\\\\%}\\n0.95↓5.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 5.0\\\\%}\\n63.0↑12.2%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 12.2\\\\%}\\n13558↓30.4%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 30.4\\\\%}\\n0.82↓18.5%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 18.5\\\\%}\\n90.2↑8.8%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 8.8\\\\%}\\n776↓70.9%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 70.9\\\\%}\\n0.12↓88.4%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 88.4\\\\%}\\nMATH-500\\nGPQA-Diamond\\nMBPP\\nAverage\\nMethod\\nAcc↑\\\\uparrow\\nLength↓\\\\downarrow\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}↓\\\\downarrow\\nAcc↑\\\\uparrow\\nLength↓\\\\downarrow\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}↓\\\\downarrow\\nAcc↑\\\\uparrow\\nLength↓\\\\downarrow\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}↓\\\\downarrow\\nAcc↑\\\\uparrow\\nLength↓\\\\downarrow\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}↓\\\\downarrow\\nCold-Start (on)\\n92.0\\n6237\\n1.00\\n61.1\\n10832\\n1.00\\n72.0\\n4411\\n1.00\\n73.8\\n12667\\n1.00\\n+ GRPO\\n93.2↑0.0%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 0.0\\\\%}\\n6256↑1.3%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 1.3\\\\%}\\n1.00−0.0%\\\\scriptstyle{\\\\scriptstyle-0.0\\\\%}\\n57.6↓5.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 5.8\\\\%}\\n10633↓1.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 1.8\\\\%}\\n1.00−0.0%\\\\scriptstyle{\\\\scriptstyle-0.0\\\\%}\\n71.8↓0.3%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 0.3\\\\%}\\n5103↑15.7%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 15.7\\\\%}\\n1.00−0.0%\\\\scriptstyle-{\\\\scriptstyle 0.0\\\\%}\\n76.3↑3.3%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 3.3\\\\%}\\n12628↓0.3%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 0.3\\\\%}\\n1.00−0.0%\\\\script<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:42:37 [engine.py:317] Added request chatcmpl-7ba231c19b63448b918883ee4742519e.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from data/output/arxiv_org_9.txt...vLLM STDOUT: INFO:     127.0.0.1:36900 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 17 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/arxiv_org_9_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/arxiv_org_9_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from data/output/arxiv_org_9.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/arxiv_org_9_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:59110 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:59118 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:42:46 [logger.py:43] Received request chatcmpl-45db58687d3646d4bd58dc4f253d09b1: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{\\\\scriptstyle 0.0\\\\%}\\n76.3↑3.3%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 3.3\\\\%}\\n12628↓0.3%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 0.3\\\\%}\\n1.00−0.0%\\\\scriptstyle{\\\\scriptstyle-0.0\\\\%}\\nCold-Start\\n93.0↑1.1%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 1.1\\\\%}\\n5215↓16.4%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 16.4\\\\%}\\n0.65↓35.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 35.0\\\\%}\\n61.6↑0.8%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 0.8\\\\%}\\n11172↑3.1%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 3.1\\\\%}\\n0.95↓4.5%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 4.5\\\\%}\\n71.4↓0.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 0.8\\\\%}\\n3561↓19.3%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 19.3\\\\%}\\n0.42↓58.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 58.0\\\\%}\\n76.8↑4.1%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 4.1\\\\%}\\n11304↓10.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 10.8\\\\%}\\n0.78↓21.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 21.8\\\\%}\\n+ GRPO\\n92.8↑0.9%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 0.9\\\\%}\\n5204↓16.6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 16.6\\\\%}\\n0.68↓31.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 31.8\\\\%}\\n58.6↓4.1%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 4.1\\\\%}\\n10581↓2.3%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 2.3\\\\%}\\n0.98↓2.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 2.0\\\\%}\\n72.0−0.0%\\\\scriptstyle{\\\\scriptstyle-0.0\\\\%}\\n4341↓1.6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 1.6\\\\%}\\n0.38↓62.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 62.0\\\\%}\\n77.0↑4.3%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 4.3\\\\%}\\n11049 ↓12.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 12.8\\\\%}\\n0.79↓20.6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 20.6\\\\%}\\nAdaptThink\\n92.8↑0.9%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 0.9\\\\%}\\n4213↓32.5%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 32.5\\\\%}\\n0.55↓45.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 45.0\\\\%}\\n56.1↓8.2%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 8.2\\\\%}\\n10242↓5.4%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 5.4\\\\%}\\n0.91↓9.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 9.0\\\\%}\\n68.0↓5.6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 5.6\\\\%}\\n4165↓5.6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 5.6\\\\%}\\n0.33↓67.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 67.0\\\\%}\\n73.8−0.0%\\\\scriptstyle-{\\\\scriptstyle 0.0\\\\%}\\n10327↓18.5%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 18.5\\\\%}\\n0.64↓36.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 36.0\\\\%}\\nAutoThink\\n92.8↑0.9%\\\\scriptstyle<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:42:46 [engine.py:317] Added request chatcmpl-45db58687d3646d4bd58dc4f253d09b1.\n",
            "\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from data/output/arxiv_org_10.txt...vLLM STDOUT: INFO:     127.0.0.1:59124 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:42:48 [logger.py:43] Received request chatcmpl-af9828f96466409f9af3c44b6016d891: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\n{\\\\scriptstyle 0.0\\\\%}\\n76.3↑3.3%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 3.3\\\\%}\\n12628↓0.3%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 0.3\\\\%}\\n1.00−0.0%\\\\scriptstyle{\\\\scriptstyle-0.0\\\\%}\\nCold-Start\\n93.0↑1.1%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 1.1\\\\%}\\n5215↓16.4%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 16.4\\\\%}\\n0.65↓35.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 35.0\\\\%}\\n61.6↑0.8%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 0.8\\\\%}\\n11172↑3.1%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 3.1\\\\%}\\n0.95↓4.5%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 4.5\\\\%}\\n71.4↓0.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 0.8\\\\%}\\n3561↓19.3%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 19.3\\\\%}\\n0.42↓58.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 58.0\\\\%}\\n76.8↑4.1%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 4.1\\\\%}\\n11304↓10.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 10.8\\\\%}\\n0.78↓21.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 21.8\\\\%}\\n+ GRPO\\n92.8↑0.9%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 0.9\\\\%}\\n5204↓16.6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 16.6\\\\%}\\n0.68↓31.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 31.8\\\\%}\\n58.6↓4.1%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 4.1\\\\%}\\n10581↓2.3%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 2.3\\\\%}\\n0.98↓2.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 2.0\\\\%}\\n72.0−0.0%\\\\scriptstyle{\\\\scriptstyle-0.0\\\\%}\\n4341↓1.6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 1.6\\\\%}\\n0.38↓62.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 62.0\\\\%}\\n77.0↑4.3%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 4.3\\\\%}\\n11049 ↓12.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 12.8\\\\%}\\n0.79↓20.6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 20.6\\\\%}\\nAdaptThink\\n92.8↑0.9%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 0.9\\\\%}\\n4213↓32.5%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 32.5\\\\%}\\n0.55↓45.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 45.0\\\\%}\\n56.1↓8.2%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 8.2\\\\%}\\n10242↓5.4%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 5.4\\\\%}\\n0.91↓9.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 9.0\\\\%}\\n68.0↓5.6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 5.6\\\\%}\\n4165↓5.6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 5.6\\\\%}\\n0.33↓67.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 67.0\\\\%}\\n73.8−0.0%\\\\scriptstyle-{\\\\scriptstyle 0.0\\\\%}\\n10327↓18.5%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 18.5\\\\%}\\n0.64↓36.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 36.0\\\\%}\\nAutoThink\\n92.8↑0.9%\\\\scriptstyle<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:42:48 [engine.py:317] Added request chatcmpl-af9828f96466409f9af3c44b6016d891.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from data/output/arxiv_org_10.txt...vLLM STDOUT: INFO:     127.0.0.1:59140 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 16 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/arxiv_org_10_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/arxiv_org_10_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from data/output/arxiv_org_10.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/arxiv_org_10_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:59488 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:59494 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:42:57 [logger.py:43] Received request chatcmpl-250f2fee5d74463eb7beba6b65d27825: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n0.0\\\\%}\\n10327↓18.5%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 18.5\\\\%}\\n0.64↓36.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 36.0\\\\%}\\nAutoThink\\n92.8↑0.9%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 0.9\\\\%}\\n4261↓31.7%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 31.7\\\\%}\\n0.56↓44.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 44.0\\\\%}\\n58.1↓4.9%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 4.9\\\\%}\\n9898↓8.6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 8.6\\\\%}\\n0.89↓11.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 11.0\\\\%}\\n70.0↓2.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 2.8\\\\%}\\n4958↓12.4%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 12.4\\\\%}\\n0.40↓60.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 60.0\\\\%}\\n74.3↑0.7%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 0.7\\\\%}\\n10240↓19.2%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 19.2\\\\%}\\n0.67↓33.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 33.0\\\\%}\\nHiPO\\n93.6 ↑1.7%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 1.7\\\\%}\\n4090 ↓34.4%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 34.4\\\\%}\\n0.54 ↓45.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 45.8\\\\%}\\n60.1 ↓1.7%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 1.7\\\\%}\\n9367 ↓13.5%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 13.5\\\\%}\\n0.92↓−8.1%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle-8.1\\\\%}\\n72.2↑0.3%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 0.3\\\\%}\\n1338↓69.7%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 69.7\\\\%}\\n0.12↓88.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 88.0\\\\%}\\n78.4↑6.3%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 6.3\\\\%}\\n8842↓30.2%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 30.2\\\\%}\\n0.63↓36.5%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 36.5\\\\%}\\nTable 2: Based on Qwen3-8B, performance of different methods on multiple benchmarks. RatioT\\\\textbf{Ratio}_{\\\\textbf{T}} denotes the ratio of “Think-on” mode over the corresponding benchmark.\\nIn Table 2, we observe that training the model solely on Think-on data leads the model to engage in reasoning for problems of any difficulty. We use this baseline as a typical example of \"overthinking\" for comparison. After applying GRPO to the Cold-Start (on) model, there is a significant improvement in accuracy, with an average accuracy increase of 3.1%. However, this does not reduce the token length and thinking rate of the model. On the contrary, to achieve higher accuracy, the token length output by the model on simpler datasets increases significantly. When training the model on a dataset containing both Think-on and Think-off data, the accuracy of the resulting Cold-Start model improves by 4.0% compared to the Cold-Start(on) model, while the token length and thinking rate decrease by 10.8% and 22%, respectively. After applying the GRPO algorithm to the Cold-Start model, there is no significant change in performance. However, when applying our method HiPO to train the Cold-Start model, the accuracy improves by 6.2%, while the token length and thinking rate decrease dramatically by 30% and 39%, respectively. Moreover, experimental results show that our HiPO outperforms existing methods on both efficiency<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:42:57 [engine.py:317] Added request chatcmpl-250f2fee5d74463eb7beba6b65d27825.\n",
            "\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from data/output/arxiv_org_11.txt...vLLM STDOUT: INFO:     127.0.0.1:59498 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:42:59 [logger.py:43] Received request chatcmpl-94db81591bb14178b49da053ac374e96: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\n0.0\\\\%}\\n10327↓18.5%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 18.5\\\\%}\\n0.64↓36.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 36.0\\\\%}\\nAutoThink\\n92.8↑0.9%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 0.9\\\\%}\\n4261↓31.7%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 31.7\\\\%}\\n0.56↓44.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 44.0\\\\%}\\n58.1↓4.9%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 4.9\\\\%}\\n9898↓8.6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 8.6\\\\%}\\n0.89↓11.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 11.0\\\\%}\\n70.0↓2.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 2.8\\\\%}\\n4958↓12.4%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 12.4\\\\%}\\n0.40↓60.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 60.0\\\\%}\\n74.3↑0.7%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 0.7\\\\%}\\n10240↓19.2%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 19.2\\\\%}\\n0.67↓33.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 33.0\\\\%}\\nHiPO\\n93.6 ↑1.7%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 1.7\\\\%}\\n4090 ↓34.4%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 34.4\\\\%}\\n0.54 ↓45.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 45.8\\\\%}\\n60.1 ↓1.7%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 1.7\\\\%}\\n9367 ↓13.5%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 13.5\\\\%}\\n0.92↓−8.1%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle-8.1\\\\%}\\n72.2↑0.3%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 0.3\\\\%}\\n1338↓69.7%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 69.7\\\\%}\\n0.12↓88.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 88.0\\\\%}\\n78.4↑6.3%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 6.3\\\\%}\\n8842↓30.2%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 30.2\\\\%}\\n0.63↓36.5%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 36.5\\\\%}\\nTable 2: Based on Qwen3-8B, performance of different methods on multiple benchmarks. RatioT\\\\textbf{Ratio}_{\\\\textbf{T}} denotes the ratio of “Think-on” mode over the corresponding benchmark.\\nIn Table 2, we observe that training the model solely on Think-on data leads the model to engage in reasoning for problems of any difficulty. We use this baseline as a typical example of \"overthinking\" for comparison. After applying GRPO to the Cold-Start (on) model, there is a significant improvement in accuracy, with an average accuracy increase of 3.1%. However, this does not reduce the token length and thinking rate of the model. On the contrary, to achieve higher accuracy, the token length output by the model on simpler datasets increases significantly. When training the model on a dataset containing both Think-on and Think-off data, the accuracy of the resulting Cold-Start model improves by 4.0% compared to the Cold-Start(on) model, while the token length and thinking rate decrease by 10.8% and 22%, respectively. After applying the GRPO algorithm to the Cold-Start model, there is no significant change in performance. However, when applying our method HiPO to train the Cold-Start model, the accuracy improves by 6.2%, while the token length and thinking rate decrease dramatically by 30% and 39%, respectively. Moreover, experimental results show that our HiPO outperforms existing methods on both efficiency<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:42:59 [engine.py:317] Added request chatcmpl-94db81591bb14178b49da053ac374e96.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from data/output/arxiv_org_11.txt...vLLM STDOUT: INFO:     127.0.0.1:59500 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 13 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/arxiv_org_11_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/arxiv_org_11_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from data/output/arxiv_org_11.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/arxiv_org_11_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:48262 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:48272 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:43:08 [logger.py:43] Received request chatcmpl-6a94a4b552ce4c94b5b06d8bb573d481: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nin performance. However, when applying our method HiPO to train the Cold-Start model, the accuracy improves by 6.2%, while the token length and thinking rate decrease dramatically by 30% and 39%, respectively. Moreover, experimental results show that our HiPO outperforms existing methods on both efficiency and accuracy.\\n4.3 Ablation Study\\nFigure 3: Performance of different response selection strategies.\\nEffect of selecting the shortest response.\\nIn the data construction pipeline, we select the shortest response (Cold-Start (Shortest)) as the final sample.\\nTo analyze the effect of this strategy,\\nwe additionally propose two variants called (Cold-Start (Longest) and Cold-Start (Random)) by selecting the longest responses and randomly selecting the responses,\\nrespectively.\\nIn Figure\\xa03,\\nCold-Start (Shortest) shows an improvement in accuracy compared to both Cold-Start (Longest) and Cold-Start (Random), with a decrease in both the Thinking ratio and Token length.\\nTherefore,\\nwe adopt this Cold-Start (Shortest) strategy for the Cold-Start stage.\\nEffect of design strategies for Aijudge\\\\mathrm{A}^{\\\\text{judge}}_{i} and Aianswer\\\\mathrm{A}^{\\\\text{answer}}_{i}.\\nIn the reinforcement learning stage, first, we utilize the term mean\\u200b(𝐫Mi)−mean\\u200b(𝐫)\\\\text{mean}(\\\\mathbf{r}_{M_{i}})-\\\\text{mean}(\\\\mathbf{r}) to quantify the global advantage of the chosen mode over the full group average. Second, the local normalization based on the mode-specific mean and standard deviation is used for Aianswer\\\\mathrm{A}^{\\\\text{answer}}_{i}.\\nTo demonstrate the effect of these strategies, as shown in Table\\xa03, we design two variants (i.e., HiPO (w/o global adv) and HiPO (w/o local norm)). For HiPO (w/o global adv), we directly remove the global advantage for Aijudge\\\\mathrm{A}^{\\\\text{judge}}_{i}. For HiPO (w/o local norm), we just use the global normalization across the responses in a group.\\nIn Table\\xa03, we observe that HiPO achieves significant improvements in performance and efficiency when compared to these two variants.\\nAIME2024\\nLiveCodeBench\\nHumanEval\\nMethod\\nAcc\\nLength\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}\\nAcc\\nLength\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}\\nAcc\\nLength\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}\\nHiPO\\n87.50\\n15107\\n0.98\\n63.00\\n13558\\n0.82\\n90.2\\n776\\n0.12\\nHiPO (w/o global adv)\\n85.83↓1.9%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 1.9\\\\%}\\n18064↑19.6%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 19.6\\\\%}\\n1.00↑2.0%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 2.0\\\\%}\\n56.83↓9.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 9.8\\\\%}\\n14561↑7.4%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 7.4\\\\%}\\n0.86↑4.9%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 4.9\\\\%}\\n89.63↓4.9%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 4.9\\\\%}\\n1660↑114.9%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 114.9\\\\%}\\n0.27↑125.0%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 125.0\\\\%}\\nHiPO (w/o local norm)\\n85.00↓2.9%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 2.9\\\\%}\\n18268↑20.9%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 20.9\\\\%}\\n1.00↑2.0%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 2.0\\\\%}\\n58.37↓7.3%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 7.3\\\\%}\\n16029↑18.2%\\\\scriptstyle\\\\up<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:43:08 [engine.py:317] Added request chatcmpl-6a94a4b552ce4c94b5b06d8bb573d481.\n",
            "\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from data/output/arxiv_org_12.txt...vLLM STDOUT: INFO:     127.0.0.1:48278 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:43:10 [logger.py:43] Received request chatcmpl-0fd89ba63aaf48d6bbaa69593b99b181: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\n in performance. However, when applying our method HiPO to train the Cold-Start model, the accuracy improves by 6.2%, while the token length and thinking rate decrease dramatically by 30% and 39%, respectively. Moreover, experimental results show that our HiPO outperforms existing methods on both efficiency and accuracy.\\n4.3 Ablation Study\\nFigure 3: Performance of different response selection strategies.\\nEffect of selecting the shortest response.\\nIn the data construction pipeline, we select the shortest response (Cold-Start (Shortest)) as the final sample.\\nTo analyze the effect of this strategy,\\nwe additionally propose two variants called (Cold-Start (Longest) and Cold-Start (Random)) by selecting the longest responses and randomly selecting the responses,\\nrespectively.\\nIn Figure\\xa03,\\nCold-Start (Shortest) shows an improvement in accuracy compared to both Cold-Start (Longest) and Cold-Start (Random), with a decrease in both the Thinking ratio and Token length.\\nTherefore,\\nwe adopt this Cold-Start (Shortest) strategy for the Cold-Start stage.\\nEffect of design strategies for Aijudge\\\\mathrm{A}^{\\\\text{judge}}_{i} and Aianswer\\\\mathrm{A}^{\\\\text{answer}}_{i}.\\nIn the reinforcement learning stage, first, we utilize the term mean\\u200b(𝐫Mi)−mean\\u200b(𝐫)\\\\text{mean}(\\\\mathbf{r}_{M_{i}})-\\\\text{mean}(\\\\mathbf{r}) to quantify the global advantage of the chosen mode over the full group average. Second, the local normalization based on the mode-specific mean and standard deviation is used for Aianswer\\\\mathrm{A}^{\\\\text{answer}}_{i}.\\nTo demonstrate the effect of these strategies, as shown in Table\\xa03, we design two variants (i.e., HiPO (w/o global adv) and HiPO (w/o local norm)). For HiPO (w/o global adv), we directly remove the global advantage for Aijudge\\\\mathrm{A}^{\\\\text{judge}}_{i}. For HiPO (w/o local norm), we just use the global normalization across the responses in a group.\\nIn Table\\xa03, we observe that HiPO achieves significant improvements in performance and efficiency when compared to these two variants.\\nAIME2024\\nLiveCodeBench\\nHumanEval\\nMethod\\nAcc\\nLength\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}\\nAcc\\nLength\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}\\nAcc\\nLength\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}\\nHiPO\\n87.50\\n15107\\n0.98\\n63.00\\n13558\\n0.82\\n90.2\\n776\\n0.12\\nHiPO (w/o global adv)\\n85.83↓1.9%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 1.9\\\\%}\\n18064↑19.6%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 19.6\\\\%}\\n1.00↑2.0%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 2.0\\\\%}\\n56.83↓9.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 9.8\\\\%}\\n14561↑7.4%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 7.4\\\\%}\\n0.86↑4.9%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 4.9\\\\%}\\n89.63↓4.9%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 4.9\\\\%}\\n1660↑114.9%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 114.9\\\\%}\\n0.27↑125.0%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 125.0\\\\%}\\nHiPO (w/o local norm)\\n85.00↓2.9%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 2.9\\\\%}\\n18268↑20.9%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 20.9\\\\%}\\n1.00↑2.0%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 2.0\\\\%}\\n58.37↓7.3%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 7.3\\\\%}\\n16029↑18.2%\\\\scriptstyle\\\\up<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:43:10 [engine.py:317] Added request chatcmpl-0fd89ba63aaf48d6bbaa69593b99b181.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from data/output/arxiv_org_12.txt...vLLM STDOUT: INFO:     127.0.0.1:48280 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 14 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/arxiv_org_12_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/arxiv_org_12_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from data/output/arxiv_org_12.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/arxiv_org_12_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:53974 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:53980 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:43:19 [logger.py:43] Received request chatcmpl-e3efb3e60aea4bebb42d3ebc1b2c8550: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n20.9\\\\%}\\n1.00↑2.0%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 2.0\\\\%}\\n58.37↓7.3%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 7.3\\\\%}\\n16029↑18.2%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 18.2\\\\%}\\n0.88↑7.3%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 7.3\\\\%}\\n89.63↓0.6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 0.6\\\\%}\\n2052↑164.4%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 164.4\\\\%}\\n0.32↑166.7%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 166.7\\\\%}\\nTable 3: Performance of different design strategies on advantage functions.\\nFigure 4: Performance of different γ\\\\gamma values.\\nFigure 5: Performance of different rollout numbers and ω\\\\omega values.\\nEffect of different γ\\\\gamma values. Figure 4 shows that, when the value of γ\\\\gamma is set to 0.00, the reward for the judge token lacks information about the current response, resulting in lower model accuracy and higher token length. On the other hand, when γ\\\\gamma is set too high, the scales of the two terms (mean(𝐫off\\\\mathbf{r}_{\\\\text{off}}) - mean(𝐫\\\\mathbf{r})) and (ri−mean\\u200b(𝐫))(r_{i}-\\\\textit{mean}(\\\\mathbf{r})) become imbalanced, which leads to a decrease in model accuracy and an increase in token length.\\nEffect of different rollout numbers. Table 5 shows that, when the rollout number NN is set to 16, the model achieves better average performance, shorter token length, and lower think rate. We attribute this to the fact that this configuration provides sufficient data to explore diverse possibilities while avoiding excessive samples with redundant reasoning that dilute the training signal. As a result, the model focuses more on learning from higher-quality samples, leading to a more concise strategy with improved accuracy, reduced token length, and lower think rate.\\nEffect of different ω\\\\omega values. Table 5 shows that, setting ω\\\\omega to 0.01 provides a balanced trade-off between performance and efficiency. This configuration mitigates the overly conservative behavior seen at 0.0 while avoiding the overly aggressive behavior at higher settings, ultimately achieving the largest efficiency gains with minimal performance loss.\\n4.4 Further Analysis\\n(a) (a) Think-on and Think-off ratio in training. (b) Think-on ratio of different datasets.\\n(b) (a) Average token usage in RL training. (b) Token usage of different datasets.\\nAIME24\\nLiveCodeBench\\nHumanEval\\nMBPP\\nMethod\\nAcc\\nLength\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}\\nAcc\\nLength\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}\\nAcc\\nLength\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}\\nAcc\\nLength\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}\\nQwen3-1.7B\\nCold-Start (On)\\n63.3\\n24214\\n1.00\\n33.7\\n25616\\n1.00\\n77.4\\n4172\\n1.00\\n54.6\\n8587\\n1.00\\nCold-Start\\n65.0↑2.7%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 2.7\\\\%}\\n21039↓13.1%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 13.1\\\\%}\\n1.00−0.0%\\\\scriptstyle-{\\\\scriptstyle 0.0\\\\%}\\n37.4↑11.0%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 11.0\\\\%}\\n21364↓16.6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 16.6\\\\%}\\n0.98↓2.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 2.0\\\\%}\\n81.7↑5.2%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 5.2\\\\%}\\n3084↓26.1%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 26.1\\\\%}\\n0.39↓61.0<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:43:19 [engine.py:317] Added request chatcmpl-e3efb3e60aea4bebb42d3ebc1b2c8550.\n",
            "\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from data/output/arxiv_org_13.txt...vLLM STDOUT: INFO:     127.0.0.1:53984 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:43:22 [logger.py:43] Received request chatcmpl-00fe6db431654501ab2d6bcb1d772570: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\n 20.9\\\\%}\\n1.00↑2.0%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 2.0\\\\%}\\n58.37↓7.3%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 7.3\\\\%}\\n16029↑18.2%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 18.2\\\\%}\\n0.88↑7.3%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 7.3\\\\%}\\n89.63↓0.6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 0.6\\\\%}\\n2052↑164.4%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 164.4\\\\%}\\n0.32↑166.7%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 166.7\\\\%}\\nTable 3: Performance of different design strategies on advantage functions.\\nFigure 4: Performance of different γ\\\\gamma values.\\nFigure 5: Performance of different rollout numbers and ω\\\\omega values.\\nEffect of different γ\\\\gamma values. Figure 4 shows that, when the value of γ\\\\gamma is set to 0.00, the reward for the judge token lacks information about the current response, resulting in lower model accuracy and higher token length. On the other hand, when γ\\\\gamma is set too high, the scales of the two terms (mean(𝐫off\\\\mathbf{r}_{\\\\text{off}}) - mean(𝐫\\\\mathbf{r})) and (ri−mean\\u200b(𝐫))(r_{i}-\\\\textit{mean}(\\\\mathbf{r})) become imbalanced, which leads to a decrease in model accuracy and an increase in token length.\\nEffect of different rollout numbers. Table 5 shows that, when the rollout number NN is set to 16, the model achieves better average performance, shorter token length, and lower think rate. We attribute this to the fact that this configuration provides sufficient data to explore diverse possibilities while avoiding excessive samples with redundant reasoning that dilute the training signal. As a result, the model focuses more on learning from higher-quality samples, leading to a more concise strategy with improved accuracy, reduced token length, and lower think rate.\\nEffect of different ω\\\\omega values. Table 5 shows that, setting ω\\\\omega to 0.01 provides a balanced trade-off between performance and efficiency. This configuration mitigates the overly conservative behavior seen at 0.0 while avoiding the overly aggressive behavior at higher settings, ultimately achieving the largest efficiency gains with minimal performance loss.\\n4.4 Further Analysis\\n(a) (a) Think-on and Think-off ratio in training. (b) Think-on ratio of different datasets.\\n(b) (a) Average token usage in RL training. (b) Token usage of different datasets.\\nAIME24\\nLiveCodeBench\\nHumanEval\\nMBPP\\nMethod\\nAcc\\nLength\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}\\nAcc\\nLength\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}\\nAcc\\nLength\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}\\nAcc\\nLength\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}\\nQwen3-1.7B\\nCold-Start (On)\\n63.3\\n24214\\n1.00\\n33.7\\n25616\\n1.00\\n77.4\\n4172\\n1.00\\n54.6\\n8587\\n1.00\\nCold-Start\\n65.0↑2.7%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 2.7\\\\%}\\n21039↓13.1%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 13.1\\\\%}\\n1.00−0.0%\\\\scriptstyle-{\\\\scriptstyle 0.0\\\\%}\\n37.4↑11.0%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 11.0\\\\%}\\n21364↓16.6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 16.6\\\\%}\\n0.98↓2.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 2.0\\\\%}\\n81.7↑5.2%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 5.2\\\\%}\\n3084↓26.1%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 26.1\\\\%}\\n0.39↓61.0<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:43:22 [engine.py:317] Added request chatcmpl-00fe6db431654501ab2d6bcb1d772570.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from data/output/arxiv_org_13.txt...vLLM STDOUT: INFO:     127.0.0.1:53986 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 10 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/arxiv_org_13_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/arxiv_org_13_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from data/output/arxiv_org_13.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/arxiv_org_13_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:46200 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:46216 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:43:31 [logger.py:43] Received request chatcmpl-f28d65fd6fc84a78958054c369221fcf: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\ndownarrow{\\\\scriptstyle 2.0\\\\%}\\n81.7↑5.2%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 5.2\\\\%}\\n3084↓26.1%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 26.1\\\\%}\\n0.39↓61.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 61.0\\\\%}\\n54.4↓0.4%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 0.4\\\\%}\\n6398↓25.5%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 25.5\\\\%}\\n0.64↓36.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 36.0\\\\%}\\nHiPO\\n68.3↑7.9%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 7.9\\\\%}\\n17614↓27.3%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 27.3\\\\%}\\n0.98↓6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 6\\\\%}\\n44.3↑31.4%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 31.4\\\\%}\\n19358↓24.4%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 24.4\\\\%}\\n0.92↓8.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 8.0\\\\%}\\n86.0↑11.1%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 11.1\\\\%}\\n1973↓52.7%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 52.7\\\\%}\\n0.28↓62.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 62.0\\\\%}\\n62.8↑15.0%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 15.0\\\\%}\\n4330↓49.6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 49.6\\\\%}\\n0.47↓53.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 53.0\\\\%}\\nQwen3-32B\\nCold-Start (On)\\n81.7\\n19551\\n1.00\\n65.4\\n17885\\n1.00\\n87.8\\n4298\\n1.00\\n76.2\\n4753\\n1.00\\nCold-Start\\n85.0↑4.3%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 4.3\\\\%}\\n16542↓15.4%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 15.4\\\\%}\\n1.00−0.0%\\\\scriptstyle{\\\\scriptstyle-0.0\\\\%}\\n65.9↑0.8%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 0.8\\\\%}\\n14935↓16.5%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 16.5\\\\%}\\n0.87↓13.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 13.0\\\\%}\\n92.1↑4.9%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 4.9\\\\%}\\n2785↓35.2%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 35.2\\\\%}\\n0.47↓53.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 53.0\\\\%}\\n78.4↑2.2%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 2.2\\\\%}\\n3991↓16.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 16.0\\\\%}\\n0.51↓49.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 49.0\\\\%}\\nHiPO\\n88.3↑8.1%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 8.1\\\\%}\\n14873↓23.9%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 23.9\\\\%}\\n0.98↓2.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 2.0\\\\%}\\n68.5↑4.5%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 4.5\\\\%}\\n12721↓28.9%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 28.9\\\\%}\\n0.82↓18.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 18.0\\\\%}\\n92.7↑5.6%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 5.6\\\\%<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:43:31 [engine.py:317] Added request chatcmpl-f28d65fd6fc84a78958054c369221fcf.\n",
            "\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from data/output/arxiv_org_14.txt...vLLM STDOUT: INFO:     127.0.0.1:46224 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:43:38 [logger.py:43] Received request chatcmpl-b6bd8bfdd18146868919ac982c94a809: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\ndownarrow{\\\\scriptstyle 2.0\\\\%}\\n81.7↑5.2%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 5.2\\\\%}\\n3084↓26.1%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 26.1\\\\%}\\n0.39↓61.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 61.0\\\\%}\\n54.4↓0.4%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 0.4\\\\%}\\n6398↓25.5%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 25.5\\\\%}\\n0.64↓36.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 36.0\\\\%}\\nHiPO\\n68.3↑7.9%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 7.9\\\\%}\\n17614↓27.3%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 27.3\\\\%}\\n0.98↓6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 6\\\\%}\\n44.3↑31.4%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 31.4\\\\%}\\n19358↓24.4%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 24.4\\\\%}\\n0.92↓8.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 8.0\\\\%}\\n86.0↑11.1%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 11.1\\\\%}\\n1973↓52.7%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 52.7\\\\%}\\n0.28↓62.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 62.0\\\\%}\\n62.8↑15.0%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 15.0\\\\%}\\n4330↓49.6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 49.6\\\\%}\\n0.47↓53.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 53.0\\\\%}\\nQwen3-32B\\nCold-Start (On)\\n81.7\\n19551\\n1.00\\n65.4\\n17885\\n1.00\\n87.8\\n4298\\n1.00\\n76.2\\n4753\\n1.00\\nCold-Start\\n85.0↑4.3%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 4.3\\\\%}\\n16542↓15.4%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 15.4\\\\%}\\n1.00−0.0%\\\\scriptstyle{\\\\scriptstyle-0.0\\\\%}\\n65.9↑0.8%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 0.8\\\\%}\\n14935↓16.5%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 16.5\\\\%}\\n0.87↓13.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 13.0\\\\%}\\n92.1↑4.9%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 4.9\\\\%}\\n2785↓35.2%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 35.2\\\\%}\\n0.47↓53.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 53.0\\\\%}\\n78.4↑2.2%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 2.2\\\\%}\\n3991↓16.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 16.0\\\\%}\\n0.51↓49.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 49.0\\\\%}\\nHiPO\\n88.3↑8.1%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 8.1\\\\%}\\n14873↓23.9%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 23.9\\\\%}\\n0.98↓2.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 2.0\\\\%}\\n68.5↑4.5%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 4.5\\\\%}\\n12721↓28.9%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 28.9\\\\%}\\n0.82↓18.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 18.0\\\\%}\\n92.7↑5.6%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 5.6\\\\%<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:43:38 [engine.py:317] Added request chatcmpl-b6bd8bfdd18146868919ac982c94a809.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from data/output/arxiv_org_14.txt...vLLM STDOUT: INFO:     127.0.0.1:46032 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 17 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/arxiv_org_14_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/arxiv_org_14_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from data/output/arxiv_org_14.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/arxiv_org_14_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:40200 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:40210 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:43:47 [logger.py:43] Received request chatcmpl-de9f03345df34e0f9bd0b6f6eba2771f: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n28.9%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 28.9\\\\%}\\n0.82↓18.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 18.0\\\\%}\\n92.7↑5.6%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 5.6\\\\%}\\n824↓80.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 80.8\\\\%}\\n0.18↓82.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 82.0\\\\%}\\n84.4↑10.8%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 10.8\\\\%}\\n2070↓56.4%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 56.4\\\\%}\\n0.24↓76.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 76.0\\\\%}\\nTable 4: Performance of HiPO on more models.\\nWe analyze two key dimensions: (i) reasoning-mode activation (<think_on> vs. <think_off>) and (ii) token efficiency across RL training steps and benchmark tasks.\\nSpecifically, during the training and evaluation processes, we track how the model’s decision-making evolves by monitoring the frequency of reasoning-mode activations and the corresponding output length.\\nThink-on vs. Think-off Dynamics During Training and Inference\\nWe logged the frequency of <think_on> and <think_off> activations at each step. As shown in Figure\\xa0LABEL:fig:Training_Ratio, HiPO not only improves final accuracy but also sharpens the model’s gating behavior, allowing it to skip unnecessary reasoning. Specifically, the gap between <think_on> and <think_off> activations decreases from 89.5% at the beginning of training to 53.1% by the end. In Figure LABEL:fig:Testing_Ratio shows the proportion of Think-on activations across different datasets during inference. Reasoning-intensive tasks, including AIME2024, and LiveCodeBench, consistently demonstrate high Think-on activation rates (>70%) throughout training. Conversely, tasks that require less explicit reasoning, such as HumanEval — exhibit a clear downward trend in Think-on activation as training progresses.\\nToken Count Dynamics During Training and Inference\\nDuring RL training, the average token count shows a consistent downward trend in Figure\\xa0LABEL:fig:Training_Token, which indicates that the model gradually learns to produce more concise responses and highlight the HiPO reward design in encouraging efficient token usage\\nBesides, Figure LABEL:fig:Testing_Token shows the corresponding dynamics in average token counts per generated response during inference, and we also observe consistent token reduction in training.\\nGeneralization on More Models\\nIn Table\\xa04, we report the performance of HiPO on Qwen3-1.7B and Qwen3-32B, which shows consistent improvements on both accuracy and efficiency.\\n5 Conclusion\\nIn this work, we introduced HiPO, a hybrid framework for adaptive reasoning in LLMs. By combining a hybrid data pipeline with a hybrid reinforcement learning reward system, HiPO enables models to dynamically balance Think-on and Think-off reasoning, mitigating the issue of overthinking while preserving accuracy. Experiments demonstrate that HiPO achieves competitive or superior accuracy with significantly improved token efficiency and reduced reasoning redundancy.\\nReferences\\nYao et\\xa0al. [2023]\\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas\\xa0L. Griffiths, Yuan Cao, and Karthik Narasimhan.\\nTree of thoughts: Deliberate problem solving with large language models.\\narXiv preprint arXiv: 2305.10601, 2023.\\nWei et\\xa0al. [2023]\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed\\xa0Chi, Quoc Le, and Denny Zhou.\\nChain-of-thought prompting elicits reasoning in large language models, 2023.\\nURL https://arxiv.org/abs/2201.11903.\\nKumar et\\xa0al. [2025]\\nAbhinav Kumar, Jaechul Roh, Ali Naseh, Marzena Karpinska, Mohit Iyyer, Amir Houmansadr, and Eugene Bagdasarian.\\nOverthink: Slowdown attacks on reasoning llms, 2025.\\nURL https://arxiv.org/abs/2502.02542.\\nSui et\\xa0al<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:43:47 [engine.py:317] Added request chatcmpl-de9f03345df34e0f9bd0b6f6eba2771f.\n",
            "\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from data/output/arxiv_org_15.txt...vLLM STDOUT: INFO:     127.0.0.1:40222 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:43:49 [logger.py:43] Received request chatcmpl-8c8fffa9b291401e82ca4cb09f791854: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\n28.9%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 28.9\\\\%}\\n0.82↓18.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 18.0\\\\%}\\n92.7↑5.6%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 5.6\\\\%}\\n824↓80.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 80.8\\\\%}\\n0.18↓82.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 82.0\\\\%}\\n84.4↑10.8%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 10.8\\\\%}\\n2070↓56.4%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 56.4\\\\%}\\n0.24↓76.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 76.0\\\\%}\\nTable 4: Performance of HiPO on more models.\\nWe analyze two key dimensions: (i) reasoning-mode activation (<think_on> vs. <think_off>) and (ii) token efficiency across RL training steps and benchmark tasks.\\nSpecifically, during the training and evaluation processes, we track how the model’s decision-making evolves by monitoring the frequency of reasoning-mode activations and the corresponding output length.\\nThink-on vs. Think-off Dynamics During Training and Inference\\nWe logged the frequency of <think_on> and <think_off> activations at each step. As shown in Figure\\xa0LABEL:fig:Training_Ratio, HiPO not only improves final accuracy but also sharpens the model’s gating behavior, allowing it to skip unnecessary reasoning. Specifically, the gap between <think_on> and <think_off> activations decreases from 89.5% at the beginning of training to 53.1% by the end. In Figure LABEL:fig:Testing_Ratio shows the proportion of Think-on activations across different datasets during inference. Reasoning-intensive tasks, including AIME2024, and LiveCodeBench, consistently demonstrate high Think-on activation rates (>70%) throughout training. Conversely, tasks that require less explicit reasoning, such as HumanEval — exhibit a clear downward trend in Think-on activation as training progresses.\\nToken Count Dynamics During Training and Inference\\nDuring RL training, the average token count shows a consistent downward trend in Figure\\xa0LABEL:fig:Training_Token, which indicates that the model gradually learns to produce more concise responses and highlight the HiPO reward design in encouraging efficient token usage\\nBesides, Figure LABEL:fig:Testing_Token shows the corresponding dynamics in average token counts per generated response during inference, and we also observe consistent token reduction in training.\\nGeneralization on More Models\\nIn Table\\xa04, we report the performance of HiPO on Qwen3-1.7B and Qwen3-32B, which shows consistent improvements on both accuracy and efficiency.\\n5 Conclusion\\nIn this work, we introduced HiPO, a hybrid framework for adaptive reasoning in LLMs. By combining a hybrid data pipeline with a hybrid reinforcement learning reward system, HiPO enables models to dynamically balance Think-on and Think-off reasoning, mitigating the issue of overthinking while preserving accuracy. Experiments demonstrate that HiPO achieves competitive or superior accuracy with significantly improved token efficiency and reduced reasoning redundancy.\\nReferences\\nYao et\\xa0al. [2023]\\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas\\xa0L. Griffiths, Yuan Cao, and Karthik Narasimhan.\\nTree of thoughts: Deliberate problem solving with large language models.\\narXiv preprint arXiv: 2305.10601, 2023.\\nWei et\\xa0al. [2023]\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed\\xa0Chi, Quoc Le, and Denny Zhou.\\nChain-of-thought prompting elicits reasoning in large language models, 2023.\\nURL https://arxiv.org/abs/2201.11903.\\nKumar et\\xa0al. [2025]\\nAbhinav Kumar, Jaechul Roh, Ali Naseh, Marzena Karpinska, Mohit Iyyer, Amir Houmansadr, and Eugene Bagdasarian.\\nOverthink: Slowdown attacks on reasoning llms, 2025.\\nURL https://arxiv.org/abs/2502.02542.\\nSui et\\xa0al<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:43:49 [engine.py:317] Added request chatcmpl-8c8fffa9b291401e82ca4cb09f791854.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from data/output/arxiv_org_15.txt...vLLM STDOUT: INFO:     127.0.0.1:40236 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 13 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/arxiv_org_15_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/arxiv_org_15_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from data/output/arxiv_org_15.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/arxiv_org_15_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:50440 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:50452 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:43:58 [logger.py:43] Received request chatcmpl-3ff4aa1ab9c74b879174df0f921b628c: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nNaseh, Marzena Karpinska, Mohit Iyyer, Amir Houmansadr, and Eugene Bagdasarian.\\nOverthink: Slowdown attacks on reasoning llms, 2025.\\nURL https://arxiv.org/abs/2502.02542.\\nSui et\\xa0al. [2025]\\nYang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu, Andrew Wen, Shaochen Zhong, Na\\xa0Zou, Hanjie Chen, and Xia Hu.\\nStop overthinking: A survey on efficient reasoning for large language models, 2025.\\nURL https://arxiv.org/abs/2503.16419.\\nNayab et\\xa0al. [2025]\\nSania Nayab, Giulio Rossolini, Marco Simoni, Andrea Saracino, Giorgio Buttazzo, Nicolamaria Manes, and Fabrizio Giacomelli.\\nConcise thoughts: Impact of output length on llm reasoning and cost, 2025.\\nURL https://arxiv.org/abs/2407.19825.\\nAggarwal and Welleck [2025]\\nPranjal Aggarwal and Sean Welleck.\\nL1: Controlling how long a reasoning model thinks with reinforcement learning, 2025.\\nURL https://arxiv.org/abs/2503.04697.\\nArora and Zanette [2025]\\nDaman Arora and Andrea Zanette.\\nTraining language models to reason efficiently, 2025.\\nURL https://arxiv.org/abs/2502.04463.\\nHou et\\xa0al. [2025]\\nBairu Hou, Yang Zhang, Jiabao Ji, Yujian Liu, Kaizhi Qian, Jacob Andreas, and Shiyu Chang.\\nThinkprune: Pruning long chain-of-thought of llms via reinforcement learning, 2025.\\nURL https://arxiv.org/abs/2504.01296.\\nLuo et\\xa0al. [2025]\\nHaotian Luo, Li\\xa0Shen, Haiying He, Yibo Wang, Shiwei Liu, Wei Li, Naiqiang Tan, Xiaochun Cao, and Dacheng Tao.\\nO1-pruner: Length-harmonizing fine-tuning for o1-like reasoning pruning, 2025.\\nURL https://arxiv.org/abs/2501.12570.\\nShen et\\xa0al. [2025]\\nYi\\xa0Shen, Jian Zhang, Jieyun Huang, Shuming Shi, Wenjing Zhang, Jiangze Yan, Ning Wang, Kai Wang, Zhaoxiang Liu, and Shiguo Lian.\\nDast: Difficulty-adaptive slow-thinking for large reasoning models, 2025.\\nURL https://arxiv.org/abs/2503.04472.\\nTeam et\\xa0al. [2025]\\nKimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, Chuning Tang, Congcong Wang, Dehao Zhang, Enming Yuan, Enzhe Lu, Fengxiang Tang, Flood Sung, Guangda Wei, Guokun Lai, Haiqing Guo, Han Zhu, Hao Ding, Hao Hu, Hao Yang, Hao Zhang, Haotian Yao, Haotian Zhao, Haoyu Lu, Haoze Li, Haozhen Yu, Hongcheng Gao, Huabin Zheng, Huan Yuan, Jia Chen, Jianhang Guo, Jianlin Su, Jianzhou Wang, Jie Zhao, Jin Zhang, Jingyuan Liu, Junjie Yan, Junyan Wu, Lidong Shi, Ling Ye, Longhui Yu, Mengnan Dong, Neo Zhang, Ningchen Ma, Qiwei Pan, Qucheng Gong, Shaowei Liu, Shengling Ma, Shupeng Wei, Sihan Cao, Siying Huang, Tao Jiang, Weihao Gao, Weimin Xiong, Weiran He, Weixiao Huang, Weixin Xu, Wenhao Wu, Wenyang He, Xianghui Wei, Xianqing Jia, Xingzhe Wu, Xinran Xu, Xinxing Zu,<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:43:58 [engine.py:317] Added request chatcmpl-3ff4aa1ab9c74b879174df0f921b628c.\n",
            "\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from data/output/arxiv_org_16.txt...vLLM STDOUT: INFO:     127.0.0.1:50466 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:44:00 [logger.py:43] Received request chatcmpl-e74ed78c3841460eb28f8f8736a6ba01: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\n Naseh, Marzena Karpinska, Mohit Iyyer, Amir Houmansadr, and Eugene Bagdasarian.\\nOverthink: Slowdown attacks on reasoning llms, 2025.\\nURL https://arxiv.org/abs/2502.02542.\\nSui et\\xa0al. [2025]\\nYang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu, Andrew Wen, Shaochen Zhong, Na\\xa0Zou, Hanjie Chen, and Xia Hu.\\nStop overthinking: A survey on efficient reasoning for large language models, 2025.\\nURL https://arxiv.org/abs/2503.16419.\\nNayab et\\xa0al. [2025]\\nSania Nayab, Giulio Rossolini, Marco Simoni, Andrea Saracino, Giorgio Buttazzo, Nicolamaria Manes, and Fabrizio Giacomelli.\\nConcise thoughts: Impact of output length on llm reasoning and cost, 2025.\\nURL https://arxiv.org/abs/2407.19825.\\nAggarwal and Welleck [2025]\\nPranjal Aggarwal and Sean Welleck.\\nL1: Controlling how long a reasoning model thinks with reinforcement learning, 2025.\\nURL https://arxiv.org/abs/2503.04697.\\nArora and Zanette [2025]\\nDaman Arora and Andrea Zanette.\\nTraining language models to reason efficiently, 2025.\\nURL https://arxiv.org/abs/2502.04463.\\nHou et\\xa0al. [2025]\\nBairu Hou, Yang Zhang, Jiabao Ji, Yujian Liu, Kaizhi Qian, Jacob Andreas, and Shiyu Chang.\\nThinkprune: Pruning long chain-of-thought of llms via reinforcement learning, 2025.\\nURL https://arxiv.org/abs/2504.01296.\\nLuo et\\xa0al. [2025]\\nHaotian Luo, Li\\xa0Shen, Haiying He, Yibo Wang, Shiwei Liu, Wei Li, Naiqiang Tan, Xiaochun Cao, and Dacheng Tao.\\nO1-pruner: Length-harmonizing fine-tuning for o1-like reasoning pruning, 2025.\\nURL https://arxiv.org/abs/2501.12570.\\nShen et\\xa0al. [2025]\\nYi\\xa0Shen, Jian Zhang, Jieyun Huang, Shuming Shi, Wenjing Zhang, Jiangze Yan, Ning Wang, Kai Wang, Zhaoxiang Liu, and Shiguo Lian.\\nDast: Difficulty-adaptive slow-thinking for large reasoning models, 2025.\\nURL https://arxiv.org/abs/2503.04472.\\nTeam et\\xa0al. [2025]\\nKimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, Chuning Tang, Congcong Wang, Dehao Zhang, Enming Yuan, Enzhe Lu, Fengxiang Tang, Flood Sung, Guangda Wei, Guokun Lai, Haiqing Guo, Han Zhu, Hao Ding, Hao Hu, Hao Yang, Hao Zhang, Haotian Yao, Haotian Zhao, Haoyu Lu, Haoze Li, Haozhen Yu, Hongcheng Gao, Huabin Zheng, Huan Yuan, Jia Chen, Jianhang Guo, Jianlin Su, Jianzhou Wang, Jie Zhao, Jin Zhang, Jingyuan Liu, Junjie Yan, Junyan Wu, Lidong Shi, Ling Ye, Longhui Yu, Mengnan Dong, Neo Zhang, Ningchen Ma, Qiwei Pan, Qucheng Gong, Shaowei Liu, Shengling Ma, Shupeng Wei, Sihan Cao, Siying Huang, Tao Jiang, Weihao Gao, Weimin Xiong, Weiran He, Weixiao Huang, Weixin Xu, Wenhao Wu, Wenyang He, Xianghui Wei, Xianqing Jia, Xingzhe Wu, Xinran Xu, Xinxing Zu,<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:44:00 [engine.py:317] Added request chatcmpl-e74ed78c3841460eb28f8f8736a6ba01.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from data/output/arxiv_org_16.txt...vLLM STDOUT: INFO:     127.0.0.1:50482 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 7 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/arxiv_org_16_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/arxiv_org_16_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from data/output/arxiv_org_16.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/arxiv_org_16_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:49280 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:49286 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:44:10 [logger.py:43] Received request chatcmpl-37818e915f6149a88c757c9a320afa98: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nJiang, Weihao Gao, Weimin Xiong, Weiran He, Weixiao Huang, Weixin Xu, Wenhao Wu, Wenyang He, Xianghui Wei, Xianqing Jia, Xingzhe Wu, Xinran Xu, Xinxing Zu, Xinyu Zhou, Xuehai Pan, Y.\\xa0Charles, Yang Li, Yangyang Hu, Yangyang Liu, Yanru Chen, Yejie Wang, Yibo Liu, Yidao Qin, Yifeng Liu, Ying Yang, Yiping Bao, Yulun Du, Yuxin Wu, Yuzhi Wang, Zaida\\nZhou, Zhaoji Wang, Zhaowei Li, Zhen Zhu, Zheng Zhang, Zhexu Wang, Zhilin Yang, Zhiqi Huang, Zihao Huang, Ziyao Xu, Zonghan Yang, and Zongyu Lin.\\nKimi k1.5: Scaling reinforcement learning with llms, 2025.\\nURL https://arxiv.org/abs/2501.12599.\\nLou et\\xa0al. [2025]\\nChenwei Lou, Zewei Sun, Xinnian Liang, Meng Qu, Wei Shen, Wenqi Wang, Yuntao Li, Qingping Yang, and Shuangzhi Wu.\\nAdacot: Pareto-optimal adaptive chain-of-thought triggering via reinforcement learning, 2025.\\nURL https://arxiv.org/abs/2505.11896.\\nMunkhdalai et\\xa0al. [2024]\\nTsendsuren Munkhdalai, Manaal Faruqui, and Siddharth Gopal.\\nLeave no context behind: Efficient infinite context transformers with infini-attention.\\narXiv preprint arXiv:2404.07143, 2024.\\nMa et\\xa0al. [2025]\\nXinyin Ma, Guangnian Wan, Runpeng Yu, Gongfan Fang, and Xinchao Wang.\\nCot-valve: Length-compressible chain-of-thought tuning, 2025.\\nURL https://arxiv.org/abs/2502.09601.\\nChen et\\xa0al. [2025a]\\nXingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, and Dong Yu.\\nDo not think that much for 2+3=? on the overthinking of o1-like llms, 2025a.\\nURL https://arxiv.org/abs/2412.21187.\\nKang et\\xa0al. [2025]\\nYu\\xa0Kang, Xianghui Sun, Liangyu Chen, and Wei Zou.\\nC3ot: generating shorter chain-of-thought without compromising effectiveness.\\nIn Proceedings of the Thirty-Ninth AAAI Conference on Artificial Intelligence and Thirty-Seventh Conference on Innovative Applications of Artificial Intelligence and Fifteenth Symposium on Educational Advances in Artificial Intelligence, AAAI’25/IAAI’25/EAAI’25. AAAI Press, 2025.\\nISBN 978-1-57735-897-8.\\ndoi: 10.1609/aaai.v39i23.34608.\\nURL https://doi.org/10.1609/aaai.v39i23.34608.\\nXu et\\xa0al. [2025]\\nSilei Xu, Wenhao Xie, Lingxiao Zhao, and Pengcheng He.\\nChain of draft: Thinking faster by writing less, 2025.\\nURL https://arxiv.org/abs/2502.18600.\\nRenze and Guven [2024]\\nMatthew Renze and Erhan Guven.\\nThe benefits of a concise chain of thought on problem-solving in large language models.\\nIn 2024 2nd International Conference on Foundation and Large Language Models (FLLM), page 476–483. IEEE, November 2024.\\ndoi: 10.1109/fllm63129.2024.10852493.\\nURL http://dx.doi.org/10.1109/FLLM63129.2024.10852493.\\nChen et\\xa0al. [2024]\\nQiguang Chen, Lib<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:44:10 [engine.py:317] Added request chatcmpl-37818e915f6149a88c757c9a320afa98.\n",
            "\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from data/output/arxiv_org_17.txt...vLLM STDOUT: INFO:     127.0.0.1:49290 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:44:12 [logger.py:43] Received request chatcmpl-e4a20e47ccd547d1a198282cc639e5aa: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\n Jiang, Weihao Gao, Weimin Xiong, Weiran He, Weixiao Huang, Weixin Xu, Wenhao Wu, Wenyang He, Xianghui Wei, Xianqing Jia, Xingzhe Wu, Xinran Xu, Xinxing Zu, Xinyu Zhou, Xuehai Pan, Y.\\xa0Charles, Yang Li, Yangyang Hu, Yangyang Liu, Yanru Chen, Yejie Wang, Yibo Liu, Yidao Qin, Yifeng Liu, Ying Yang, Yiping Bao, Yulun Du, Yuxin Wu, Yuzhi Wang, Zaida\\nZhou, Zhaoji Wang, Zhaowei Li, Zhen Zhu, Zheng Zhang, Zhexu Wang, Zhilin Yang, Zhiqi Huang, Zihao Huang, Ziyao Xu, Zonghan Yang, and Zongyu Lin.\\nKimi k1.5: Scaling reinforcement learning with llms, 2025.\\nURL https://arxiv.org/abs/2501.12599.\\nLou et\\xa0al. [2025]\\nChenwei Lou, Zewei Sun, Xinnian Liang, Meng Qu, Wei Shen, Wenqi Wang, Yuntao Li, Qingping Yang, and Shuangzhi Wu.\\nAdacot: Pareto-optimal adaptive chain-of-thought triggering via reinforcement learning, 2025.\\nURL https://arxiv.org/abs/2505.11896.\\nMunkhdalai et\\xa0al. [2024]\\nTsendsuren Munkhdalai, Manaal Faruqui, and Siddharth Gopal.\\nLeave no context behind: Efficient infinite context transformers with infini-attention.\\narXiv preprint arXiv:2404.07143, 2024.\\nMa et\\xa0al. [2025]\\nXinyin Ma, Guangnian Wan, Runpeng Yu, Gongfan Fang, and Xinchao Wang.\\nCot-valve: Length-compressible chain-of-thought tuning, 2025.\\nURL https://arxiv.org/abs/2502.09601.\\nChen et\\xa0al. [2025a]\\nXingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, and Dong Yu.\\nDo not think that much for 2+3=? on the overthinking of o1-like llms, 2025a.\\nURL https://arxiv.org/abs/2412.21187.\\nKang et\\xa0al. [2025]\\nYu\\xa0Kang, Xianghui Sun, Liangyu Chen, and Wei Zou.\\nC3ot: generating shorter chain-of-thought without compromising effectiveness.\\nIn Proceedings of the Thirty-Ninth AAAI Conference on Artificial Intelligence and Thirty-Seventh Conference on Innovative Applications of Artificial Intelligence and Fifteenth Symposium on Educational Advances in Artificial Intelligence, AAAI’25/IAAI’25/EAAI’25. AAAI Press, 2025.\\nISBN 978-1-57735-897-8.\\ndoi: 10.1609/aaai.v39i23.34608.\\nURL https://doi.org/10.1609/aaai.v39i23.34608.\\nXu et\\xa0al. [2025]\\nSilei Xu, Wenhao Xie, Lingxiao Zhao, and Pengcheng He.\\nChain of draft: Thinking faster by writing less, 2025.\\nURL https://arxiv.org/abs/2502.18600.\\nRenze and Guven [2024]\\nMatthew Renze and Erhan Guven.\\nThe benefits of a concise chain of thought on problem-solving in large language models.\\nIn 2024 2nd International Conference on Foundation and Large Language Models (FLLM), page 476–483. IEEE, November 2024.\\ndoi: 10.1109/fllm63129.2024.10852493.\\nURL http://dx.doi.org/10.1109/FLLM63129.2024.10852493.\\nChen et\\xa0al. [2024]\\nQiguang Chen, Lib<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:44:12 [engine.py:317] Added request chatcmpl-e4a20e47ccd547d1a198282cc639e5aa.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from data/output/arxiv_org_17.txt...vLLM STDOUT: INFO:     127.0.0.1:49306 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 6 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/arxiv_org_17_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/arxiv_org_17_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from data/output/arxiv_org_17.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/arxiv_org_17_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:58688 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:58694 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:44:21 [logger.py:43] Received request chatcmpl-40f44be8e3db4860afae15607191b9bf: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n2024.\\ndoi: 10.1109/fllm63129.2024.10852493.\\nURL http://dx.doi.org/10.1109/FLLM63129.2024.10852493.\\nChen et\\xa0al. [2024]\\nQiguang Chen, Libo Qin, Jiaqi Wang, Jinxuan Zhou, and Wanxiang Che.\\nUnlocking the capabilities of thought: A reasoning boundary framework to quantify and optimize chain-of-thought, 2024.\\nURL https://arxiv.org/abs/2410.05695.\\nMunkhbat et\\xa0al. [2025]\\nTergel Munkhbat, Namgyu Ho, Seo\\xa0Hyun Kim, Yongjin Yang, Yujin Kim, and Se-Young Yun.\\nSelf-training elicits concise reasoning in large language models, 2025.\\nURL https://arxiv.org/abs/2502.20122.\\nLiu et\\xa0al. [2024a]\\nAixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et\\xa0al.\\nDeepseek-v3 technical report.\\narXiv preprint arXiv:2412.19437, 2024a.\\nShao et\\xa0al. [2024]\\nZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y.\\xa0K. Li, Y.\\xa0Wu, and Daya Guo.\\nDeepseekmath: Pushing the limits of mathematical reasoning in open language models.\\narXiv preprint arXiv: 2402.03300, 2024.\\nURL https://arxiv.org/abs/2402.03300v3.\\nZheng et\\xa0al. [2025]\\nChujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An\\xa0Yang, Jingren Zhou, and Junyang Lin.\\nGroup sequence policy optimization, 2025.\\nURL https://arxiv.org/abs/2507.18071.\\nYue et\\xa0al. [2025]\\nYu\\xa0Yue, Yufeng Yuan, Qiying Yu, Xiaochen Zuo, Ruofei Zhu, Wenyuan Xu, Jiaze Chen, Chengyi Wang, TianTian Fan, Zhengyin Du, Xiangpeng Wei, Xiangyu Yu, Gaohong Liu, Juncai Liu, Lingjun Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Ru\\xa0Zhang, Xin Liu, Mingxuan Wang, Yonghui Wu, and Lin Yan.\\nVapo: Efficient and reliable reinforcement learning for advanced reasoning tasks, 2025.\\nURL https://arxiv.org/abs/2504.05118.\\nSchulman et\\xa0al. [2017]\\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.\\nProximal policy optimization algorithms, 2017.\\nURL https://arxiv.org/abs/1707.06347.\\nRafailov et\\xa0al. [2024]\\nRafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher\\xa0D. Manning, and Chelsea Finn.\\nDirect preference optimization: Your language model is secretly a reward model, 2024.\\nURL https://arxiv.org/abs/2305.18290.\\nDeepSeek-AI et\\xa0al. [2025]\\nDeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu\\xa0Wu, Z.\\xa0F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:44:21 [engine.py:317] Added request chatcmpl-40f44be8e3db4860afae15607191b9bf.\n",
            "\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from data/output/arxiv_org_18.txt...vLLM STDOUT: INFO:     127.0.0.1:58704 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:44:24 [logger.py:43] Received request chatcmpl-beb230888395448ea158d9d4061fe801: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\n2024.\\ndoi: 10.1109/fllm63129.2024.10852493.\\nURL http://dx.doi.org/10.1109/FLLM63129.2024.10852493.\\nChen et\\xa0al. [2024]\\nQiguang Chen, Libo Qin, Jiaqi Wang, Jinxuan Zhou, and Wanxiang Che.\\nUnlocking the capabilities of thought: A reasoning boundary framework to quantify and optimize chain-of-thought, 2024.\\nURL https://arxiv.org/abs/2410.05695.\\nMunkhbat et\\xa0al. [2025]\\nTergel Munkhbat, Namgyu Ho, Seo\\xa0Hyun Kim, Yongjin Yang, Yujin Kim, and Se-Young Yun.\\nSelf-training elicits concise reasoning in large language models, 2025.\\nURL https://arxiv.org/abs/2502.20122.\\nLiu et\\xa0al. [2024a]\\nAixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et\\xa0al.\\nDeepseek-v3 technical report.\\narXiv preprint arXiv:2412.19437, 2024a.\\nShao et\\xa0al. [2024]\\nZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y.\\xa0K. Li, Y.\\xa0Wu, and Daya Guo.\\nDeepseekmath: Pushing the limits of mathematical reasoning in open language models.\\narXiv preprint arXiv: 2402.03300, 2024.\\nURL https://arxiv.org/abs/2402.03300v3.\\nZheng et\\xa0al. [2025]\\nChujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An\\xa0Yang, Jingren Zhou, and Junyang Lin.\\nGroup sequence policy optimization, 2025.\\nURL https://arxiv.org/abs/2507.18071.\\nYue et\\xa0al. [2025]\\nYu\\xa0Yue, Yufeng Yuan, Qiying Yu, Xiaochen Zuo, Ruofei Zhu, Wenyuan Xu, Jiaze Chen, Chengyi Wang, TianTian Fan, Zhengyin Du, Xiangpeng Wei, Xiangyu Yu, Gaohong Liu, Juncai Liu, Lingjun Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Ru\\xa0Zhang, Xin Liu, Mingxuan Wang, Yonghui Wu, and Lin Yan.\\nVapo: Efficient and reliable reinforcement learning for advanced reasoning tasks, 2025.\\nURL https://arxiv.org/abs/2504.05118.\\nSchulman et\\xa0al. [2017]\\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.\\nProximal policy optimization algorithms, 2017.\\nURL https://arxiv.org/abs/1707.06347.\\nRafailov et\\xa0al. [2024]\\nRafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher\\xa0D. Manning, and Chelsea Finn.\\nDirect preference optimization: Your language model is secretly a reward model, 2024.\\nURL https://arxiv.org/abs/2305.18290.\\nDeepSeek-AI et\\xa0al. [2025]\\nDeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu\\xa0Wu, Z.\\xa0F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:44:24 [engine.py:317] Added request chatcmpl-beb230888395448ea158d9d4061fe801.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from data/output/arxiv_org_18.txt...vLLM STDOUT: INFO:     127.0.0.1:44712 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 11 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/arxiv_org_18_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/arxiv_org_18_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from data/output/arxiv_org_18.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/arxiv_org_18_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:44716 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:44718 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:44:33 [logger.py:43] Received request chatcmpl-bbc1cbadba1141f6a3f4e382373c0d2d: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nXiao Bi, Xiaokang Zhang, Xingkai Yu, Yu\\xa0Wu, Z.\\xa0F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H.\\xa0Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J.\\xa0L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong\\nZhang, Ruizhe Pan, Runji Wang, R.\\xa0J. Chen, R.\\xa0L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S.\\xa0S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T.\\xa0Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W.\\xa0L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X.\\xa0Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y.\\xa0K. Li, Y.\\xa0Q. Wang, Y.\\xa0X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi\\xa0Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y.\\xa0X. Zhu,\\nYanhong Xu, Yanping Huang, Yaohui Li, Yi\\xa0Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z.\\xa0Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang.\\nDeepseek-r1: Incentivizing reasoning capability in llms via reinforcement<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:44:33 [engine.py:317] Added request chatcmpl-bbc1cbadba1141f6a3f4e382373c0d2d.\n",
            "\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from data/output/arxiv_org_19.txt...vLLM STDOUT: INFO:     127.0.0.1:44722 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from data/output/arxiv_org_19.txt...vLLM STDOUT: INFO 10-03 08:44:36 [logger.py:43] Received request chatcmpl-b8e8ec6b6f574ee9b7b32a0efcf76350: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\n Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu\\xa0Wu, Z.\\xa0F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H.\\xa0Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J.\\xa0L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong\\nZhang, Ruizhe Pan, Runji Wang, R.\\xa0J. Chen, R.\\xa0L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S.\\xa0S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T.\\xa0Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W.\\xa0L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X.\\xa0Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y.\\xa0K. Li, Y.\\xa0Q. Wang, Y.\\xa0X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi\\xa0Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y.\\xa0X. Zhu,\\nYanhong Xu, Yanping Huang, Yaohui Li, Yi\\xa0Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z.\\xa0Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang.\\nDeepseek-r1: Incentivizing reasoning capability in llms via reinforcement<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:44:36 [engine.py:317] Added request chatcmpl-b8e8ec6b6f574ee9b7b32a0efcf76350.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from data/output/arxiv_org_19.txt...vLLM STDOUT: INFO:     127.0.0.1:45206 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 0 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/arxiv_org_19_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/arxiv_org_19_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from data/output/arxiv_org_19.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/arxiv_org_19_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:43480 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:43488 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:44:45 [logger.py:43] Received request chatcmpl-151e3197853f4bd5927f4491e3dd6dc9: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang.\\nDeepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025.\\nURL https://arxiv.org/abs/2501.12948.\\nZhan et\\xa0al. [2025]\\nZizheng Zhan, Ken Deng, Huaixi Tang, Wen Xiang, Kun Wu, Weihao Li, Wenqiang Zhu, Jingxuan Xu, Lecheng Huang, Zongxian Feng, Shaojie Wang, Shangpeng Yan, Xuxing Chen, Jiaheng Liu, Zhongyuan Peng, Zuchen Gao, Haoyang Huang, Xiaojiang Zhang, Jinghui Wang, Zheng Lin, Mengtong Li, Huiming Wang, Ziqi Zhan, Yanan Wu, Yuanxing Zhang, Jian Yang, Guang Chen, Haotian Zhang, Bin Chen, and Bing Yu.\\nKat-v1: Kwai-autothink technical report, 2025.\\nURL https://arxiv.org/abs/2507.08297.\\nAytes et\\xa0al. [2025]\\nSimon\\xa0A. Aytes, Jinheon Baek, and Sung\\xa0Ju Hwang.\\nSketch-of-thought: Efficient llm reasoning with adaptive cognitive-inspired sketching, 2025.\\nURL https://arxiv.org/abs/2503.05179.\\nXia et\\xa0al. [2025]\\nHeming Xia, Chak\\xa0Tou Leong, Wenjie Wang, Yongqi Li, and Wenjie Li.\\nTokenskip: Controllable chain-of-thought compression in llms, 2025.\\nURL https://arxiv.org/abs/2502.12067.\\nLiu et\\xa0al. [2024b]\\nTengxiao Liu, Qipeng Guo, Xiangkun Hu, Cheng Jiayang, Yue Zhang, Xipeng Qiu, and Zheng Zhang.\\nCan language models learn to skip steps?, 2024b.\\nURL https://arxiv.org/abs/2411.01855.\\nSun et\\xa0al. [2024]\\nHanshi Sun, Momin Haider, Ruiqi Zhang, Huitao Yang, Jiahao Qiu, Ming Yin, Mengdi Wang, Peter Bartlett, and Andrea Zanette.\\nFast best-of-n decoding via speculative rejection, 2024.\\nURL https://arxiv.org/abs/2410.20290.\\nYang et\\xa0al. [2025]\\nChenxu Yang, Qingyi Si, Yongjie Duan, Zheliang Zhu, Chenyu Zhu, Qiaowei Li, Zheng Lin, Li\\xa0Cao, and Weiping Wang.\\nDynamic early exit in reasoning models, 2025.\\nURL https://arxiv.org/abs/2504.15895.\\nTian et\\xa0al. [2025]\\nXiaoyu Tian, Yunjie Ji, Haotian Wang, Shuaiting Chen, Sitong Zhao, Yiping Peng, Han Zhao, and Xiangang Li.\\nNot all correct answers are equal: Why your distillation source matters.\\narXiv preprint arXiv:2505.14464, 2025.\\nInternet [2025]\\nIntelligent Internet.\\nIi-thought : A large-scale, high-quality reasoning dataset, 2025.\\nChen et\\xa0al. [2025b]\\nYang Chen, Zhuolin Yang, Zihan Liu, Chankyu Lee, Peng Xu, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping.\\nAcereason-nemotron: Advancing math and code reasoning through reinforcement learning.\\narXiv preprint arXiv:2505.16400, 2025b.\\nHe et\\xa0al. [2025]\\nJujie He, Jiacai Liu, Chris\\xa0Yuhao Liu, Rui Yan, Chaojie Wang, Peng Cheng, Xiaoyu Zhang, Fuxiang Zhang, Jiacheng Xu, Wei Shen, Siyuan Li, Liang Zeng, Tianwen Wei, Cheng Cheng, Yang Liu, and Yahui Zhou.\\nSkywork open reasoner series.\\nhttps://capricious-hydrogen<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:44:45 [engine.py:317] Added request chatcmpl-151e3197853f4bd5927f4491e3dd6dc9.\n",
            "\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from data/output/arxiv_org_20.txt...vLLM STDOUT: INFO:     127.0.0.1:43498 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from data/output/arxiv_org_20.txt...vLLM STDOUT: INFO 10-03 08:44:49 [logger.py:43] Received request chatcmpl-70fe558f61ce46fbb1a589ba2578f54f: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang.\\nDeepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025.\\nURL https://arxiv.org/abs/2501.12948.\\nZhan et\\xa0al. [2025]\\nZizheng Zhan, Ken Deng, Huaixi Tang, Wen Xiang, Kun Wu, Weihao Li, Wenqiang Zhu, Jingxuan Xu, Lecheng Huang, Zongxian Feng, Shaojie Wang, Shangpeng Yan, Xuxing Chen, Jiaheng Liu, Zhongyuan Peng, Zuchen Gao, Haoyang Huang, Xiaojiang Zhang, Jinghui Wang, Zheng Lin, Mengtong Li, Huiming Wang, Ziqi Zhan, Yanan Wu, Yuanxing Zhang, Jian Yang, Guang Chen, Haotian Zhang, Bin Chen, and Bing Yu.\\nKat-v1: Kwai-autothink technical report, 2025.\\nURL https://arxiv.org/abs/2507.08297.\\nAytes et\\xa0al. [2025]\\nSimon\\xa0A. Aytes, Jinheon Baek, and Sung\\xa0Ju Hwang.\\nSketch-of-thought: Efficient llm reasoning with adaptive cognitive-inspired sketching, 2025.\\nURL https://arxiv.org/abs/2503.05179.\\nXia et\\xa0al. [2025]\\nHeming Xia, Chak\\xa0Tou Leong, Wenjie Wang, Yongqi Li, and Wenjie Li.\\nTokenskip: Controllable chain-of-thought compression in llms, 2025.\\nURL https://arxiv.org/abs/2502.12067.\\nLiu et\\xa0al. [2024b]\\nTengxiao Liu, Qipeng Guo, Xiangkun Hu, Cheng Jiayang, Yue Zhang, Xipeng Qiu, and Zheng Zhang.\\nCan language models learn to skip steps?, 2024b.\\nURL https://arxiv.org/abs/2411.01855.\\nSun et\\xa0al. [2024]\\nHanshi Sun, Momin Haider, Ruiqi Zhang, Huitao Yang, Jiahao Qiu, Ming Yin, Mengdi Wang, Peter Bartlett, and Andrea Zanette.\\nFast best-of-n decoding via speculative rejection, 2024.\\nURL https://arxiv.org/abs/2410.20290.\\nYang et\\xa0al. [2025]\\nChenxu Yang, Qingyi Si, Yongjie Duan, Zheliang Zhu, Chenyu Zhu, Qiaowei Li, Zheng Lin, Li\\xa0Cao, and Weiping Wang.\\nDynamic early exit in reasoning models, 2025.\\nURL https://arxiv.org/abs/2504.15895.\\nTian et\\xa0al. [2025]\\nXiaoyu Tian, Yunjie Ji, Haotian Wang, Shuaiting Chen, Sitong Zhao, Yiping Peng, Han Zhao, and Xiangang Li.\\nNot all correct answers are equal: Why your distillation source matters.\\narXiv preprint arXiv:2505.14464, 2025.\\nInternet [2025]\\nIntelligent Internet.\\nIi-thought : A large-scale, high-quality reasoning dataset, 2025.\\nChen et\\xa0al. [2025b]\\nYang Chen, Zhuolin Yang, Zihan Liu, Chankyu Lee, Peng Xu, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping.\\nAcereason-nemotron: Advancing math and code reasoning through reinforcement learning.\\narXiv preprint arXiv:2505.16400, 2025b.\\nHe et\\xa0al. [2025]\\nJujie He, Jiacai Liu, Chris\\xa0Yuhao Liu, Rui Yan, Chaojie Wang, Peng Cheng, Xiaoyu Zhang, Fuxiang Zhang, Jiacheng Xu, Wei Shen, Siyuan Li, Liang Zeng, Tianwen Wei, Cheng Cheng, Yang Liu, and Yahui Zhou.\\nSkywork open reasoner series.\\nhttps://capricious-hydrogen<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:44:49 [engine.py:317] Added request chatcmpl-70fe558f61ce46fbb1a589ba2578f54f.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from data/output/arxiv_org_20.txt...vLLM STDOUT: INFO:     127.0.0.1:43504 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 9 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/arxiv_org_20_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/arxiv_org_20_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from data/output/arxiv_org_20.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/arxiv_org_20_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:57454 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:57466 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:44:58 [logger.py:43] Received request chatcmpl-73dd6b813f4c4ab7ba6afd48bd09987b: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\njie Wang, Peng Cheng, Xiaoyu Zhang, Fuxiang Zhang, Jiacheng Xu, Wei Shen, Siyuan Li, Liang Zeng, Tianwen Wei, Cheng Cheng, Yang Liu, and Yahui Zhou.\\nSkywork open reasoner series.\\nhttps://capricious-hydrogen-41c.notion.site/Skywork-Open-Reaonser-Series-1d0bc9ae823a80459b46c149e4f51680, 2025.\\nNotion Blog.\\nZhang et\\xa0al. [2025]\\nJiajie Zhang, Nianyi Lin, Lei Hou, Ling Feng, and Juanzi Li.\\nAdaptthink: Reasoning models can learn when to think, 2025.\\nURL https://arxiv.org/abs/2505.13417.\\nTu et\\xa0al. [2025]\\nSongjun Tu, Jiahao Lin, Qichao Zhang, Xiangyu Tian, Linjing Li, Xiangyuan Lan, and Dongbin Zhao.\\nLearning when to think: Shaping adaptive reasoning in r1-style models via multi-stage rl, 2025.\\nURL https://arxiv.org/abs/2505.10832.\\nChen et\\xa0al. [2021]\\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique\\xa0Ponde de\\xa0Oliveira\\xa0Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe\\xa0Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William\\xa0Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew\\xa0N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba.\\nEvaluating large language models trained on code, 2021.\\nURL https://arxiv.org/abs/2107.03374.\\nJain et\\xa0al. [2024]\\nNaman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica.\\nLivecodebench: Holistic and contamination free evaluation of large language models for code, 2024.\\nURL https://arxiv.org/abs/2403.07974.\\nAustin et\\xa0al. [2021]\\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton.\\nProgram synthesis with large language models, 2021.\\nURL https://arxiv.org/abs/2108.07732.\\nLightman et\\xa0al. [2023]\\nHunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe.\\nLet’s verify step by step, 2023.\\nURL https://arxiv.org/abs/2305.20050.\\nRein et\\xa0al. [2023]\\nDavid Rein, Betty\\xa0Li Hou, Asa\\xa0Cooper Stickland, Jackson Petty, Richard\\xa0Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel\\xa0R. Bowman.\\nGpqa: A graduate-level google-proof q&a benchmark.\\nArXiv, abs/2311.12022, 2023.\\nURL https://api.semanticscholar.org/CorpusID:265295009<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:44:58 [engine.py:317] Added request chatcmpl-73dd6b813f4c4ab7ba6afd48bd09987b.\n",
            "\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from data/output/arxiv_org_21.txt...vLLM STDOUT: INFO:     127.0.0.1:57470 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:45:00 [logger.py:43] Received request chatcmpl-22579e6ff37940d9a272cd11dd411350: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\njie Wang, Peng Cheng, Xiaoyu Zhang, Fuxiang Zhang, Jiacheng Xu, Wei Shen, Siyuan Li, Liang Zeng, Tianwen Wei, Cheng Cheng, Yang Liu, and Yahui Zhou.\\nSkywork open reasoner series.\\nhttps://capricious-hydrogen-41c.notion.site/Skywork-Open-Reaonser-Series-1d0bc9ae823a80459b46c149e4f51680, 2025.\\nNotion Blog.\\nZhang et\\xa0al. [2025]\\nJiajie Zhang, Nianyi Lin, Lei Hou, Ling Feng, and Juanzi Li.\\nAdaptthink: Reasoning models can learn when to think, 2025.\\nURL https://arxiv.org/abs/2505.13417.\\nTu et\\xa0al. [2025]\\nSongjun Tu, Jiahao Lin, Qichao Zhang, Xiangyu Tian, Linjing Li, Xiangyuan Lan, and Dongbin Zhao.\\nLearning when to think: Shaping adaptive reasoning in r1-style models via multi-stage rl, 2025.\\nURL https://arxiv.org/abs/2505.10832.\\nChen et\\xa0al. [2021]\\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique\\xa0Ponde de\\xa0Oliveira\\xa0Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe\\xa0Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William\\xa0Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew\\xa0N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba.\\nEvaluating large language models trained on code, 2021.\\nURL https://arxiv.org/abs/2107.03374.\\nJain et\\xa0al. [2024]\\nNaman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica.\\nLivecodebench: Holistic and contamination free evaluation of large language models for code, 2024.\\nURL https://arxiv.org/abs/2403.07974.\\nAustin et\\xa0al. [2021]\\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton.\\nProgram synthesis with large language models, 2021.\\nURL https://arxiv.org/abs/2108.07732.\\nLightman et\\xa0al. [2023]\\nHunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe.\\nLet’s verify step by step, 2023.\\nURL https://arxiv.org/abs/2305.20050.\\nRein et\\xa0al. [2023]\\nDavid Rein, Betty\\xa0Li Hou, Asa\\xa0Cooper Stickland, Jackson Petty, Richard\\xa0Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel\\xa0R. Bowman.\\nGpqa: A graduate-level google-proof q&a benchmark.\\nArXiv, abs/2311.12022, 2023.\\nURL https://api.semanticscholar.org/CorpusID:265295009<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:45:00 [engine.py:317] Added request chatcmpl-22579e6ff37940d9a272cd11dd411350.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from data/output/arxiv_org_21.txt...vLLM STDOUT: INFO:     127.0.0.1:57480 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 11 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/arxiv_org_21_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/arxiv_org_21_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from data/output/arxiv_org_21.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/arxiv_org_21_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:38056 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:38062 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:45:09 [logger.py:43] Received request chatcmpl-30b63ada5b3744e4b801136111a468e8: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nuanzhe Pang, Julien Dirani, Julian Michael, and Samuel\\xa0R. Bowman.\\nGpqa: A graduate-level google-proof q&a benchmark.\\nArXiv, abs/2311.12022, 2023.\\nURL https://api.semanticscholar.org/CorpusID:265295009.\\nAppendix A Appendix\\nA.1 Use of LLMs\\nLLMs were used solely to assist in editing, formatting, and improving the clarity of the manuscript.\\nAll ideas, experiments, and analyses were conceived and executed by the authors.\\nNo LLM outputs were used as experimental data or results in this work.\\nA.2 The decline in Qwen3’s performance on the test set.\\nThis section demonstrates the decline in Qwen3’s performance on AIME2024, AIME2025, HumanEval, and LiverCodeBench.\\nWe trained Qwen3 using AM-DeepSeek-R1-0528-Distilled, AM-Thinking-v1-Distilled, and OpenThoughts3-1.2M.\\nThe Figure 7, when the number of training steps reaches 150, Qwen3’s accuracy on all benchmarks declines.\\nNote that the batch size is set as 512 and other parameters are same as the implementation details in the main paper.\\nFigure 7: The decline in Qwen3’s performance on the AIME2024, AIME2025, HumanEval, LiveCodeBench.\\nA.3 Data Source\\nOur dataset is derived from several open-source reasoning corpora covering both code and mathematics.\\nAs shown in Table\\xa05, queries come from AM-Thinking-v1-Distilled\\xa0222https://huggingface.co/datasets/a-m-team/AM-Thinking-v1-Distilled, II-Thought-RL\\xa0333https://huggingface.co/datasets/Intelligent-Internet/II-Thought-RL-v0, AceReason-Math\\xa0444https://huggingface.co/datasets/nvidia/AceReason-Math, and Skywork-OR1-RL-Data\\xa0555https://huggingface.co/datasets/Skywork/Skywork-OR1-RL-Data.\\nThis composition ensures diversity across domains and provides a reliable basis for model training and evaluation.\\nCategory\\nData Source\\n# Query\\nCode\\nAM-Thinking-v1-Distilled\\xa0[Tian et\\xa0al., 2025]\\n85k\\nII-Thought-RL\\xa0[Internet, 2025]\\n20k\\nMath\\nAceReason-Math\\xa0[Chen et\\xa0al., 2025b]\\n49k\\nAM-Thinking-v1-Distilled\\xa0[Tian et\\xa0al., 2025]\\n32k\\nII-Thought-RL\\xa0[Internet, 2025]\\n30k\\nSkywork-OR1-RL-Data\\xa0[He et\\xa0al., 2025]\\n24k\\nTable 5: Description of data sources.\\nA.4 Prompt Templates\\nIn this section, we provide the prompt templates for the response generation and judge analysis generation.\\nResponse Generation\\nPlease read the following question carefully and provide a clear answer.\\n—\\nQuery\\n—\\nJudge Analysis Generation\\nYou are tasked with analyzing the characteristics of a question to determine why it **requires** complex reasoning.\\nYour should **not** attempting to answer or infer its solution.\\nYou should analyse user’s question to determine the **core task intention**—that is, what the user wants the model to do. (e.g., write and validate code based on a problem description, etc.).\\nThen briefly outline the basic approach to accomplishing this task (e.g., write SQL code to retrieve imformation, etc.).\\nBased on the required approach, assess the **reasoning complexity**, and indicate whether it involves multiple steps or deep analysis. Do not solve the question or provide an answer. Focus solely on interpreting the task type, approach, and cognitive demand.\\nBe concise: your analysis must be no more than two lines and under 500 characters. Use clear, natural, and varied language.\\nEnd your explanation with a statement indicating that complex reasoning is required (Think-on), but express this conclusion with a natural and diverse phrase, not repeating any single pattern. The meaning must be clear, but the expression can vary.\\nPlease analyze the following question as required above:\\n—\\nModel Response\\n—\\nGenerated\\non Sun Sep 28 16:41:38 2025 by LaTeXML<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:45:09 [engine.py:317] Added request chatcmpl-30b63ada5b3744e4b801136111a468e8.\n",
            "\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from data/output/arxiv_org_22.txt...vLLM STDOUT: INFO:     127.0.0.1:38070 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:45:11 [logger.py:43] Received request chatcmpl-7515748ae1ff4f669c8f9c02565512a1: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nuanzhe Pang, Julien Dirani, Julian Michael, and Samuel\\xa0R. Bowman.\\nGpqa: A graduate-level google-proof q&a benchmark.\\nArXiv, abs/2311.12022, 2023.\\nURL https://api.semanticscholar.org/CorpusID:265295009.\\nAppendix A Appendix\\nA.1 Use of LLMs\\nLLMs were used solely to assist in editing, formatting, and improving the clarity of the manuscript.\\nAll ideas, experiments, and analyses were conceived and executed by the authors.\\nNo LLM outputs were used as experimental data or results in this work.\\nA.2 The decline in Qwen3’s performance on the test set.\\nThis section demonstrates the decline in Qwen3’s performance on AIME2024, AIME2025, HumanEval, and LiverCodeBench.\\nWe trained Qwen3 using AM-DeepSeek-R1-0528-Distilled, AM-Thinking-v1-Distilled, and OpenThoughts3-1.2M.\\nThe Figure 7, when the number of training steps reaches 150, Qwen3’s accuracy on all benchmarks declines.\\nNote that the batch size is set as 512 and other parameters are same as the implementation details in the main paper.\\nFigure 7: The decline in Qwen3’s performance on the AIME2024, AIME2025, HumanEval, LiveCodeBench.\\nA.3 Data Source\\nOur dataset is derived from several open-source reasoning corpora covering both code and mathematics.\\nAs shown in Table\\xa05, queries come from AM-Thinking-v1-Distilled\\xa0222https://huggingface.co/datasets/a-m-team/AM-Thinking-v1-Distilled, II-Thought-RL\\xa0333https://huggingface.co/datasets/Intelligent-Internet/II-Thought-RL-v0, AceReason-Math\\xa0444https://huggingface.co/datasets/nvidia/AceReason-Math, and Skywork-OR1-RL-Data\\xa0555https://huggingface.co/datasets/Skywork/Skywork-OR1-RL-Data.\\nThis composition ensures diversity across domains and provides a reliable basis for model training and evaluation.\\nCategory\\nData Source\\n# Query\\nCode\\nAM-Thinking-v1-Distilled\\xa0[Tian et\\xa0al., 2025]\\n85k\\nII-Thought-RL\\xa0[Internet, 2025]\\n20k\\nMath\\nAceReason-Math\\xa0[Chen et\\xa0al., 2025b]\\n49k\\nAM-Thinking-v1-Distilled\\xa0[Tian et\\xa0al., 2025]\\n32k\\nII-Thought-RL\\xa0[Internet, 2025]\\n30k\\nSkywork-OR1-RL-Data\\xa0[He et\\xa0al., 2025]\\n24k\\nTable 5: Description of data sources.\\nA.4 Prompt Templates\\nIn this section, we provide the prompt templates for the response generation and judge analysis generation.\\nResponse Generation\\nPlease read the following question carefully and provide a clear answer.\\n—\\nQuery\\n—\\nJudge Analysis Generation\\nYou are tasked with analyzing the characteristics of a question to determine why it **requires** complex reasoning.\\nYour should **not** attempting to answer or infer its solution.\\nYou should analyse user’s question to determine the **core task intention**—that is, what the user wants the model to do. (e.g., write and validate code based on a problem description, etc.).\\nThen briefly outline the basic approach to accomplishing this task (e.g., write SQL code to retrieve imformation, etc.).\\nBased on the required approach, assess the **reasoning complexity**, and indicate whether it involves multiple steps or deep analysis. Do not solve the question or provide an answer. Focus solely on interpreting the task type, approach, and cognitive demand.\\nBe concise: your analysis must be no more than two lines and under 500 characters. Use clear, natural, and varied language.\\nEnd your explanation with a statement indicating that complex reasoning is required (Think-on), but express this conclusion with a natural and diverse phrase, not repeating any single pattern. The meaning must be clear, but the expression can vary.\\nPlease analyze the following question as required above:\\n—\\nModel Response\\n—\\nGenerated\\non Sun Sep 28 16:41:38 2025 by LaTeXML<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:45:11 [engine.py:317] Added request chatcmpl-7515748ae1ff4f669c8f9c02565512a1.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from data/output/arxiv_org_22.txt...vLLM STDOUT: INFO:     127.0.0.1:38074 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 17 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/arxiv_org_22_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/arxiv_org_22_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from data/output/arxiv_org_22.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/arxiv_org_22_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:54828 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:54838 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:45:20 [logger.py:43] Received request chatcmpl-15308e9fa27b4c0583ee941edc405513: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\njie Wang, Peng Cheng, Xiaoyu Zhang, Fuxiang Zhang, Jiacheng Xu, Wei Shen, Siyuan Li, Liang Zeng, Tianwen Wei, Cheng Cheng, Yang Liu, and Yahui Zhou.\\nSkywork open reasoner series.\\nhttps://capricious-hydrogen-41c.notion.site/Skywork-Open-Reaonser-Series-1d0bc9ae823a80459b46c149e4f51680, 2025.\\nNotion Blog.\\nZhang et\\xa0al. [2025]\\nJiajie Zhang, Nianyi Lin, Lei Hou, Ling Feng, and Juanzi Li.\\nAdaptthink: Reasoning models can learn when to think, 2025.\\nURL https://arxiv.org/abs/2505.13417.\\nTu et\\xa0al. [2025]\\nSongjun Tu, Jiahao Lin, Qichao Zhang, Xiangyu Tian, Linjing Li, Xiangyuan Lan, and Dongbin Zhao.\\nLearning when to think: Shaping adaptive reasoning in r1-style models via multi-stage rl, 2025.\\nURL https://arxiv.org/abs/2505.10832.\\nChen et\\xa0al. [2021]\\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique\\xa0Ponde de\\xa0Oliveira\\xa0Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe\\xa0Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William\\xa0Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew\\xa0N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba.\\nEvaluating large language models trained on code, 2021.\\nURL https://arxiv.org/abs/2107.03374.\\nJain et\\xa0al. [2024]\\nNaman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica.\\nLivecodebench: Holistic and contamination free evaluation of large language models for code, 2024.\\nURL https://arxiv.org/abs/2403.07974.\\nAustin et\\xa0al. [2021]\\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton.\\nProgram synthesis with large language models, 2021.\\nURL https://arxiv.org/abs/2108.07732.\\nLightman et\\xa0al. [2023]\\nHunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe.\\nLet’s verify step by step, 2023.\\nURL https://arxiv.org/abs/2305.20050.\\nRein et\\xa0al. [2023]\\nDavid Rein, Betty\\xa0Li Hou, Asa\\xa0Cooper Stickland, Jackson Petty, Richard\\xa0Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel\\xa0R. Bowman.\\nGpqa: A graduate-level google-proof q&a benchmark.\\nArXiv, abs/2311.12022, 2023.\\nURL https://api.semanticscholar.org/CorpusID:265295009<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:45:20 [engine.py:317] Added request chatcmpl-15308e9fa27b4c0583ee941edc405513.\n",
            "\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from data/output/arxiv_org_21_0_0.txt...vLLM STDOUT: INFO:     127.0.0.1:54842 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:45:22 [logger.py:43] Received request chatcmpl-e577df325cdc498baca6e27765c2cc28: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\njie Wang, Peng Cheng, Xiaoyu Zhang, Fuxiang Zhang, Jiacheng Xu, Wei Shen, Siyuan Li, Liang Zeng, Tianwen Wei, Cheng Cheng, Yang Liu, and Yahui Zhou.\\nSkywork open reasoner series.\\nhttps://capricious-hydrogen-41c.notion.site/Skywork-Open-Reaonser-Series-1d0bc9ae823a80459b46c149e4f51680, 2025.\\nNotion Blog.\\nZhang et\\xa0al. [2025]\\nJiajie Zhang, Nianyi Lin, Lei Hou, Ling Feng, and Juanzi Li.\\nAdaptthink: Reasoning models can learn when to think, 2025.\\nURL https://arxiv.org/abs/2505.13417.\\nTu et\\xa0al. [2025]\\nSongjun Tu, Jiahao Lin, Qichao Zhang, Xiangyu Tian, Linjing Li, Xiangyuan Lan, and Dongbin Zhao.\\nLearning when to think: Shaping adaptive reasoning in r1-style models via multi-stage rl, 2025.\\nURL https://arxiv.org/abs/2505.10832.\\nChen et\\xa0al. [2021]\\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique\\xa0Ponde de\\xa0Oliveira\\xa0Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe\\xa0Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William\\xa0Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew\\xa0N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba.\\nEvaluating large language models trained on code, 2021.\\nURL https://arxiv.org/abs/2107.03374.\\nJain et\\xa0al. [2024]\\nNaman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica.\\nLivecodebench: Holistic and contamination free evaluation of large language models for code, 2024.\\nURL https://arxiv.org/abs/2403.07974.\\nAustin et\\xa0al. [2021]\\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton.\\nProgram synthesis with large language models, 2021.\\nURL https://arxiv.org/abs/2108.07732.\\nLightman et\\xa0al. [2023]\\nHunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe.\\nLet’s verify step by step, 2023.\\nURL https://arxiv.org/abs/2305.20050.\\nRein et\\xa0al. [2023]\\nDavid Rein, Betty\\xa0Li Hou, Asa\\xa0Cooper Stickland, Jackson Petty, Richard\\xa0Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel\\xa0R. Bowman.\\nGpqa: A graduate-level google-proof q&a benchmark.\\nArXiv, abs/2311.12022, 2023.\\nURL https://api.semanticscholar.org/CorpusID:265295009<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:45:22 [engine.py:317] Added request chatcmpl-e577df325cdc498baca6e27765c2cc28.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from data/output/arxiv_org_21_0_0.txt...vLLM STDOUT: INFO:     127.0.0.1:54846 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 10 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/arxiv_org_21_0_0_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/arxiv_org_21_0_0_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from data/output/arxiv_org_21_0_0.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/arxiv_org_21_0_0_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:56990 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:57006 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:45:31 [logger.py:43] Received request chatcmpl-55438e0caade457da2cce28b23821098: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\ndownarrow{\\\\scriptstyle 2.0\\\\%}\\n81.7↑5.2%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 5.2\\\\%}\\n3084↓26.1%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 26.1\\\\%}\\n0.39↓61.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 61.0\\\\%}\\n54.4↓0.4%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 0.4\\\\%}\\n6398↓25.5%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 25.5\\\\%}\\n0.64↓36.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 36.0\\\\%}\\nHiPO\\n68.3↑7.9%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 7.9\\\\%}\\n17614↓27.3%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 27.3\\\\%}\\n0.98↓6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 6\\\\%}\\n44.3↑31.4%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 31.4\\\\%}\\n19358↓24.4%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 24.4\\\\%}\\n0.92↓8.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 8.0\\\\%}\\n86.0↑11.1%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 11.1\\\\%}\\n1973↓52.7%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 52.7\\\\%}\\n0.28↓62.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 62.0\\\\%}\\n62.8↑15.0%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 15.0\\\\%}\\n4330↓49.6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 49.6\\\\%}\\n0.47↓53.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 53.0\\\\%}\\nQwen3-32B\\nCold-Start (On)\\n81.7\\n19551\\n1.00\\n65.4\\n17885\\n1.00\\n87.8\\n4298\\n1.00\\n76.2\\n4753\\n1.00\\nCold-Start\\n85.0↑4.3%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 4.3\\\\%}\\n16542↓15.4%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 15.4\\\\%}\\n1.00−0.0%\\\\scriptstyle{\\\\scriptstyle-0.0\\\\%}\\n65.9↑0.8%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 0.8\\\\%}\\n14935↓16.5%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 16.5\\\\%}\\n0.87↓13.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 13.0\\\\%}\\n92.1↑4.9%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 4.9\\\\%}\\n2785↓35.2%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 35.2\\\\%}\\n0.47↓53.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 53.0\\\\%}\\n78.4↑2.2%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 2.2\\\\%}\\n3991↓16.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 16.0\\\\%}\\n0.51↓49.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 49.0\\\\%}\\nHiPO\\n88.3↑8.1%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 8.1\\\\%}\\n14873↓23.9%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 23.9\\\\%}\\n0.98↓2.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 2.0\\\\%}\\n68.5↑4.5%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 4.5\\\\%}\\n12721↓28.9%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 28.9\\\\%}\\n0.82↓18.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 18.0\\\\%}\\n92.7↑5.6%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 5.6\\\\%<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:45:31 [engine.py:317] Added request chatcmpl-55438e0caade457da2cce28b23821098.\n",
            "\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from data/output/arxiv_org_14_0.txt...vLLM STDOUT: INFO:     127.0.0.1:57010 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:45:34 [logger.py:43] Received request chatcmpl-ef3feeb33c094c68ac138c74751e9c61: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\ndownarrow{\\\\scriptstyle 2.0\\\\%}\\n81.7↑5.2%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 5.2\\\\%}\\n3084↓26.1%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 26.1\\\\%}\\n0.39↓61.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 61.0\\\\%}\\n54.4↓0.4%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 0.4\\\\%}\\n6398↓25.5%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 25.5\\\\%}\\n0.64↓36.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 36.0\\\\%}\\nHiPO\\n68.3↑7.9%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 7.9\\\\%}\\n17614↓27.3%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 27.3\\\\%}\\n0.98↓6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 6\\\\%}\\n44.3↑31.4%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 31.4\\\\%}\\n19358↓24.4%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 24.4\\\\%}\\n0.92↓8.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 8.0\\\\%}\\n86.0↑11.1%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 11.1\\\\%}\\n1973↓52.7%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 52.7\\\\%}\\n0.28↓62.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 62.0\\\\%}\\n62.8↑15.0%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 15.0\\\\%}\\n4330↓49.6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 49.6\\\\%}\\n0.47↓53.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 53.0\\\\%}\\nQwen3-32B\\nCold-Start (On)\\n81.7\\n19551\\n1.00\\n65.4\\n17885\\n1.00\\n87.8\\n4298\\n1.00\\n76.2\\n4753\\n1.00\\nCold-Start\\n85.0↑4.3%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 4.3\\\\%}\\n16542↓15.4%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 15.4\\\\%}\\n1.00−0.0%\\\\scriptstyle{\\\\scriptstyle-0.0\\\\%}\\n65.9↑0.8%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 0.8\\\\%}\\n14935↓16.5%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 16.5\\\\%}\\n0.87↓13.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 13.0\\\\%}\\n92.1↑4.9%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 4.9\\\\%}\\n2785↓35.2%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 35.2\\\\%}\\n0.47↓53.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 53.0\\\\%}\\n78.4↑2.2%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 2.2\\\\%}\\n3991↓16.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 16.0\\\\%}\\n0.51↓49.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 49.0\\\\%}\\nHiPO\\n88.3↑8.1%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 8.1\\\\%}\\n14873↓23.9%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 23.9\\\\%}\\n0.98↓2.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 2.0\\\\%}\\n68.5↑4.5%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 4.5\\\\%}\\n12721↓28.9%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 28.9\\\\%}\\n0.82↓18.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 18.0\\\\%}\\n92.7↑5.6%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 5.6\\\\%<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:45:34 [engine.py:317] Added request chatcmpl-ef3feeb33c094c68ac138c74751e9c61.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from data/output/arxiv_org_14_0.txt...vLLM STDOUT: INFO:     127.0.0.1:43308 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 12 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/arxiv_org_14_0_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/arxiv_org_14_0_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from data/output/arxiv_org_14_0.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/arxiv_org_14_0_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:43316 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:43332 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:45:43 [logger.py:43] Received request chatcmpl-4af8dd242969499a85193865c4db9e0c: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n0.0\\\\%}\\n10327↓18.5%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 18.5\\\\%}\\n0.64↓36.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 36.0\\\\%}\\nAutoThink\\n92.8↑0.9%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 0.9\\\\%}\\n4261↓31.7%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 31.7\\\\%}\\n0.56↓44.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 44.0\\\\%}\\n58.1↓4.9%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 4.9\\\\%}\\n9898↓8.6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 8.6\\\\%}\\n0.89↓11.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 11.0\\\\%}\\n70.0↓2.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 2.8\\\\%}\\n4958↓12.4%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 12.4\\\\%}\\n0.40↓60.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 60.0\\\\%}\\n74.3↑0.7%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 0.7\\\\%}\\n10240↓19.2%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 19.2\\\\%}\\n0.67↓33.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 33.0\\\\%}\\nHiPO\\n93.6 ↑1.7%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 1.7\\\\%}\\n4090 ↓34.4%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 34.4\\\\%}\\n0.54 ↓45.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 45.8\\\\%}\\n60.1 ↓1.7%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 1.7\\\\%}\\n9367 ↓13.5%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 13.5\\\\%}\\n0.92↓−8.1%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle-8.1\\\\%}\\n72.2↑0.3%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 0.3\\\\%}\\n1338↓69.7%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 69.7\\\\%}\\n0.12↓88.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 88.0\\\\%}\\n78.4↑6.3%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 6.3\\\\%}\\n8842↓30.2%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 30.2\\\\%}\\n0.63↓36.5%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 36.5\\\\%}\\nTable 2: Based on Qwen3-8B, performance of different methods on multiple benchmarks. RatioT\\\\textbf{Ratio}_{\\\\textbf{T}} denotes the ratio of “Think-on” mode over the corresponding benchmark.\\nIn Table 2, we observe that training the model solely on Think-on data leads the model to engage in reasoning for problems of any difficulty. We use this baseline as a typical example of \"overthinking\" for comparison. After applying GRPO to the Cold-Start (on) model, there is a significant improvement in accuracy, with an average accuracy increase of 3.1%. However, this does not reduce the token length and thinking rate of the model. On the contrary, to achieve higher accuracy, the token length output by the model on simpler datasets increases significantly. When training the model on a dataset containing both Think-on and Think-off data, the accuracy of the resulting Cold-Start model improves by 4.0% compared to the Cold-Start(on) model, while the token length and thinking rate decrease by 10.8% and 22%, respectively. After applying the GRPO algorithm to the Cold-Start model, there is no significant change in performance. However, when applying our method HiPO to train the Cold-Start model, the accuracy improves by 6.2%, while the token length and thinking rate decrease dramatically by 30% and 39%, respectively. Moreover, experimental results show that our HiPO outperforms existing methods on both efficiency<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:45:43 [engine.py:317] Added request chatcmpl-4af8dd242969499a85193865c4db9e0c.\n",
            "\u001b[2K\u001b[32m⠏\u001b[0m Generating qa content from data/output/arxiv_org_11_0_0.txt...vLLM STDOUT: INFO:     127.0.0.1:43334 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:45:45 [logger.py:43] Received request chatcmpl-2c7b2a5dc91849e196599fa423f43a84: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\n0.0\\\\%}\\n10327↓18.5%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 18.5\\\\%}\\n0.64↓36.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 36.0\\\\%}\\nAutoThink\\n92.8↑0.9%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 0.9\\\\%}\\n4261↓31.7%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 31.7\\\\%}\\n0.56↓44.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 44.0\\\\%}\\n58.1↓4.9%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 4.9\\\\%}\\n9898↓8.6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 8.6\\\\%}\\n0.89↓11.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 11.0\\\\%}\\n70.0↓2.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 2.8\\\\%}\\n4958↓12.4%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 12.4\\\\%}\\n0.40↓60.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 60.0\\\\%}\\n74.3↑0.7%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 0.7\\\\%}\\n10240↓19.2%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 19.2\\\\%}\\n0.67↓33.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 33.0\\\\%}\\nHiPO\\n93.6 ↑1.7%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 1.7\\\\%}\\n4090 ↓34.4%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 34.4\\\\%}\\n0.54 ↓45.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 45.8\\\\%}\\n60.1 ↓1.7%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 1.7\\\\%}\\n9367 ↓13.5%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 13.5\\\\%}\\n0.92↓−8.1%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle-8.1\\\\%}\\n72.2↑0.3%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 0.3\\\\%}\\n1338↓69.7%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 69.7\\\\%}\\n0.12↓88.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 88.0\\\\%}\\n78.4↑6.3%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 6.3\\\\%}\\n8842↓30.2%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 30.2\\\\%}\\n0.63↓36.5%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 36.5\\\\%}\\nTable 2: Based on Qwen3-8B, performance of different methods on multiple benchmarks. RatioT\\\\textbf{Ratio}_{\\\\textbf{T}} denotes the ratio of “Think-on” mode over the corresponding benchmark.\\nIn Table 2, we observe that training the model solely on Think-on data leads the model to engage in reasoning for problems of any difficulty. We use this baseline as a typical example of \"overthinking\" for comparison. After applying GRPO to the Cold-Start (on) model, there is a significant improvement in accuracy, with an average accuracy increase of 3.1%. However, this does not reduce the token length and thinking rate of the model. On the contrary, to achieve higher accuracy, the token length output by the model on simpler datasets increases significantly. When training the model on a dataset containing both Think-on and Think-off data, the accuracy of the resulting Cold-Start model improves by 4.0% compared to the Cold-Start(on) model, while the token length and thinking rate decrease by 10.8% and 22%, respectively. After applying the GRPO algorithm to the Cold-Start model, there is no significant change in performance. However, when applying our method HiPO to train the Cold-Start model, the accuracy improves by 6.2%, while the token length and thinking rate decrease dramatically by 30% and 39%, respectively. Moreover, experimental results show that our HiPO outperforms existing methods on both efficiency<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:45:45 [engine.py:317] Added request chatcmpl-2c7b2a5dc91849e196599fa423f43a84.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from data/output/arxiv_org_11_0_0.txt...vLLM STDOUT: INFO:     127.0.0.1:48486 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 11 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/arxiv_org_11_0_0_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/arxiv_org_11_0_0_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from data/output/arxiv_org_11_0_0.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/arxiv_org_11_0_0_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:55684 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:55696 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:45:54 [logger.py:43] Received request chatcmpl-42d69f65f0e34d53b15ee5858fcfe3ed: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n0.0\\\\%}\\n10327↓18.5%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 18.5\\\\%}\\n0.64↓36.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 36.0\\\\%}\\nAutoThink\\n92.8↑0.9%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 0.9\\\\%}\\n4261↓31.7%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 31.7\\\\%}\\n0.56↓44.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 44.0\\\\%}\\n58.1↓4.9%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 4.9\\\\%}\\n9898↓8.6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 8.6\\\\%}\\n0.89↓11.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 11.0\\\\%}\\n70.0↓2.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 2.8\\\\%}\\n4958↓12.4%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 12.4\\\\%}\\n0.40↓60.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 60.0\\\\%}\\n74.3↑0.7%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 0.7\\\\%}\\n10240↓19.2%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 19.2\\\\%}\\n0.67↓33.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 33.0\\\\%}\\nHiPO\\n93.6 ↑1.7%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 1.7\\\\%}\\n4090 ↓34.4%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 34.4\\\\%}\\n0.54 ↓45.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 45.8\\\\%}\\n60.1 ↓1.7%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 1.7\\\\%}\\n9367 ↓13.5%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 13.5\\\\%}\\n0.92↓−8.1%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle-8.1\\\\%}\\n72.2↑0.3%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 0.3\\\\%}\\n1338↓69.7%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 69.7\\\\%}\\n0.12↓88.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 88.0\\\\%}\\n78.4↑6.3%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 6.3\\\\%}\\n8842↓30.2%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 30.2\\\\%}\\n0.63↓36.5%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 36.5\\\\%}\\nTable 2: Based on Qwen3-8B, performance of different methods on multiple benchmarks. RatioT\\\\textbf{Ratio}_{\\\\textbf{T}} denotes the ratio of “Think-on” mode over the corresponding benchmark.\\nIn Table 2, we observe that training the model solely on Think-on data leads the model to engage in reasoning for problems of any difficulty. We use this baseline as a typical example of \"overthinking\" for comparison. After applying GRPO to the Cold-Start (on) model, there is a significant improvement in accuracy, with an average accuracy increase of 3.1%. However, this does not reduce the token length and thinking rate of the model. On the contrary, to achieve higher accuracy, the token length output by the model on simpler datasets increases significantly. When training the model on a dataset containing both Think-on and Think-off data, the accuracy of the resulting Cold-Start model improves by 4.0% compared to the Cold-Start(on) model, while the token length and thinking rate decrease by 10.8% and 22%, respectively. After applying the GRPO algorithm to the Cold-Start model, there is no significant change in performance. However, when applying our method HiPO to train the Cold-Start model, the accuracy improves by 6.2%, while the token length and thinking rate decrease dramatically by 30% and 39%, respectively. Moreover, experimental results show that our HiPO outperforms existing methods on both efficiency<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:45:54 [engine.py:317] Added request chatcmpl-42d69f65f0e34d53b15ee5858fcfe3ed.\n",
            "\u001b[2K\u001b[32m⠹\u001b[0m Generating qa content from data/output/arxiv_org_11_0.txt...vLLM STDOUT: INFO:     127.0.0.1:55706 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:45:56 [logger.py:43] Received request chatcmpl-673fac14b04640cea001c3ddb5c22a06: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\n0.0\\\\%}\\n10327↓18.5%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 18.5\\\\%}\\n0.64↓36.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 36.0\\\\%}\\nAutoThink\\n92.8↑0.9%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 0.9\\\\%}\\n4261↓31.7%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 31.7\\\\%}\\n0.56↓44.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 44.0\\\\%}\\n58.1↓4.9%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 4.9\\\\%}\\n9898↓8.6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 8.6\\\\%}\\n0.89↓11.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 11.0\\\\%}\\n70.0↓2.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 2.8\\\\%}\\n4958↓12.4%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 12.4\\\\%}\\n0.40↓60.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 60.0\\\\%}\\n74.3↑0.7%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 0.7\\\\%}\\n10240↓19.2%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 19.2\\\\%}\\n0.67↓33.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 33.0\\\\%}\\nHiPO\\n93.6 ↑1.7%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 1.7\\\\%}\\n4090 ↓34.4%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 34.4\\\\%}\\n0.54 ↓45.8%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 45.8\\\\%}\\n60.1 ↓1.7%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 1.7\\\\%}\\n9367 ↓13.5%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 13.5\\\\%}\\n0.92↓−8.1%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle-8.1\\\\%}\\n72.2↑0.3%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 0.3\\\\%}\\n1338↓69.7%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 69.7\\\\%}\\n0.12↓88.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 88.0\\\\%}\\n78.4↑6.3%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 6.3\\\\%}\\n8842↓30.2%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 30.2\\\\%}\\n0.63↓36.5%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 36.5\\\\%}\\nTable 2: Based on Qwen3-8B, performance of different methods on multiple benchmarks. RatioT\\\\textbf{Ratio}_{\\\\textbf{T}} denotes the ratio of “Think-on” mode over the corresponding benchmark.\\nIn Table 2, we observe that training the model solely on Think-on data leads the model to engage in reasoning for problems of any difficulty. We use this baseline as a typical example of \"overthinking\" for comparison. After applying GRPO to the Cold-Start (on) model, there is a significant improvement in accuracy, with an average accuracy increase of 3.1%. However, this does not reduce the token length and thinking rate of the model. On the contrary, to achieve higher accuracy, the token length output by the model on simpler datasets increases significantly. When training the model on a dataset containing both Think-on and Think-off data, the accuracy of the resulting Cold-Start model improves by 4.0% compared to the Cold-Start(on) model, while the token length and thinking rate decrease by 10.8% and 22%, respectively. After applying the GRPO algorithm to the Cold-Start model, there is no significant change in performance. However, when applying our method HiPO to train the Cold-Start model, the accuracy improves by 6.2%, while the token length and thinking rate decrease dramatically by 30% and 39%, respectively. Moreover, experimental results show that our HiPO outperforms existing methods on both efficiency<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:45:56 [engine.py:317] Added request chatcmpl-673fac14b04640cea001c3ddb5c22a06.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from data/output/arxiv_org_11_0.txt...vLLM STDOUT: INFO:     127.0.0.1:55720 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 13 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/arxiv_org_11_0_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/arxiv_org_11_0_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from data/output/arxiv_org_11_0.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/arxiv_org_11_0_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:55234 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:55238 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:46:05 [logger.py:43] Received request chatcmpl-be36db366d0247129274d6fd61a8cf74: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n20.9\\\\%}\\n1.00↑2.0%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 2.0\\\\%}\\n58.37↓7.3%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 7.3\\\\%}\\n16029↑18.2%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 18.2\\\\%}\\n0.88↑7.3%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 7.3\\\\%}\\n89.63↓0.6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 0.6\\\\%}\\n2052↑164.4%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 164.4\\\\%}\\n0.32↑166.7%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 166.7\\\\%}\\nTable 3: Performance of different design strategies on advantage functions.\\nFigure 4: Performance of different γ\\\\gamma values.\\nFigure 5: Performance of different rollout numbers and ω\\\\omega values.\\nEffect of different γ\\\\gamma values. Figure 4 shows that, when the value of γ\\\\gamma is set to 0.00, the reward for the judge token lacks information about the current response, resulting in lower model accuracy and higher token length. On the other hand, when γ\\\\gamma is set too high, the scales of the two terms (mean(𝐫off\\\\mathbf{r}_{\\\\text{off}}) - mean(𝐫\\\\mathbf{r})) and (ri−mean\\u200b(𝐫))(r_{i}-\\\\textit{mean}(\\\\mathbf{r})) become imbalanced, which leads to a decrease in model accuracy and an increase in token length.\\nEffect of different rollout numbers. Table 5 shows that, when the rollout number NN is set to 16, the model achieves better average performance, shorter token length, and lower think rate. We attribute this to the fact that this configuration provides sufficient data to explore diverse possibilities while avoiding excessive samples with redundant reasoning that dilute the training signal. As a result, the model focuses more on learning from higher-quality samples, leading to a more concise strategy with improved accuracy, reduced token length, and lower think rate.\\nEffect of different ω\\\\omega values. Table 5 shows that, setting ω\\\\omega to 0.01 provides a balanced trade-off between performance and efficiency. This configuration mitigates the overly conservative behavior seen at 0.0 while avoiding the overly aggressive behavior at higher settings, ultimately achieving the largest efficiency gains with minimal performance loss.\\n4.4 Further Analysis\\n(a) (a) Think-on and Think-off ratio in training. (b) Think-on ratio of different datasets.\\n(b) (a) Average token usage in RL training. (b) Token usage of different datasets.\\nAIME24\\nLiveCodeBench\\nHumanEval\\nMBPP\\nMethod\\nAcc\\nLength\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}\\nAcc\\nLength\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}\\nAcc\\nLength\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}\\nAcc\\nLength\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}\\nQwen3-1.7B\\nCold-Start (On)\\n63.3\\n24214\\n1.00\\n33.7\\n25616\\n1.00\\n77.4\\n4172\\n1.00\\n54.6\\n8587\\n1.00\\nCold-Start\\n65.0↑2.7%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 2.7\\\\%}\\n21039↓13.1%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 13.1\\\\%}\\n1.00−0.0%\\\\scriptstyle-{\\\\scriptstyle 0.0\\\\%}\\n37.4↑11.0%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 11.0\\\\%}\\n21364↓16.6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 16.6\\\\%}\\n0.98↓2.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 2.0\\\\%}\\n81.7↑5.2%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 5.2\\\\%}\\n3084↓26.1%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 26.1\\\\%}\\n0.39↓61.0<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:46:05 [engine.py:317] Added request chatcmpl-be36db366d0247129274d6fd61a8cf74.\n",
            "\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from data/output/arxiv_org_13_0_0.txt...vLLM STDOUT: INFO:     127.0.0.1:55244 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:46:09 [logger.py:43] Received request chatcmpl-068ea83726fb45b5aa7c9bc6310b6fb5: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\n 20.9\\\\%}\\n1.00↑2.0%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 2.0\\\\%}\\n58.37↓7.3%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 7.3\\\\%}\\n16029↑18.2%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 18.2\\\\%}\\n0.88↑7.3%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 7.3\\\\%}\\n89.63↓0.6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 0.6\\\\%}\\n2052↑164.4%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 164.4\\\\%}\\n0.32↑166.7%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 166.7\\\\%}\\nTable 3: Performance of different design strategies on advantage functions.\\nFigure 4: Performance of different γ\\\\gamma values.\\nFigure 5: Performance of different rollout numbers and ω\\\\omega values.\\nEffect of different γ\\\\gamma values. Figure 4 shows that, when the value of γ\\\\gamma is set to 0.00, the reward for the judge token lacks information about the current response, resulting in lower model accuracy and higher token length. On the other hand, when γ\\\\gamma is set too high, the scales of the two terms (mean(𝐫off\\\\mathbf{r}_{\\\\text{off}}) - mean(𝐫\\\\mathbf{r})) and (ri−mean\\u200b(𝐫))(r_{i}-\\\\textit{mean}(\\\\mathbf{r})) become imbalanced, which leads to a decrease in model accuracy and an increase in token length.\\nEffect of different rollout numbers. Table 5 shows that, when the rollout number NN is set to 16, the model achieves better average performance, shorter token length, and lower think rate. We attribute this to the fact that this configuration provides sufficient data to explore diverse possibilities while avoiding excessive samples with redundant reasoning that dilute the training signal. As a result, the model focuses more on learning from higher-quality samples, leading to a more concise strategy with improved accuracy, reduced token length, and lower think rate.\\nEffect of different ω\\\\omega values. Table 5 shows that, setting ω\\\\omega to 0.01 provides a balanced trade-off between performance and efficiency. This configuration mitigates the overly conservative behavior seen at 0.0 while avoiding the overly aggressive behavior at higher settings, ultimately achieving the largest efficiency gains with minimal performance loss.\\n4.4 Further Analysis\\n(a) (a) Think-on and Think-off ratio in training. (b) Think-on ratio of different datasets.\\n(b) (a) Average token usage in RL training. (b) Token usage of different datasets.\\nAIME24\\nLiveCodeBench\\nHumanEval\\nMBPP\\nMethod\\nAcc\\nLength\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}\\nAcc\\nLength\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}\\nAcc\\nLength\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}\\nAcc\\nLength\\nRatioT\\\\textbf{Ratio}_{\\\\textbf{T}}\\nQwen3-1.7B\\nCold-Start (On)\\n63.3\\n24214\\n1.00\\n33.7\\n25616\\n1.00\\n77.4\\n4172\\n1.00\\n54.6\\n8587\\n1.00\\nCold-Start\\n65.0↑2.7%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 2.7\\\\%}\\n21039↓13.1%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 13.1\\\\%}\\n1.00−0.0%\\\\scriptstyle-{\\\\scriptstyle 0.0\\\\%}\\n37.4↑11.0%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 11.0\\\\%}\\n21364↓16.6%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 16.6\\\\%}\\n0.98↓2.0%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 2.0\\\\%}\\n81.7↑5.2%\\\\scriptstyle\\\\uparrow{\\\\scriptstyle 5.2\\\\%}\\n3084↓26.1%\\\\scriptstyle\\\\downarrow{\\\\scriptstyle 26.1\\\\%}\\n0.39↓61.0<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:46:09 [engine.py:317] Added request chatcmpl-068ea83726fb45b5aa7c9bc6310b6fb5.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from data/output/arxiv_org_13_0_0.txt...vLLM STDOUT: INFO:     127.0.0.1:55256 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 13 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/arxiv_org_13_0_0_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/arxiv_org_13_0_0_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from data/output/arxiv_org_13_0_0.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/arxiv_org_13_0_0_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:49356 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:49370 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:46:18 [logger.py:43] Received request chatcmpl-f5b5bca7d5564f50bdeaa9e60c126e88: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nNaseh, Marzena Karpinska, Mohit Iyyer, Amir Houmansadr, and Eugene Bagdasarian.\\nOverthink: Slowdown attacks on reasoning llms, 2025.\\nURL https://arxiv.org/abs/2502.02542.\\nSui et\\xa0al. [2025]\\nYang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu, Andrew Wen, Shaochen Zhong, Na\\xa0Zou, Hanjie Chen, and Xia Hu.\\nStop overthinking: A survey on efficient reasoning for large language models, 2025.\\nURL https://arxiv.org/abs/2503.16419.\\nNayab et\\xa0al. [2025]\\nSania Nayab, Giulio Rossolini, Marco Simoni, Andrea Saracino, Giorgio Buttazzo, Nicolamaria Manes, and Fabrizio Giacomelli.\\nConcise thoughts: Impact of output length on llm reasoning and cost, 2025.\\nURL https://arxiv.org/abs/2407.19825.\\nAggarwal and Welleck [2025]\\nPranjal Aggarwal and Sean Welleck.\\nL1: Controlling how long a reasoning model thinks with reinforcement learning, 2025.\\nURL https://arxiv.org/abs/2503.04697.\\nArora and Zanette [2025]\\nDaman Arora and Andrea Zanette.\\nTraining language models to reason efficiently, 2025.\\nURL https://arxiv.org/abs/2502.04463.\\nHou et\\xa0al. [2025]\\nBairu Hou, Yang Zhang, Jiabao Ji, Yujian Liu, Kaizhi Qian, Jacob Andreas, and Shiyu Chang.\\nThinkprune: Pruning long chain-of-thought of llms via reinforcement learning, 2025.\\nURL https://arxiv.org/abs/2504.01296.\\nLuo et\\xa0al. [2025]\\nHaotian Luo, Li\\xa0Shen, Haiying He, Yibo Wang, Shiwei Liu, Wei Li, Naiqiang Tan, Xiaochun Cao, and Dacheng Tao.\\nO1-pruner: Length-harmonizing fine-tuning for o1-like reasoning pruning, 2025.\\nURL https://arxiv.org/abs/2501.12570.\\nShen et\\xa0al. [2025]\\nYi\\xa0Shen, Jian Zhang, Jieyun Huang, Shuming Shi, Wenjing Zhang, Jiangze Yan, Ning Wang, Kai Wang, Zhaoxiang Liu, and Shiguo Lian.\\nDast: Difficulty-adaptive slow-thinking for large reasoning models, 2025.\\nURL https://arxiv.org/abs/2503.04472.\\nTeam et\\xa0al. [2025]\\nKimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, Chuning Tang, Congcong Wang, Dehao Zhang, Enming Yuan, Enzhe Lu, Fengxiang Tang, Flood Sung, Guangda Wei, Guokun Lai, Haiqing Guo, Han Zhu, Hao Ding, Hao Hu, Hao Yang, Hao Zhang, Haotian Yao, Haotian Zhao, Haoyu Lu, Haoze Li, Haozhen Yu, Hongcheng Gao, Huabin Zheng, Huan Yuan, Jia Chen, Jianhang Guo, Jianlin Su, Jianzhou Wang, Jie Zhao, Jin Zhang, Jingyuan Liu, Junjie Yan, Junyan Wu, Lidong Shi, Ling Ye, Longhui Yu, Mengnan Dong, Neo Zhang, Ningchen Ma, Qiwei Pan, Qucheng Gong, Shaowei Liu, Shengling Ma, Shupeng Wei, Sihan Cao, Siying Huang, Tao Jiang, Weihao Gao, Weimin Xiong, Weiran He, Weixiao Huang, Weixin Xu, Wenhao Wu, Wenyang He, Xianghui Wei, Xianqing Jia, Xingzhe Wu, Xinran Xu, Xinxing Zu,<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:46:18 [engine.py:317] Added request chatcmpl-f5b5bca7d5564f50bdeaa9e60c126e88.\n",
            "\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from data/output/arxiv_org_16_0.txt...vLLM STDOUT: INFO:     127.0.0.1:49384 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-03 08:46:21 [logger.py:43] Received request chatcmpl-87c1d5f265f94b5fa522d273868a01b4: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\n Naseh, Marzena Karpinska, Mohit Iyyer, Amir Houmansadr, and Eugene Bagdasarian.\\nOverthink: Slowdown attacks on reasoning llms, 2025.\\nURL https://arxiv.org/abs/2502.02542.\\nSui et\\xa0al. [2025]\\nYang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu, Andrew Wen, Shaochen Zhong, Na\\xa0Zou, Hanjie Chen, and Xia Hu.\\nStop overthinking: A survey on efficient reasoning for large language models, 2025.\\nURL https://arxiv.org/abs/2503.16419.\\nNayab et\\xa0al. [2025]\\nSania Nayab, Giulio Rossolini, Marco Simoni, Andrea Saracino, Giorgio Buttazzo, Nicolamaria Manes, and Fabrizio Giacomelli.\\nConcise thoughts: Impact of output length on llm reasoning and cost, 2025.\\nURL https://arxiv.org/abs/2407.19825.\\nAggarwal and Welleck [2025]\\nPranjal Aggarwal and Sean Welleck.\\nL1: Controlling how long a reasoning model thinks with reinforcement learning, 2025.\\nURL https://arxiv.org/abs/2503.04697.\\nArora and Zanette [2025]\\nDaman Arora and Andrea Zanette.\\nTraining language models to reason efficiently, 2025.\\nURL https://arxiv.org/abs/2502.04463.\\nHou et\\xa0al. [2025]\\nBairu Hou, Yang Zhang, Jiabao Ji, Yujian Liu, Kaizhi Qian, Jacob Andreas, and Shiyu Chang.\\nThinkprune: Pruning long chain-of-thought of llms via reinforcement learning, 2025.\\nURL https://arxiv.org/abs/2504.01296.\\nLuo et\\xa0al. [2025]\\nHaotian Luo, Li\\xa0Shen, Haiying He, Yibo Wang, Shiwei Liu, Wei Li, Naiqiang Tan, Xiaochun Cao, and Dacheng Tao.\\nO1-pruner: Length-harmonizing fine-tuning for o1-like reasoning pruning, 2025.\\nURL https://arxiv.org/abs/2501.12570.\\nShen et\\xa0al. [2025]\\nYi\\xa0Shen, Jian Zhang, Jieyun Huang, Shuming Shi, Wenjing Zhang, Jiangze Yan, Ning Wang, Kai Wang, Zhaoxiang Liu, and Shiguo Lian.\\nDast: Difficulty-adaptive slow-thinking for large reasoning models, 2025.\\nURL https://arxiv.org/abs/2503.04472.\\nTeam et\\xa0al. [2025]\\nKimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, Chuning Tang, Congcong Wang, Dehao Zhang, Enming Yuan, Enzhe Lu, Fengxiang Tang, Flood Sung, Guangda Wei, Guokun Lai, Haiqing Guo, Han Zhu, Hao Ding, Hao Hu, Hao Yang, Hao Zhang, Haotian Yao, Haotian Zhao, Haoyu Lu, Haoze Li, Haozhen Yu, Hongcheng Gao, Huabin Zheng, Huan Yuan, Jia Chen, Jianhang Guo, Jianlin Su, Jianzhou Wang, Jie Zhao, Jin Zhang, Jingyuan Liu, Junjie Yan, Junyan Wu, Lidong Shi, Ling Ye, Longhui Yu, Mengnan Dong, Neo Zhang, Ningchen Ma, Qiwei Pan, Qucheng Gong, Shaowei Liu, Shengling Ma, Shupeng Wei, Sihan Cao, Siying Huang, Tao Jiang, Weihao Gao, Weimin Xiong, Weiran He, Weixiao Huang, Weixin Xu, Wenhao Wu, Wenyang He, Xianghui Wei, Xianqing Jia, Xingzhe Wu, Xinran Xu, Xinxing Zu,<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-03 08:46:21 [engine.py:317] Added request chatcmpl-87c1d5f265f94b5fa522d273868a01b4.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m⠙\u001b[0m Generating qa content from data/output/arxiv_org_16_0.txt...vLLM STDOUT: INFO:     127.0.0.1:49400 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 6 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/arxiv_org_16_0_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/arxiv_org_16_0_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from data/output/arxiv_org_16_0.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/arxiv_org_16_0_qa_pairs.json\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "# Process 3 chunks for now -> can increase but slower!\n",
        "for filename in filenames:\n",
        "    !synthetic-data-kit \\\n",
        "        -c synthetic_data_kit_config.yaml \\\n",
        "        create {filename} \\\n",
        "        --num-pairs 25 \\\n",
        "        --type \"qa\"\n",
        "    time.sleep(2) # Sleep some time to leave some room for processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNkxxvBx7Csp"
      },
      "source": [
        "Optionally, you can clean up the data via pruning \"bad\" or low quality examples and convert the rest to JSON format for finetuning!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "HMD-izj5OiAK"
      },
      "outputs": [],
      "source": [
        "!synthetic-data-kit \\\n",
        "    -c synthetic_data_kit_config.yaml \\\n",
        "    curate --threshold 0.0 \\\n",
        "    \"data/generated/arxiv_org_0_qa_pairs.json\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AScJ5-vAOjYj"
      },
      "source": [
        "We now convert the generated datasets into QA formats so we can load it for finetuning:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9Um4Z8SqUTB",
        "outputId": "0dad8ab5-8a5e-414d-df89-417acd8a03e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\u001b[32m⠋\u001b[0m Converting data/generated/arxiv_org_0_qa_pairs.json to ft format with json \n",
            "storage...\n",
            "\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[32m Converted to ft format and saved to \u001b[0m\u001b[1;32mdata/final/arxiv_org_0_qa_pairs_ft.json\u001b[0m\n",
            "\u001b[?25l\u001b[32m⠋\u001b[0m Converting data/generated/arxiv_org_1_qa_pairs.json to ft format with json \n",
            "storage...\n",
            "\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[32m Converted to ft format and saved to \u001b[0m\u001b[1;32mdata/final/arxiv_org_1_qa_pairs_ft.json\u001b[0m\n",
            "\u001b[?25l\u001b[32m⠋\u001b[0m Converting data/generated/arxiv_org_2_qa_pairs.json to ft format with json \n",
            "storage...\n",
            "\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[32m Converted to ft format and saved to \u001b[0m\u001b[1;32mdata/final/arxiv_org_2_qa_pairs_ft.json\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "qa_pairs_filenames = [\n",
        "    f\"data/generated/arxiv_org_{i}_qa_pairs.json\"\n",
        "    for i in range(len(filenames[:3]))\n",
        "]\n",
        "for filename in qa_pairs_filenames:\n",
        "    !synthetic-data-kit \\\n",
        "        -c synthetic_data_kit_config.yaml \\\n",
        "        save-as {filename} -f ft"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dVK-qza7rPB"
      },
      "source": [
        "Let's load up the data and see what the synthetic data looks like!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "VrBwG2KT7dam"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "final_filenames = [\n",
        "    f\"data/final/arxiv_org_{i}_qa_pairs_ft.json\"\n",
        "    for i in range(len(filenames[:3]))\n",
        "]\n",
        "conversations = pd.concat([\n",
        "    pd.read_json(name) for name in final_filenames\n",
        "]).reset_index(drop = True)\n",
        "\n",
        "dataset = Dataset.from_pandas(conversations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaZ3tRP8frSn",
        "outputId": "58097f3f-2007-45bf-c35b-c7934c709be3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'messages': [{'content': 'You are a helpful assistant.', 'role': 'system'},\n",
              "  {'content': 'What is the primary driver of the progress in Large Language Models (LLMs)?',\n",
              "   'role': 'user'},\n",
              "  {'content': 'Integrating Chain-of-Thought (CoT) reasoning, which decomposes complex queries into sequential, interpretable steps to derive accurate outputs.',\n",
              "   'role': 'assistant'}]}"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ],
      "source": [
        "dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "504n46Sxfruu",
        "outputId": "4cfa8ce3-97d0-4f8e-d133-9dbb5cb8c4a4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'messages': [{'content': 'You are a helpful assistant.', 'role': 'system'},\n",
              "  {'content': 'What is the main drawback of overthinking in models?',\n",
              "   'role': 'user'},\n",
              "  {'content': 'Inflated token usage, higher latency, and reduced efficiency in interactive applications.',\n",
              "   'role': 'assistant'}]}"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ],
      "source": [
        "dataset[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BVBp9YXRw_o",
        "outputId": "c28fe0c7-fa75-4532-ecc7-6c373a016b95"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'messages': [{'content': 'You are a helpful assistant.', 'role': 'system'},\n",
              "  {'content': 'What is the main objective of the Method 3?', 'role': 'user'},\n",
              "  {'content': 'To improve complex problem solving via a hybrid data construction pipeline and a hybrid reinforcement learning reward system.',\n",
              "   'role': 'assistant'}]}"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aO9qePmP7yaY"
      },
      "source": [
        "Finally free vLLM process to save memory and to allow for finetuning!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8qgTjywzgl6",
        "outputId": "3fbaf737-d833-4a40-fd4d-2b372aadcd32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to terminate the VLLM server gracefully...\n",
            "vLLM STDOUT: INFO 10-03 08:46:31 [launcher.py:80] Shutting down FastAPI HTTP server.\n",
            "Server terminated gracefully.\n"
          ]
        }
      ],
      "source": [
        "generator.cleanup()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQo2PR7oqDQE"
      },
      "source": [
        "### Fine-tuning Synthetic Dataset with Unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmUBVEnvCDJv",
        "outputId": "ea393b46-b235-4045-eb94-310561a2d51c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.9.11: Fast Llama patching. Transformers: 4.55.4. vLLM: 0.9.2.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.30. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Llama-3.2-1B-Instruct\",\n",
        "    max_seq_length = 2048, # Choose any for long context!\n",
        "    load_in_4bit = True,  # 4 bit quantization to reduce memory\n",
        "    load_in_8bit = False, # [NEW!] A bit more accurate, uses 2x memory\n",
        "    full_finetuning = True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXd9bTZd1aaL"
      },
      "source": [
        "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "6bZsfBuZDeCL"
      },
      "outputs": [],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,\n",
        "    loftq_config = None,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vITh0KVJ10qX"
      },
      "source": [
        "<a name=\"Data\"></a>\n",
        "### Data Prep\n",
        "We now use the `Llama-3.2` format for conversation style finetunes. The chat template renders conversations like below: (Cutting Knowledge Date is by default there!)\n",
        "\n",
        "```\n",
        "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "Cutting Knowledge Date: December 2023\n",
        "Today Date: 01 May 2025\n",
        "\n",
        "You are a helpful assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "What is 1+1?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "2<|eot_id|>\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "db8f5cee4be84c798268127406471088",
            "204a0b2190e244f09784b7bf6bc2397d",
            "7a4d0c544db5487e8db33302efb62097",
            "ca93ce6e35e241b78259896f0507c985",
            "8b8df34a01a644d6aa5df58698f29d96",
            "b404c1e9049e4560b4daa89cacd67c64",
            "5e2788cfbd3f44039ebcda334ead120c",
            "04383eb83e914cc29de8413a05650d44",
            "1b322db01b104d20bedc717f55397bc5",
            "adc86bd2ce2c4b0f84e98f61f0be7cf2",
            "26d0d564d2214573b833df3bdadb9561"
          ]
        },
        "id": "LjY75GoYUCB8",
        "outputId": "588d2527-b122-4da0-f3b8-3aee58611c34"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/33 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "db8f5cee4be84c798268127406471088"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "def formatting_prompts_func(examples):\n",
        "    convos = examples[\"messages\"]\n",
        "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
        "    return { \"text\" : texts, }\n",
        "\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKA0VEF4CfCB"
      },
      "source": [
        "See result of the first row:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0usAI0M40hpT",
        "outputId": "a58cb329-6447-48d8-a867-5ba8e45815e7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'messages': [{'content': 'You are a helpful assistant.', 'role': 'system'},\n",
              "  {'content': 'What is the primary driver of the progress in Large Language Models (LLMs)?',\n",
              "   'role': 'user'},\n",
              "  {'content': 'Integrating Chain-of-Thought (CoT) reasoning, which decomposes complex queries into sequential, interpretable steps to derive accurate outputs.',\n",
              "   'role': 'assistant'}],\n",
              " 'text': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Oct 2025\\n\\nYou are a helpful assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhat is the primary driver of the progress in Large Language Models (LLMs)?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nIntegrating Chain-of-Thought (CoT) reasoning, which decomposes complex queries into sequential, interpretable steps to derive accurate outputs.<|eot_id|>'}"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ],
      "source": [
        "dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "Now let's train our model. We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "77f657710d9b40fdb8b8854faff6a984",
            "9a50977e94c64659af5983aaaa5536b3",
            "277dd073f8354058becaad1f6fe9b8f8",
            "14d1b36a23474717a6c7dded32e96ae7",
            "35a488aeafba4c8fa3a48f4094685390",
            "98eb549d03344462be38d0a59a2863b7",
            "70337c0e5a4d404b9cc1e5c0bdcb0c8f",
            "24790702e527421daa479ca64e7998db",
            "b9f296553ef4413face044c0c4cfbaa8",
            "841e9f8dd0e74238a87efaf91f5cabe1",
            "4df77f934ad24e53916d6d1ffc3d46f5"
          ]
        },
        "id": "95_Nn-89DhsL",
        "outputId": "fa231e7a-e9d6-4f78-91d9-04e019ebb5a9"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Unsloth: Tokenizing [\"text\"] (num_proc=6):   0%|          | 0/33 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "77f657710d9b40fdb8b8854faff6a984"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    eval_dataset = None, # Can set up evaluation!\n",
        "    args = SFTConfig(\n",
        "        dataset_text_field = \"text\",\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4, # Use GA to mimic batch size!\n",
        "        warmup_steps = 5,\n",
        "        max_steps = 60,\n",
        "        learning_rate = 2e-4,\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        report_to = \"none\", # Use this for WandB etc\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ejIt2xSNKKp",
        "outputId": "848d962f-27ed-493b-e45b-4cf574c7fe95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU = Tesla T4. Max memory = 14.741 GB.\n",
            "6.297 GB of memory reserved.\n"
          ]
        }
      ],
      "source": [
        "# @title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yqxqAZ7KJ4oL",
        "outputId": "8f2bc5b1-2c04-4a95-ad80-404f9d778b24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 33 | Num Epochs = 12 | Total steps = 60\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 24,313,856 of 3,237,063,680 (0.75% trained)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [60/60 01:28, Epoch 12/12]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>4.575200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>4.534000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>4.605400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>4.196100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>3.594400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>3.431100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>2.958500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>2.809000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>2.511200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.469000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>2.187700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>1.829500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>1.964100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>1.800400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>1.192400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>1.558200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>1.567400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>1.354200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>1.489000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>2.205100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>1.375900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>1.322500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>1.106600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>1.313400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>1.023500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>1.072000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>1.120200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>1.140400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.948200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.770100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.855300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.755100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.946300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.982700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>1.313400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.824200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.724100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.681100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.819300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.639000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.640100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.665300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.694900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.623700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.399800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.653900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>0.587800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.452400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.540600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.575700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>0.436000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>0.464400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>0.558600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>0.529700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>0.512700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>0.407000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>0.503300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>0.435800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>0.482500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.548100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCqnaKmlO1U9",
        "outputId": "a8cd990c-c9fb-4238-8b73-4c0affad3187"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "90.3167 seconds used for training.\n",
            "1.51 minutes used for training.\n",
            "Peak reserved memory = 6.297 GB.\n",
            "Peak reserved memory for training = 0.0 GB.\n",
            "Peak reserved memory % of max memory = 42.718 %.\n",
            "Peak reserved memory for training % of max memory = 0.0 %.\n"
          ]
        }
      ],
      "source": [
        "# @title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(\n",
        "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
        ")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Let's run the model! Use `apply_chat_template` with `add_generation_prompt` set to `True` for inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kR3gIAX-SM2q",
        "outputId": "821be68c-a48d-4cd3-e25a-84f7f6f4a9ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A hybrid data construction pipeline that combines Think-on and Think-off responses.<|eot_id|>\n"
          ]
        }
      ],
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"What is the Byte Latent Transformer?\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
        "_ = model.generate(input_ids = inputs, streamer = text_streamer,\n",
        "                   max_new_tokens = 256, temperature = 0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRrr--20Udm9"
      },
      "source": [
        "The model learns about the research paper!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2strt31SUc5W",
        "outputId": "c856d975-465d-4981-d5ad-65b6b63f5c11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HiPO is a hybrid data construction pipeline that combines the strengths of HiPO and LLM to produce accurate and informative responses.<|eot_id|>\n"
          ]
        }
      ],
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"What is HiPO?\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
        "_ = model.generate(input_ids = inputs, streamer = text_streamer,\n",
        "                   max_new_tokens = 256, temperature = 0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMuVrWbjAzhc"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving, loading finetuned models\n",
        "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
        "\n",
        "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upcOlWe7A1vc",
        "outputId": "407b16b9-d6f5-415b-b12e-cdae5c902abc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('lora_model/tokenizer_config.json',\n",
              " 'lora_model/special_tokens_map.json',\n",
              " 'lora_model/chat_template.jinja',\n",
              " 'lora_model/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ],
      "source": [
        "model.save_pretrained(\"lora_model\")  # Local saving\n",
        "tokenizer.save_pretrained(\"lora_model\")\n",
        "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
        "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEEcJ4qfC7Lp"
      },
      "source": [
        "Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKX_XKs_BNZR",
        "outputId": "7466105c-350b-42e9-9296-cdf025550b9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HiPO: A Hybrid Policy Optimization Framework.<|eot_id|>\n"
          ]
        }
      ],
      "source": [
        "if False:\n",
        "    from unsloth import FastLanguageModel\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        max_seq_length = max_seq_length,\n",
        "        dtype = dtype,\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"what does HiPO stand fore?\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
        "_ = model.generate(input_ids = inputs, streamer = text_streamer,\n",
        "                   max_new_tokens = 256, temperature = 0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f422JgM9sdVT"
      },
      "source": [
        "### Saving to float16 for VLLM\n",
        "\n",
        "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "iHjt_SMYsd3P"
      },
      "outputs": [],
      "source": [
        "# Merge to 16bit\n",
        "if False:\n",
        "    model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
        "if False: # Change to True to upload finetune\n",
        "    model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
        "\n",
        "# Merge to 4bit\n",
        "if False:\n",
        "    model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
        "if False: # Change to True to upload finetune\n",
        "    model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
        "\n",
        "# Just LoRA adapters\n",
        "if False:\n",
        "    model.save_pretrained(\"model\")\n",
        "    tokenizer.save_pretrained(\"model\")\n",
        "if False: # Change to True to upload finetune\n",
        "    model.push_to_hub(\"hf/model\", token = \"\")\n",
        "    tokenizer.push_to_hub(\"hf/model\", token = \"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCv4vXHd61i7"
      },
      "source": [
        "### GGUF / llama.cpp Conversion\n",
        "To save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n",
        "\n",
        "Some supported quant methods (full list on our [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n",
        "* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n",
        "* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n",
        "* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "FqfebeAdT073"
      },
      "outputs": [],
      "source": [
        "# Save to 8bit Q8_0\n",
        "if False:\n",
        "    model.save_pretrained_gguf(\"model\", tokenizer,)\n",
        "if False: # Change to True to upload finetune\n",
        "    model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
        "\n",
        "# Save to 16bit GGUF\n",
        "if False:\n",
        "    model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
        "if False: # Change to True to upload finetune\n",
        "    model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
        "\n",
        "# Save to q4_k_m GGUF\n",
        "if False:\n",
        "    model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
        "if False: # Change to True to upload finetune\n",
        "    model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCGixd9hARrV"
      },
      "source": [
        "Now, use the `model-unsloth.gguf` file or `model-unsloth-Q4_K_M.gguf` file in llama.cpp.\n",
        "\n",
        "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
        "\n",
        "Some other links:\n",
        "1. Train your own reasoning model - Llama GRPO notebook [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb)\n",
        "2. Saving finetunes to Ollama. [Free notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)\n",
        "3. Llama 3.2 Vision finetuning - Radiography use case. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)\n",
        "6. See notebooks for DPO, ORPO, Continued pretraining, conversational finetuning and more on our [documentation](https://docs.unsloth.ai/get-started/unsloth-notebooks)!\n",
        "\n",
        "<div class=\"align-center\">\n",
        "  <a href=\"https://unsloth.ai\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "  <a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
        "  <a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a>\n",
        "\n",
        "  Join Discord if you need help + ⭐️ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ⭐️\n",
        "</div>\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "bc5255f132404e10aa6d8e189bab9b36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c902678c9a1b46e3a4666b85a97a3acd",
              "IPY_MODEL_5556359e0de44c9b9c66c818838aedf5",
              "IPY_MODEL_1d8ffbac8a6041ba875512ea492b076c"
            ],
            "layout": "IPY_MODEL_35d09c7ae0a049fcaaf8e86a8e417621"
          }
        },
        "c902678c9a1b46e3a4666b85a97a3acd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e18653fd6aa46acb621937f3aac9a8a",
            "placeholder": "​",
            "style": "IPY_MODEL_9cdcae8f9cd5448e8c79154cadcc352b",
            "value": "config.json: 100%"
          }
        },
        "5556359e0de44c9b9c66c818838aedf5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_85f2e9b6f0064d8f8a9e9bc8bfd6df38",
            "max": 894,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2950e1ee64da49b4acbf9944dda3314d",
            "value": 894
          }
        },
        "1d8ffbac8a6041ba875512ea492b076c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_843acde25a6e45aa8b5c7a1c75cd3544",
            "placeholder": "​",
            "style": "IPY_MODEL_e0af34e805c94388b8a8115a376897c7",
            "value": " 894/894 [00:00&lt;00:00, 103kB/s]"
          }
        },
        "35d09c7ae0a049fcaaf8e86a8e417621": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e18653fd6aa46acb621937f3aac9a8a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9cdcae8f9cd5448e8c79154cadcc352b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "85f2e9b6f0064d8f8a9e9bc8bfd6df38": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2950e1ee64da49b4acbf9944dda3314d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "843acde25a6e45aa8b5c7a1c75cd3544": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0af34e805c94388b8a8115a376897c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8ba5a4dbdb674789a0a617719437186a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0b8f6cae89f444c5a3c5b541b9621d03",
              "IPY_MODEL_a0ac13bcb305452ea5aec6ade87c68eb",
              "IPY_MODEL_43a5b03a04e84627a154946dc2d41e93"
            ],
            "layout": "IPY_MODEL_b472f5d4587a4ad39ea781e964fe0f5b"
          }
        },
        "0b8f6cae89f444c5a3c5b541b9621d03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5dfe7b3128a349369e930ebdf4842d44",
            "placeholder": "​",
            "style": "IPY_MODEL_655ff447e5c640ba9025df3e9fc10512",
            "value": "tokenizer_config.json: "
          }
        },
        "a0ac13bcb305452ea5aec6ade87c68eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_52c6e4e34953424f910df0f62f615160",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d7350a837b07472386b9ebb2a46c80be",
            "value": 1
          }
        },
        "43a5b03a04e84627a154946dc2d41e93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dcf94d67981e47fe86641b6bf2bec101",
            "placeholder": "​",
            "style": "IPY_MODEL_c99d35bf75984cee9a2089166974b190",
            "value": " 54.7k/? [00:00&lt;00:00, 4.89MB/s]"
          }
        },
        "b472f5d4587a4ad39ea781e964fe0f5b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5dfe7b3128a349369e930ebdf4842d44": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "655ff447e5c640ba9025df3e9fc10512": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "52c6e4e34953424f910df0f62f615160": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "d7350a837b07472386b9ebb2a46c80be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dcf94d67981e47fe86641b6bf2bec101": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c99d35bf75984cee9a2089166974b190": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0b44a74efd614512919ad1ad45c5b904": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1c676305631b43a68998ce34eeec566d",
              "IPY_MODEL_c6fa1942a4e2484b82b8740a4b581436",
              "IPY_MODEL_a2638345cfbf4949be4a7d107aad3ece"
            ],
            "layout": "IPY_MODEL_574f8061456c46759600bf8e22f26409"
          }
        },
        "1c676305631b43a68998ce34eeec566d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_888d76a2597b4db3b9d1cbc196f67664",
            "placeholder": "​",
            "style": "IPY_MODEL_89545bd2548f456f889e237b415c42dd",
            "value": "tokenizer.json: 100%"
          }
        },
        "c6fa1942a4e2484b82b8740a4b581436": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76bfee01fe7c4f0aa75ead68c0bcc49a",
            "max": 17209920,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b04f99bb1c64449fbf132ed8e6129c6d",
            "value": 17209920
          }
        },
        "a2638345cfbf4949be4a7d107aad3ece": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_731e4030f7b24b89870ff82eb7fa6eed",
            "placeholder": "​",
            "style": "IPY_MODEL_6f650273d91544e08f99ca860392df55",
            "value": " 17.2M/17.2M [00:00&lt;00:00, 27.5MB/s]"
          }
        },
        "574f8061456c46759600bf8e22f26409": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "888d76a2597b4db3b9d1cbc196f67664": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "89545bd2548f456f889e237b415c42dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "76bfee01fe7c4f0aa75ead68c0bcc49a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b04f99bb1c64449fbf132ed8e6129c6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "731e4030f7b24b89870ff82eb7fa6eed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f650273d91544e08f99ca860392df55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4e02c9fb37f149ac80c295eaf0eaeb80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_47e583bdc96e4682a0f514cb47658394",
              "IPY_MODEL_41f4db933794493e9cecc3f895a39f8b",
              "IPY_MODEL_18ed295f23c64bb8827bdf3b46e14e83"
            ],
            "layout": "IPY_MODEL_907fb80c198c4bcdae5488c41d463924"
          }
        },
        "47e583bdc96e4682a0f514cb47658394": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4103ea9f667b454683a8b1fbd4829297",
            "placeholder": "​",
            "style": "IPY_MODEL_dcfc7aad9f12453d8f5cad38c175823c",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "41f4db933794493e9cecc3f895a39f8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1158e1862798461a9b1985b3f352d2e6",
            "max": 454,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3bfc0ee630c7477d9f736915c22891c0",
            "value": 454
          }
        },
        "18ed295f23c64bb8827bdf3b46e14e83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b74a0cae76e4c239cb8c589f787cf2d",
            "placeholder": "​",
            "style": "IPY_MODEL_73609c795f6441018999cc9ebf799bfd",
            "value": " 454/454 [00:00&lt;00:00, 51.3kB/s]"
          }
        },
        "907fb80c198c4bcdae5488c41d463924": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4103ea9f667b454683a8b1fbd4829297": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dcfc7aad9f12453d8f5cad38c175823c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1158e1862798461a9b1985b3f352d2e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3bfc0ee630c7477d9f736915c22891c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1b74a0cae76e4c239cb8c589f787cf2d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73609c795f6441018999cc9ebf799bfd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "25d95c66ad4e4f2ca54f8071beee56fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aea11869fbaf47419aa13f93bcc72743",
              "IPY_MODEL_c9adc625c7d54d4cae3a3d9f290c2f74",
              "IPY_MODEL_757c9a5d8a7848a5a431b0dbb00b2a50"
            ],
            "layout": "IPY_MODEL_bee3a00153e443e6aec65b4a10a3ecf3"
          }
        },
        "aea11869fbaf47419aa13f93bcc72743": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_09f7aab30e8a4d43b82d5aaa7382b064",
            "placeholder": "​",
            "style": "IPY_MODEL_2d244b7c165d43f2a5f6bc894559d300",
            "value": "chat_template.jinja: "
          }
        },
        "c9adc625c7d54d4cae3a3d9f290c2f74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fc42d1b265c0444a886415304e34ba1f",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a1d8c68967cf454aa1707721b9988f9f",
            "value": 1
          }
        },
        "757c9a5d8a7848a5a431b0dbb00b2a50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c2a616e79f347f4a99d2a9016a0d729",
            "placeholder": "​",
            "style": "IPY_MODEL_8509d49d58c94523801d45f23c4ce92d",
            "value": " 3.83k/? [00:00&lt;00:00, 422kB/s]"
          }
        },
        "bee3a00153e443e6aec65b4a10a3ecf3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09f7aab30e8a4d43b82d5aaa7382b064": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d244b7c165d43f2a5f6bc894559d300": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fc42d1b265c0444a886415304e34ba1f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "a1d8c68967cf454aa1707721b9988f9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7c2a616e79f347f4a99d2a9016a0d729": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8509d49d58c94523801d45f23c4ce92d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "db8f5cee4be84c798268127406471088": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_204a0b2190e244f09784b7bf6bc2397d",
              "IPY_MODEL_7a4d0c544db5487e8db33302efb62097",
              "IPY_MODEL_ca93ce6e35e241b78259896f0507c985"
            ],
            "layout": "IPY_MODEL_8b8df34a01a644d6aa5df58698f29d96"
          }
        },
        "204a0b2190e244f09784b7bf6bc2397d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b404c1e9049e4560b4daa89cacd67c64",
            "placeholder": "​",
            "style": "IPY_MODEL_5e2788cfbd3f44039ebcda334ead120c",
            "value": "Map: 100%"
          }
        },
        "7a4d0c544db5487e8db33302efb62097": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_04383eb83e914cc29de8413a05650d44",
            "max": 33,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1b322db01b104d20bedc717f55397bc5",
            "value": 33
          }
        },
        "ca93ce6e35e241b78259896f0507c985": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_adc86bd2ce2c4b0f84e98f61f0be7cf2",
            "placeholder": "​",
            "style": "IPY_MODEL_26d0d564d2214573b833df3bdadb9561",
            "value": " 33/33 [00:00&lt;00:00, 1074.79 examples/s]"
          }
        },
        "8b8df34a01a644d6aa5df58698f29d96": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b404c1e9049e4560b4daa89cacd67c64": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e2788cfbd3f44039ebcda334ead120c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "04383eb83e914cc29de8413a05650d44": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b322db01b104d20bedc717f55397bc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "adc86bd2ce2c4b0f84e98f61f0be7cf2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26d0d564d2214573b833df3bdadb9561": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "77f657710d9b40fdb8b8854faff6a984": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9a50977e94c64659af5983aaaa5536b3",
              "IPY_MODEL_277dd073f8354058becaad1f6fe9b8f8",
              "IPY_MODEL_14d1b36a23474717a6c7dded32e96ae7"
            ],
            "layout": "IPY_MODEL_35a488aeafba4c8fa3a48f4094685390"
          }
        },
        "9a50977e94c64659af5983aaaa5536b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_98eb549d03344462be38d0a59a2863b7",
            "placeholder": "​",
            "style": "IPY_MODEL_70337c0e5a4d404b9cc1e5c0bdcb0c8f",
            "value": "Unsloth: Tokenizing [&quot;text&quot;] (num_proc=6): 100%"
          }
        },
        "277dd073f8354058becaad1f6fe9b8f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24790702e527421daa479ca64e7998db",
            "max": 33,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b9f296553ef4413face044c0c4cfbaa8",
            "value": 33
          }
        },
        "14d1b36a23474717a6c7dded32e96ae7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_841e9f8dd0e74238a87efaf91f5cabe1",
            "placeholder": "​",
            "style": "IPY_MODEL_4df77f934ad24e53916d6d1ffc3d46f5",
            "value": " 33/33 [00:06&lt;00:00,  8.25 examples/s]"
          }
        },
        "35a488aeafba4c8fa3a48f4094685390": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98eb549d03344462be38d0a59a2863b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70337c0e5a4d404b9cc1e5c0bdcb0c8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "24790702e527421daa479ca64e7998db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9f296553ef4413face044c0c4cfbaa8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "841e9f8dd0e74238a87efaf91f5cabe1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4df77f934ad24e53916d6d1ffc3d46f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}